Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Longitudinal Cracks Transverse Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Alligator Cracks Alligator Cracks Transverse Cracks Transverse Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.077541450659434, train_acc: 0.4136904776096344, val_loss: 1.0682250261306763, val_acc: 0.5
epoch: 1: train_loss: 1.044340749581655, train_acc: 0.3526785671710968, val_loss: 1.0619142055511475, val_acc: 0.421875
epoch: 2: train_loss: 1.0323933362960815, train_acc: 0.3616071442763011, val_loss: 0.8967224955558777, val_acc: 0.8125
epoch: 3: train_loss: 1.012625937660535, train_acc: 0.4345238109429677, val_loss: 1.0502154231071472, val_acc: 0.359375
epoch: 4: train_loss: 1.0197423458099366, train_acc: 0.3764880994955699, val_loss: 0.8837599456310272, val_acc: 0.71875
epoch: 5: train_loss: 1.010491904285219, train_acc: 0.4151785671710968, val_loss: 1.170184075832367, val_acc: 0.265625
epoch: 6: train_loss: 1.0069727159681776, train_acc: 0.3794642885526021, val_loss: 1.0450951159000397, val_acc: 0.421875
epoch: 7: train_loss: 1.0042481074730556, train_acc: 0.4122023781140645, val_loss: 0.8669373989105225, val_acc: 0.703125
epoch: 8: train_loss: 1.002871961505325, train_acc: 0.3928571442763011, val_loss: 0.8144189417362213, val_acc: 0.75
epoch: 9: train_loss: 0.9982159793376925, train_acc: 0.4241071442763011, val_loss: 0.8682871162891388, val_acc: 0.71875
epoch: 10: train_loss: 0.984705504142877, train_acc: 0.5446428656578064, val_loss: 0.954213410615921, val_acc: 0.625
epoch: 11: train_loss: 0.9790831655263902, train_acc: 0.4136904776096344, val_loss: 0.9205438196659088, val_acc: 0.578125
epoch: 12: train_loss: 0.9724548336787103, train_acc: 0.4449404776096344, val_loss: 0.8561821579933167, val_acc: 0.65625
epoch: 13: train_loss: 0.9682025512059532, train_acc: 0.4657738109429677, val_loss: 0.7371974289417267, val_acc: 0.875
epoch: 14: train_loss: 0.9601947810914784, train_acc: 0.523809532324473, val_loss: 0.75925412774086, val_acc: 0.796875
epoch: 15: train_loss: 0.958185893793901, train_acc: 0.4002976218859355, val_loss: 0.8138042092323303, val_acc: 0.78125
epoch: 16: train_loss: 0.9557433689341828, train_acc: 0.4345238109429677, val_loss: 0.882378339767456, val_acc: 0.6875
epoch: 17: train_loss: 0.9557615595835228, train_acc: 0.3497023781140645, val_loss: 0.8842926323413849, val_acc: 0.671875
epoch: 18: train_loss: 0.9494372365767497, train_acc: 0.4627976218859355, val_loss: 0.7953003644943237, val_acc: 0.75
epoch: 19: train_loss: 0.9476692328850429, train_acc: 0.4196428557236989, val_loss: 0.8610511720180511, val_acc: 0.6875
epoch: 20: train_loss: 0.9437646383330938, train_acc: 0.4508928656578064, val_loss: 0.8264965116977692, val_acc: 0.765625
epoch: 21: train_loss: 0.9419215095765665, train_acc: 0.4627976218859355, val_loss: 0.7407137453556061, val_acc: 0.828125
epoch: 22: train_loss: 0.939098239808843, train_acc: 0.4806547562281291, val_loss: 0.7350991070270538, val_acc: 0.828125
epoch: 23: train_loss: 0.9413770288228992, train_acc: 0.3556547661622365, val_loss: 0.7533418238162994, val_acc: 0.828125
epoch: 24: train_loss: 0.9409171136220299, train_acc: 0.4017857114473979, val_loss: 0.7840075194835663, val_acc: 0.75
epoch: 25: train_loss: 0.9428062721704827, train_acc: 0.3824404776096344, val_loss: 0.8470625281333923, val_acc: 0.71875
epoch: 26: train_loss: 0.9415947338681164, train_acc: 0.4166666666666667, val_loss: 0.7386399507522583, val_acc: 0.828125
epoch: 27: train_loss: 0.9403162073521389, train_acc: 0.4791666666666667, val_loss: 0.7888209223747253, val_acc: 0.78125
epoch: 28: train_loss: 0.9385240489038931, train_acc: 0.4657738109429677, val_loss: 0.912647545337677, val_acc: 0.640625
epoch: 29: train_loss: 0.9363758391804168, train_acc: 0.5, val_loss: 0.7720674872398376, val_acc: 0.734375
epoch: 30: train_loss: 0.9393303150771769, train_acc: 0.3333333333333333, val_loss: 0.6966356039047241, val_acc: 0.875
epoch: 31: train_loss: 0.9382368208219609, train_acc: 0.40625, val_loss: 0.7926031053066254, val_acc: 0.765625
epoch: 32: train_loss: 0.9372353312945126, train_acc: 0.4360119005044301, val_loss: 0.7253388464450836, val_acc: 0.8125
epoch: 33: train_loss: 0.9357356072640888, train_acc: 0.46875, val_loss: 0.8202668726444244, val_acc: 0.734375
epoch: 34: train_loss: 0.9358464320500693, train_acc: 0.4032738109429677, val_loss: 0.8632158935070038, val_acc: 0.71875
epoch: 35: train_loss: 0.9356099416812262, train_acc: 0.4404761989911397, val_loss: 0.7692987024784088, val_acc: 0.78125
epoch: 36: train_loss: 0.9320883111910779, train_acc: 0.4940476218859355, val_loss: 0.7451737821102142, val_acc: 0.8125
epoch: 37: train_loss: 0.933442892212617, train_acc: 0.3854166666666667, val_loss: 0.7742694914340973, val_acc: 0.78125
epoch: 38: train_loss: 0.9317782923706579, train_acc: 0.4880952338377635, val_loss: 0.7951216697692871, val_acc: 0.75
epoch: 39: train_loss: 0.932527221242587, train_acc: 0.3958333333333333, val_loss: 0.7981463968753815, val_acc: 0.734375
epoch: 40: train_loss: 0.9343904058138531, train_acc: 0.3333333333333333, val_loss: 0.7370985150337219, val_acc: 0.828125
epoch: 41: train_loss: 0.9339146968864261, train_acc: 0.4151785671710968, val_loss: 0.8061627149581909, val_acc: 0.765625
epoch: 42: train_loss: 0.9335588013478954, train_acc: 0.5014880895614624, val_loss: 0.6873267292976379, val_acc: 0.859375
epoch: 43: train_loss: 0.9324547368468662, train_acc: 0.4761904776096344, val_loss: 0.8170130550861359, val_acc: 0.6875
epoch: 44: train_loss: 0.931696556232594, train_acc: 0.4598214228947957, val_loss: 0.8013352155685425, val_acc: 0.734375
epoch: 45: train_loss: 0.930647098499796, train_acc: 0.4970238109429677, val_loss: 0.8217895925045013, val_acc: 0.734375
epoch: 46: train_loss: 0.9306394812908582, train_acc: 0.4122023781140645, val_loss: 0.6945156753063202, val_acc: 0.859375
epoch: 47: train_loss: 0.9309401106503278, train_acc: 0.46875, val_loss: 0.7524342238903046, val_acc: 0.8125
epoch: 48: train_loss: 0.9306867921433485, train_acc: 0.4404761890570323, val_loss: 0.7363201975822449, val_acc: 0.828125
epoch: 49: train_loss: 0.9300766511758173, train_acc: 0.4747023781140645, val_loss: 0.7724672555923462, val_acc: 0.78125
epoch: 50: train_loss: 0.9287982337615074, train_acc: 0.5074404776096344, val_loss: 0.7321267127990723, val_acc: 0.8125
epoch: 51: train_loss: 0.9278959226913945, train_acc: 0.4657738109429677, val_loss: 0.8140662014484406, val_acc: 0.734375
epoch: 52: train_loss: 0.9270056523616962, train_acc: 0.4776785671710968, val_loss: 0.7831957936286926, val_acc: 0.78125
epoch: 53: train_loss: 0.9278609248591061, train_acc: 0.3943452388048172, val_loss: 0.7224656939506531, val_acc: 0.828125
epoch: 54: train_loss: 0.9271492791898326, train_acc: 0.4523809552192688, val_loss: 0.7180543541908264, val_acc: 0.828125
epoch: 55: train_loss: 0.92558827996254, train_acc: 0.523809532324473, val_loss: 0.8551628589630127, val_acc: 0.640625
epoch: 56: train_loss: 0.9243745549380434, train_acc: 0.4375, val_loss: 0.7365283071994781, val_acc: 0.921875
epoch: 57: train_loss: 0.9220264417001574, train_acc: 0.5119047661622366, val_loss: 0.675678551197052, val_acc: 0.84375
epoch: 58: train_loss: 0.9191238172983722, train_acc: 0.4940476218859355, val_loss: 0.7521923184394836, val_acc: 0.859375
epoch: 59: train_loss: 0.9170551309982939, train_acc: 0.4538690447807312, val_loss: 0.6307280659675598, val_acc: 0.96875
epoch: 60: train_loss: 0.9133712330151129, train_acc: 0.5788690447807312, val_loss: 0.7070415616035461, val_acc: 0.84375
epoch: 61: train_loss: 0.9108994356406636, train_acc: 0.4821428656578064, val_loss: 0.6208947598934174, val_acc: 0.9375
epoch: 62: train_loss: 0.9082084865797138, train_acc: 0.5208333333333334, val_loss: 0.6283445358276367, val_acc: 0.890625
epoch: 63: train_loss: 0.9055207098523779, train_acc: 0.4910714228947957, val_loss: 0.6920326054096222, val_acc: 0.828125
epoch: 64: train_loss: 0.9028072277704877, train_acc: 0.4895833333333333, val_loss: 0.6404347717761993, val_acc: 0.90625
epoch: 65: train_loss: 0.8996151447898213, train_acc: 0.5565476218859354, val_loss: 0.6041077673435211, val_acc: 0.96875
epoch: 66: train_loss: 0.8973142264494258, train_acc: 0.5014880895614624, val_loss: 0.6861866116523743, val_acc: 0.859375
epoch: 67: train_loss: 0.89345737064586, train_acc: 0.6116071343421936, val_loss: 0.6319935619831085, val_acc: 0.9375
epoch: 68: train_loss: 0.8897186722156509, train_acc: 0.5982142885526022, val_loss: 0.6046652793884277, val_acc: 0.9375
epoch: 69: train_loss: 0.8865363907246365, train_acc: 0.5639880895614624, val_loss: 0.5746822655200958, val_acc: 1.0
epoch: 70: train_loss: 0.8840956587187003, train_acc: 0.4940476218859355, val_loss: 0.6092165112495422, val_acc: 0.921875
epoch: 71: train_loss: 0.8808113940887983, train_acc: 0.5699404776096344, val_loss: 0.5879517495632172, val_acc: 0.96875
epoch: 72: train_loss: 0.8784709371388232, train_acc: 0.4791666666666667, val_loss: 0.6362826228141785, val_acc: 0.90625
epoch: 73: train_loss: 0.8754563100703129, train_acc: 0.5461309552192688, val_loss: 0.575498640537262, val_acc: 0.96875
epoch: 74: train_loss: 0.8728231268458897, train_acc: 0.523809532324473, val_loss: 0.668670266866684, val_acc: 0.875
epoch: 75: train_loss: 0.870559803226538, train_acc: 0.5148809552192688, val_loss: 0.5865978300571442, val_acc: 0.96875
epoch: 76: train_loss: 0.8670970309864393, train_acc: 0.6116071343421936, val_loss: 0.6226127445697784, val_acc: 0.90625
epoch: 77: train_loss: 0.8651789724826815, train_acc: 0.4985119005044301, val_loss: 0.6339575350284576, val_acc: 0.921875
epoch: 78: train_loss: 0.8633727926745196, train_acc: 0.5416666666666666, val_loss: 0.6090894043445587, val_acc: 0.953125
epoch: 79: train_loss: 0.8611516147851945, train_acc: 0.5401785671710968, val_loss: 0.6140036880970001, val_acc: 0.921875
epoch: 80: train_loss: 0.8588873258343451, train_acc: 0.5163690447807312, val_loss: 0.6634170413017273, val_acc: 0.890625
epoch: 81: train_loss: 0.8576844438789338, train_acc: 0.4910714228947957, val_loss: 0.5970281958580017, val_acc: 0.953125
epoch: 82: train_loss: 0.8549106188088537, train_acc: 0.5684523781140646, val_loss: 0.5804900825023651, val_acc: 0.984375
epoch: 83: train_loss: 0.8524993409713111, train_acc: 0.5446428656578064, val_loss: 0.5810981690883636, val_acc: 0.96875
epoch: 84: train_loss: 0.8508148144273199, train_acc: 0.4955357114473979, val_loss: 0.5674686133861542, val_acc: 1.0
epoch: 85: train_loss: 0.8486202909040822, train_acc: 0.5491071343421936, val_loss: 0.5694052875041962, val_acc: 1.0
epoch: 86: train_loss: 0.8458139562972206, train_acc: 0.6354166666666666, val_loss: 0.6259225010871887, val_acc: 0.9375
epoch: 87: train_loss: 0.8444511703017987, train_acc: 0.46875, val_loss: 0.5870616734027863, val_acc: 0.96875
epoch: 88: train_loss: 0.8420331567414245, train_acc: 0.6116071343421936, val_loss: 0.5939092338085175, val_acc: 0.953125
epoch: 89: train_loss: 0.8394223965980389, train_acc: 0.6116071343421936, val_loss: 0.5656853020191193, val_acc: 0.984375
epoch: 90: train_loss: 0.839010903686831, train_acc: 0.3824404776096344, val_loss: 0.6424601376056671, val_acc: 0.859375
epoch: 91: train_loss: 0.8375234839276992, train_acc: 0.5074404776096344, val_loss: 0.6184857487678528, val_acc: 0.875
epoch: 92: train_loss: 0.8364019265738868, train_acc: 0.4895833333333333, val_loss: 0.5687694549560547, val_acc: 0.984375
epoch: 93: train_loss: 0.8345044491984321, train_acc: 0.5282738109429678, val_loss: 0.576214611530304, val_acc: 0.984375
epoch: 94: train_loss: 0.8334918609836646, train_acc: 0.4583333333333333, val_loss: 0.5609733462333679, val_acc: 1.0
epoch: 95: train_loss: 0.8322782141880857, train_acc: 0.4880952338377635, val_loss: 0.5723407566547394, val_acc: 0.984375
epoch: 96: train_loss: 0.8302248292362567, train_acc: 0.5788690447807312, val_loss: 0.5732791721820831, val_acc: 0.984375
epoch: 97: train_loss: 0.8282245079270837, train_acc: 0.5654761989911398, val_loss: 0.5944374799728394, val_acc: 0.9375
epoch: 98: train_loss: 0.8267799232543921, train_acc: 0.5193452338377634, val_loss: 0.5562686622142792, val_acc: 1.0
epoch: 99: train_loss: 0.8249021595716478, train_acc: 0.5982142885526022, val_loss: 0.5541791617870331, val_acc: 1.0
epoch: 100: train_loss: 0.8233201877512162, train_acc: 0.5401785671710968, val_loss: 0.5613954365253448, val_acc: 0.984375
epoch: 101: train_loss: 0.8221028593241001, train_acc: 0.4925595323244731, val_loss: 0.6110123693943024, val_acc: 0.9375
epoch: 102: train_loss: 0.821582903561083, train_acc: 0.4553571442763011, val_loss: 0.5737518668174744, val_acc: 1.0
epoch: 103: train_loss: 0.8204232693100588, train_acc: 0.4985119005044301, val_loss: 0.5582916736602783, val_acc: 1.0
epoch: 104: train_loss: 0.8190368856702533, train_acc: 0.5327380895614624, val_loss: 0.5795353353023529, val_acc: 0.96875
epoch: 105: train_loss: 0.817533296421639, train_acc: 0.543154756228129, val_loss: 0.5604516565799713, val_acc: 0.984375
epoch: 106: train_loss: 0.8158971052674862, train_acc: 0.5982142885526022, val_loss: 0.56737220287323, val_acc: 1.0
epoch: 107: train_loss: 0.814853690288685, train_acc: 0.5059523781140646, val_loss: 0.5658161640167236, val_acc: 0.984375
epoch: 108: train_loss: 0.813981082643573, train_acc: 0.5416666666666666, val_loss: 0.5890403091907501, val_acc: 0.9375
epoch: 109: train_loss: 0.8130023535453912, train_acc: 0.4717261989911397, val_loss: 0.5944039821624756, val_acc: 0.953125
epoch: 110: train_loss: 0.8118725046739205, train_acc: 0.549107144276301, val_loss: 0.5978786051273346, val_acc: 0.953125
epoch: 111: train_loss: 0.810924102685281, train_acc: 0.5074404776096344, val_loss: 0.5775959193706512, val_acc: 0.96875
epoch: 112: train_loss: 0.8101788173734613, train_acc: 0.4598214228947957, val_loss: 0.5562819242477417, val_acc: 1.0
epoch: 113: train_loss: 0.8092816923445426, train_acc: 0.5089285671710968, val_loss: 0.5626204013824463, val_acc: 1.0
epoch: 114: train_loss: 0.8082611802695452, train_acc: 0.4955357114473979, val_loss: 0.5765360593795776, val_acc: 0.984375
epoch: 115: train_loss: 0.8066255243002681, train_acc: 0.574404756228129, val_loss: 0.5813789069652557, val_acc: 0.96875
epoch: 116: train_loss: 0.8055471693009052, train_acc: 0.5357142885526022, val_loss: 0.629656046628952, val_acc: 0.90625
epoch: 117: train_loss: 0.804482456317729, train_acc: 0.5327380895614624, val_loss: 0.5608891248703003, val_acc: 1.0
epoch: 118: train_loss: 0.8033164205337437, train_acc: 0.5639880895614624, val_loss: 0.5934851169586182, val_acc: 0.984375
epoch: 119: train_loss: 0.801734397974279, train_acc: 0.5922619104385376, val_loss: 0.5593801140785217, val_acc: 1.0
epoch: 120: train_loss: 0.8012656129424564, train_acc: 0.5, val_loss: 0.5698354542255402, val_acc: 0.984375
epoch: 121: train_loss: 0.8008212473223116, train_acc: 0.4136904776096344, val_loss: 0.6240153014659882, val_acc: 0.9375
epoch: 122: train_loss: 0.7997248658965919, train_acc: 0.5491071343421936, val_loss: 0.627694696187973, val_acc: 0.921875
epoch: 123: train_loss: 0.7984113414441384, train_acc: 0.5520833333333334, val_loss: 0.5762399137020111, val_acc: 1.0
epoch: 124: train_loss: 0.7971167952219643, train_acc: 0.5565476218859354, val_loss: 0.5731402337551117, val_acc: 0.96875
epoch: 125: train_loss: 0.796737995097246, train_acc: 0.4434523781140645, val_loss: 0.5578512251377106, val_acc: 1.0
epoch: 126: train_loss: 0.7955074031834838, train_acc: 0.5565476218859354, val_loss: 0.579397439956665, val_acc: 0.984375
epoch: 127: train_loss: 0.7950600829596319, train_acc: 0.4613095223903656, val_loss: 0.5756082236766815, val_acc: 0.96875
epoch: 128: train_loss: 0.7940380585594077, train_acc: 0.5386904776096344, val_loss: 0.574510395526886, val_acc: 0.984375
epoch: 129: train_loss: 0.7931080831931185, train_acc: 0.5223214228947958, val_loss: 0.6139285862445831, val_acc: 0.9375
epoch: 130: train_loss: 0.7916993163620847, train_acc: 0.6116071343421936, val_loss: 0.5613448619842529, val_acc: 1.0
epoch: 131: train_loss: 0.7904264949487915, train_acc: 0.5877976218859354, val_loss: 0.5559099912643433, val_acc: 1.0
epoch: 132: train_loss: 0.7894911630111826, train_acc: 0.5327380895614624, val_loss: 0.5603072643280029, val_acc: 0.984375
epoch: 133: train_loss: 0.7882457966383415, train_acc: 0.5982142885526022, val_loss: 0.5829963088035583, val_acc: 0.953125
epoch: 134: train_loss: 0.7869240649688389, train_acc: 0.6071428656578064, val_loss: 0.5586811900138855, val_acc: 1.0
epoch: 135: train_loss: 0.785999065155492, train_acc: 0.5461309552192688, val_loss: 0.5843661725521088, val_acc: 0.984375
epoch: 136: train_loss: 0.7854789512145837, train_acc: 0.4732142885526021, val_loss: 0.5620854794979095, val_acc: 1.0
epoch: 137: train_loss: 0.7846495686953768, train_acc: 0.5223214228947958, val_loss: 0.6426675319671631, val_acc: 0.875
epoch: 138: train_loss: 0.7835704033180391, train_acc: 0.5773809552192688, val_loss: 0.6221920549869537, val_acc: 0.875
epoch: 139: train_loss: 0.7829065090843608, train_acc: 0.4985119005044301, val_loss: 0.6005378067493439, val_acc: 0.921875
epoch: 140: train_loss: 0.7823603462524727, train_acc: 0.4940476218859355, val_loss: 0.5727883875370026, val_acc: 0.984375
epoch: 141: train_loss: 0.7813661914895955, train_acc: 0.5877976218859354, val_loss: 0.612900584936142, val_acc: 0.90625
epoch: 142: train_loss: 0.7806358856337885, train_acc: 0.5267857114473978, val_loss: 0.590561717748642, val_acc: 0.9375
epoch: 143: train_loss: 0.7808271818276906, train_acc: 0.4017857114473979, val_loss: 0.5688461065292358, val_acc: 1.0
epoch: 144: train_loss: 0.7797626404241581, train_acc: 0.617559532324473, val_loss: 0.5620386004447937, val_acc: 1.0
epoch: 145: train_loss: 0.779273898974401, train_acc: 0.5148809552192688, val_loss: 0.5589282214641571, val_acc: 1.0
epoch: 146: train_loss: 0.7784398482635175, train_acc: 0.5639880895614624, val_loss: 0.5616959035396576, val_acc: 1.0
epoch: 147: train_loss: 0.7778208260466384, train_acc: 0.5163690447807312, val_loss: 0.55279740691185, val_acc: 1.0
epoch: 148: train_loss: 0.777419414762949, train_acc: 0.4985119005044301, val_loss: 0.5692171156406403, val_acc: 0.984375
epoch: 149: train_loss: 0.776762726638052, train_acc: 0.5014880895614624, val_loss: 0.5883194506168365, val_acc: 0.9375
epoch: 150: train_loss: 0.7763082446246745, train_acc: 0.4627976218859355, val_loss: 0.5590800344944, val_acc: 1.0
epoch: 151: train_loss: 0.775757080695608, train_acc: 0.4925595323244731, val_loss: 0.5623204112052917, val_acc: 1.0
epoch: 152: train_loss: 0.774817881306272, train_acc: 0.574404756228129, val_loss: 0.5595921874046326, val_acc: 1.0
epoch: 153: train_loss: 0.7737967322915146, train_acc: 0.59375, val_loss: 0.5954883098602295, val_acc: 0.96875
epoch: 154: train_loss: 0.7733495452070748, train_acc: 0.5059523781140646, val_loss: 0.5820794701576233, val_acc: 0.96875
epoch: 155: train_loss: 0.7726892607334331, train_acc: 0.5327380895614624, val_loss: 0.5868221819400787, val_acc: 0.984375
epoch: 156: train_loss: 0.7724516470214616, train_acc: 0.4642857114473979, val_loss: 0.5819587111473083, val_acc: 0.96875
epoch: 157: train_loss: 0.771882385266984, train_acc: 0.5818452338377634, val_loss: 0.6437393426895142, val_acc: 0.890625
epoch: 158: train_loss: 0.7715845780302644, train_acc: 0.4836309552192688, val_loss: 0.5765397548675537, val_acc: 1.0
epoch: 159: train_loss: 0.7713594769438108, train_acc: 0.4494047562281291, val_loss: 0.6292438507080078, val_acc: 0.921875
epoch: 160: train_loss: 0.7705160052386376, train_acc: 0.574404756228129, val_loss: 0.579550713300705, val_acc: 0.96875
epoch: 161: train_loss: 0.7696304691671834, train_acc: 0.6264880895614624, val_loss: 0.573228657245636, val_acc: 1.0
epoch: 162: train_loss: 0.7683636914245433, train_acc: 0.6547619005044302, val_loss: 0.5581933856010437, val_acc: 1.0
epoch: 163: train_loss: 0.7678255918549327, train_acc: 0.5223214228947958, val_loss: 0.5712885856628418, val_acc: 0.984375
epoch: 164: train_loss: 0.7666676246758662, train_acc: 0.6354166666666666, val_loss: 0.5954975187778473, val_acc: 0.9375
epoch: 165: train_loss: 0.7659234964703938, train_acc: 0.5654761989911398, val_loss: 0.5746089518070221, val_acc: 0.984375
epoch: 166: train_loss: 0.7657882823439652, train_acc: 0.4642857114473979, val_loss: 0.5750021934509277, val_acc: 0.96875
epoch: 167: train_loss: 0.7654478484912524, train_acc: 0.5267857114473978, val_loss: 0.57913738489151, val_acc: 0.984375
epoch: 168: train_loss: 0.764914583054755, train_acc: 0.5505952338377634, val_loss: 0.6052872240543365, val_acc: 0.921875
epoch: 169: train_loss: 0.764749849543852, train_acc: 0.5, val_loss: 0.6017984747886658, val_acc: 0.921875
epoch: 170: train_loss: 0.7641632344290527, train_acc: 0.5297619005044302, val_loss: 0.5537882447242737, val_acc: 1.0
epoch: 171: train_loss: 0.7635934143796447, train_acc: 0.53125, val_loss: 0.6302831768989563, val_acc: 0.90625
epoch: 172: train_loss: 0.7631143163853756, train_acc: 0.53125, val_loss: 0.5902531743049622, val_acc: 0.953125
epoch: 173: train_loss: 0.7626765808839906, train_acc: 0.5148809552192688, val_loss: 0.572160929441452, val_acc: 0.984375
epoch: 174: train_loss: 0.7617063837959651, train_acc: 0.5922619104385376, val_loss: 0.5816018581390381, val_acc: 0.96875
epoch: 175: train_loss: 0.760881632792227, train_acc: 0.605654756228129, val_loss: 0.573294997215271, val_acc: 1.0
epoch: 176: train_loss: 0.7605297620462605, train_acc: 0.4895833333333333, val_loss: 0.578339159488678, val_acc: 0.984375
epoch: 177: train_loss: 0.7601839708031785, train_acc: 0.5, val_loss: 0.5757128894329071, val_acc: 0.96875
epoch: 178: train_loss: 0.7600877021944055, train_acc: 0.4553571442763011, val_loss: 0.5656433403491974, val_acc: 0.984375
epoch: 179: train_loss: 0.7595548666185802, train_acc: 0.543154756228129, val_loss: 0.5827995538711548, val_acc: 0.9375
epoch: 180: train_loss: 0.7590907784676242, train_acc: 0.5520833333333334, val_loss: 0.5566039085388184, val_acc: 1.0
epoch: 181: train_loss: 0.7585485530641924, train_acc: 0.5327380895614624, val_loss: 0.5586222112178802, val_acc: 1.0
epoch: 182: train_loss: 0.7581976614146065, train_acc: 0.5476190447807312, val_loss: 0.6050597131252289, val_acc: 0.921875
epoch: 183: train_loss: 0.7578967739490493, train_acc: 0.4717261989911397, val_loss: 0.6419345736503601, val_acc: 0.90625
epoch: 184: train_loss: 0.7573582523577919, train_acc: 0.5491071343421936, val_loss: 0.5954537987709045, val_acc: 0.953125
epoch: 185: train_loss: 0.7573923077634582, train_acc: 0.4538690447807312, val_loss: 0.6048348248004913, val_acc: 0.9375
epoch: 186: train_loss: 0.7570092844878074, train_acc: 0.517857144276301, val_loss: 0.5988166630268097, val_acc: 0.984375
epoch: 187: train_loss: 0.7566809620417599, train_acc: 0.5, val_loss: 0.5561767518520355, val_acc: 1.0
epoch: 188: train_loss: 0.7565031504715138, train_acc: 0.4583333333333333, val_loss: 0.5523577332496643, val_acc: 1.0
epoch: 189: train_loss: 0.7560840729035828, train_acc: 0.5163690447807312, val_loss: 0.5730803906917572, val_acc: 0.984375
epoch: 190: train_loss: 0.7554960045931017, train_acc: 0.5684523781140646, val_loss: 0.5639934539794922, val_acc: 0.984375
epoch: 191: train_loss: 0.7551503427740599, train_acc: 0.4910714228947957, val_loss: 0.5601032674312592, val_acc: 1.0
epoch: 192: train_loss: 0.7547917217788299, train_acc: 0.5089285671710968, val_loss: 0.6456890106201172, val_acc: 0.90625
epoch: 193: train_loss: 0.7549393158598043, train_acc: 0.4672619005044301, val_loss: 0.5620076656341553, val_acc: 0.984375
epoch: 194: train_loss: 0.7543550998736649, train_acc: 0.574404756228129, val_loss: 0.567576676607132, val_acc: 0.984375
epoch: 195: train_loss: 0.7539576684333839, train_acc: 0.5223214228947958, val_loss: 0.5958791077136993, val_acc: 0.96875
epoch: 196: train_loss: 0.753667969489864, train_acc: 0.5669642885526022, val_loss: 0.6029084920883179, val_acc: 0.953125
epoch: 197: train_loss: 0.753333432104451, train_acc: 0.53125, val_loss: 0.5727549493312836, val_acc: 0.984375
epoch: 198: train_loss: 0.752821915413267, train_acc: 0.5848214228947958, val_loss: 0.5664426386356354, val_acc: 0.984375
epoch: 199: train_loss: 0.7526605476935704, train_acc: 0.4806547661622365, val_loss: 0.6919574737548828, val_acc: 0.859375
epoch: 200: train_loss: 0.752459603261394, train_acc: 0.4985119005044301, val_loss: 0.5523742139339447, val_acc: 1.0
epoch: 201: train_loss: 0.7523414074587742, train_acc: 0.4761904776096344, val_loss: 0.5982691049575806, val_acc: 0.9375
epoch: 202: train_loss: 0.7520739421468648, train_acc: 0.5029761989911398, val_loss: 0.5718261003494263, val_acc: 0.984375
epoch: 203: train_loss: 0.7516552675393671, train_acc: 0.5372023781140646, val_loss: 0.5692733228206635, val_acc: 0.984375
epoch: 204: train_loss: 0.7517272486919309, train_acc: 0.4419642885526021, val_loss: 0.5763400793075562, val_acc: 0.984375
epoch: 205: train_loss: 0.7513838729619208, train_acc: 0.5208333333333334, val_loss: 0.586393415927887, val_acc: 0.96875
epoch: 206: train_loss: 0.7510595313016918, train_acc: 0.5089285671710968, val_loss: 0.5819298028945923, val_acc: 0.96875
epoch: 207: train_loss: 0.7509134157727926, train_acc: 0.5, val_loss: 0.5716828405857086, val_acc: 0.984375
epoch: 208: train_loss: 0.7503549781712617, train_acc: 0.5892857114473978, val_loss: 0.5544787347316742, val_acc: 1.0
epoch: 209: train_loss: 0.7497379861180743, train_acc: 0.5669642885526022, val_loss: 0.5535075962543488, val_acc: 1.0
epoch: 210: train_loss: 0.7492303133575836, train_acc: 0.5877976218859354, val_loss: 0.5590679347515106, val_acc: 1.0
epoch: 211: train_loss: 0.7489927382026826, train_acc: 0.4985119005044301, val_loss: 0.5696463882923126, val_acc: 0.984375
epoch: 212: train_loss: 0.7488760856768706, train_acc: 0.4806547562281291, val_loss: 0.5518860220909119, val_acc: 1.0
epoch: 213: train_loss: 0.7485067783115064, train_acc: 0.5223214228947958, val_loss: 0.5887718200683594, val_acc: 0.9375
epoch: 214: train_loss: 0.7481821891873379, train_acc: 0.5104166666666666, val_loss: 0.561501532793045, val_acc: 1.0
epoch: 215: train_loss: 0.7478224802532311, train_acc: 0.5282738109429678, val_loss: 0.5671519935131073, val_acc: 0.984375
epoch: 216: train_loss: 0.7473415379883137, train_acc: 0.5773809552192688, val_loss: 0.5659325122833252, val_acc: 0.984375
epoch: 217: train_loss: 0.7470735285989367, train_acc: 0.4985119005044301, val_loss: 0.559412807226181, val_acc: 1.0
epoch: 218: train_loss: 0.7468664687881001, train_acc: 0.4985119005044301, val_loss: 0.56263467669487, val_acc: 0.984375
epoch: 219: train_loss: 0.7463945729262899, train_acc: 0.5654761989911398, val_loss: 0.5541532635688782, val_acc: 1.0
epoch: 220: train_loss: 0.7458799158645788, train_acc: 0.5669642885526022, val_loss: 0.5766546428203583, val_acc: 0.984375
epoch: 221: train_loss: 0.7453220564502849, train_acc: 0.5773809552192688, val_loss: 0.5558399260044098, val_acc: 1.0
epoch: 222: train_loss: 0.7448166478731525, train_acc: 0.555059532324473, val_loss: 0.5810493528842926, val_acc: 0.984375
epoch: 223: train_loss: 0.744409915148502, train_acc: 0.5446428656578064, val_loss: 0.5979292988777161, val_acc: 0.96875
epoch: 224: train_loss: 0.7438681792329855, train_acc: 0.581845243771871, val_loss: 0.5566849708557129, val_acc: 1.0
epoch: 225: train_loss: 0.7434306369770238, train_acc: 0.5505952338377634, val_loss: 0.5871464312076569, val_acc: 0.96875
epoch: 226: train_loss: 0.7434933906784839, train_acc: 0.4330357114473979, val_loss: 0.6331868767738342, val_acc: 0.921875
epoch: 227: train_loss: 0.7431290591495077, train_acc: 0.543154756228129, val_loss: 0.6326358914375305, val_acc: 0.921875
epoch: 228: train_loss: 0.7428872598310743, train_acc: 0.5327380895614624, val_loss: 0.5832972824573517, val_acc: 0.96875
epoch: 229: train_loss: 0.74244827222133, train_acc: 0.5848214228947958, val_loss: 0.565587043762207, val_acc: 0.984375
epoch: 230: train_loss: 0.7422908444769281, train_acc: 0.5193452338377634, val_loss: 0.6093178987503052, val_acc: 0.9375
epoch: 231: train_loss: 0.741767773340488, train_acc: 0.5669642885526022, val_loss: 0.5674160122871399, val_acc: 0.984375
epoch: 232: train_loss: 0.7414672119764126, train_acc: 0.5074404776096344, val_loss: 0.5751037001609802, val_acc: 1.0
epoch: 233: train_loss: 0.741061118593243, train_acc: 0.581845243771871, val_loss: 0.569817066192627, val_acc: 0.984375
epoch: 234: train_loss: 0.7407282795466428, train_acc: 0.5327380895614624, val_loss: 0.6183559000492096, val_acc: 0.9375
epoch: 235: train_loss: 0.7403893080976721, train_acc: 0.53125, val_loss: 0.624513566493988, val_acc: 0.921875
epoch: 236: train_loss: 0.7401723161696044, train_acc: 0.523809532324473, val_loss: 0.6177999973297119, val_acc: 0.9375
epoch: 237: train_loss: 0.7398759382946483, train_acc: 0.5327380895614624, val_loss: 0.5585029721260071, val_acc: 1.0
epoch: 238: train_loss: 0.7397783115152865, train_acc: 0.4985119005044301, val_loss: 0.5541769564151764, val_acc: 1.0
epoch: 239: train_loss: 0.7396559210287198, train_acc: 0.5, val_loss: 0.5881136357784271, val_acc: 0.9375
epoch: 240: train_loss: 0.7396440181989392, train_acc: 0.4464285671710968, val_loss: 0.5710134506225586, val_acc: 0.984375
epoch: 241: train_loss: 0.7391645046305064, train_acc: 0.5595238010088602, val_loss: 0.6138930022716522, val_acc: 0.9375
epoch: 242: train_loss: 0.7387965497329564, train_acc: 0.5342261989911398, val_loss: 0.5560317933559418, val_acc: 1.0
epoch: 243: train_loss: 0.7385815181693092, train_acc: 0.5892857114473978, val_loss: 0.5737197697162628, val_acc: 0.984375
epoch: 244: train_loss: 0.7382032111388483, train_acc: 0.5535714228947958, val_loss: 0.553001880645752, val_acc: 1.0
epoch: 245: train_loss: 0.7381001275567826, train_acc: 0.4717261989911397, val_loss: 0.587512731552124, val_acc: 0.9375
epoch: 246: train_loss: 0.7380943396474345, train_acc: 0.4583333333333333, val_loss: 0.5530849099159241, val_acc: 1.0
epoch: 247: train_loss: 0.7377698503995452, train_acc: 0.5193452338377634, val_loss: 0.602291077375412, val_acc: 0.9375
epoch: 248: train_loss: 0.7374416927257214, train_acc: 0.5491071343421936, val_loss: 0.5793002545833588, val_acc: 0.96875
epoch: 249: train_loss: 0.7376418147087095, train_acc: 0.4077380895614624, val_loss: 0.55778568983078, val_acc: 1.0
epoch: 250: train_loss: 0.7373987210699284, train_acc: 0.550595243771871, val_loss: 0.557842344045639, val_acc: 1.0
epoch: 251: train_loss: 0.7373116651067025, train_acc: 0.4672619005044301, val_loss: 0.5539468228816986, val_acc: 1.0
epoch: 252: train_loss: 0.7370643574886798, train_acc: 0.5327380895614624, val_loss: 0.5534016788005829, val_acc: 1.0
epoch: 253: train_loss: 0.736649828554764, train_acc: 0.5877976218859354, val_loss: 0.5566739439964294, val_acc: 1.0
epoch: 254: train_loss: 0.7364460293763602, train_acc: 0.5193452338377634, val_loss: 0.5903553664684296, val_acc: 0.9375
epoch: 255: train_loss: 0.7362150965879359, train_acc: 0.5014880895614624, val_loss: 0.5848435163497925, val_acc: 0.9375
epoch: 256: train_loss: 0.736003651615865, train_acc: 0.5252976218859354, val_loss: 0.560612827539444, val_acc: 0.984375
epoch: 257: train_loss: 0.7360514569498154, train_acc: 0.4345238109429677, val_loss: 0.6372758448123932, val_acc: 0.9375
epoch: 258: train_loss: 0.735786381298664, train_acc: 0.5505952338377634, val_loss: 0.5570084750652313, val_acc: 1.0
epoch: 259: train_loss: 0.7355511565239, train_acc: 0.5178571343421936, val_loss: 0.5539482235908508, val_acc: 1.0
epoch: 260: train_loss: 0.7350819847364534, train_acc: 0.59375, val_loss: 0.5720334351062775, val_acc: 0.984375
epoch: 261: train_loss: 0.7348864723480383, train_acc: 0.4940476218859355, val_loss: 0.5516780316829681, val_acc: 1.0
epoch: 262: train_loss: 0.7345975542627509, train_acc: 0.5461309552192688, val_loss: 0.6010721027851105, val_acc: 0.9375
epoch: 263: train_loss: 0.734383593793168, train_acc: 0.5223214228947958, val_loss: 0.5528484582901001, val_acc: 1.0
epoch: 264: train_loss: 0.7340123931192, train_acc: 0.5699404776096344, val_loss: 0.5973049998283386, val_acc: 0.9375
epoch: 265: train_loss: 0.7338168965022365, train_acc: 0.517857144276301, val_loss: 0.5701347887516022, val_acc: 1.0
epoch: 266: train_loss: 0.7334274111987646, train_acc: 0.5892857114473978, val_loss: 0.5739721953868866, val_acc: 0.984375
epoch: 267: train_loss: 0.7330043462675007, train_acc: 0.5714285771052042, val_loss: 0.551805704832077, val_acc: 1.0
epoch: 268: train_loss: 0.732822362522801, train_acc: 0.5148809552192688, val_loss: 0.5539517402648926, val_acc: 1.0
epoch: 269: train_loss: 0.7325155915301521, train_acc: 0.586309532324473, val_loss: 0.57396599650383, val_acc: 0.96875
epoch: 270: train_loss: 0.7323721590077303, train_acc: 0.5297619104385376, val_loss: 0.5609801411628723, val_acc: 1.0
epoch: 271: train_loss: 0.731991642392149, train_acc: 0.569940467675527, val_loss: 0.5527356564998627, val_acc: 1.0
epoch: 272: train_loss: 0.731939705373021, train_acc: 0.4642857114473979, val_loss: 0.552109956741333, val_acc: 1.0
epoch: 273: train_loss: 0.7317960420374161, train_acc: 0.4940476218859355, val_loss: 0.6161991953849792, val_acc: 0.9375
epoch: 274: train_loss: 0.7318335299058393, train_acc: 0.4255952338377635, val_loss: 0.5904108285903931, val_acc: 0.984375
epoch: 275: train_loss: 0.7315749136578056, train_acc: 0.5565476218859354, val_loss: 0.5565959811210632, val_acc: 1.0
epoch: 276: train_loss: 0.731756486330365, train_acc: 0.4375, val_loss: 0.5691109001636505, val_acc: 0.984375
epoch: 277: train_loss: 0.7313295431988986, train_acc: 0.5997023781140646, val_loss: 0.5557029545307159, val_acc: 1.0
epoch: 278: train_loss: 0.7311117601366167, train_acc: 0.53125, val_loss: 0.565876305103302, val_acc: 0.984375
epoch: 279: train_loss: 0.7311259725973718, train_acc: 0.4970238109429677, val_loss: 0.561210423707962, val_acc: 1.0
epoch: 280: train_loss: 0.73101613339156, train_acc: 0.4985119005044301, val_loss: 0.5764248073101044, val_acc: 0.984375
epoch: 281: train_loss: 0.7308276082888843, train_acc: 0.53125, val_loss: 0.5967343747615814, val_acc: 0.9375
epoch: 282: train_loss: 0.7305964144436011, train_acc: 0.538690467675527, val_loss: 0.5635862648487091, val_acc: 1.0
epoch: 283: train_loss: 0.730705387085816, train_acc: 0.4345238109429677, val_loss: 0.6030522286891937, val_acc: 0.921875
epoch: 284: train_loss: 0.730223424462547, train_acc: 0.644345243771871, val_loss: 0.555812269449234, val_acc: 1.0
epoch: 285: train_loss: 0.7301399198584344, train_acc: 0.5208333333333334, val_loss: 0.5629549622535706, val_acc: 0.984375
epoch: 286: train_loss: 0.7302031734402309, train_acc: 0.4360119005044301, val_loss: 0.5517817735671997, val_acc: 1.0
epoch: 287: train_loss: 0.7300022522470464, train_acc: 0.5089285671710968, val_loss: 0.5586093068122864, val_acc: 1.0
epoch: 288: train_loss: 0.7298882686143101, train_acc: 0.46875, val_loss: 0.5821035802364349, val_acc: 0.96875
epoch: 289: train_loss: 0.7294885130449271, train_acc: 0.6205357114473978, val_loss: 0.5783207714557648, val_acc: 0.96875
epoch: 290: train_loss: 0.7294001635270702, train_acc: 0.4985119005044301, val_loss: 0.5754672288894653, val_acc: 0.96875
epoch: 291: train_loss: 0.7292612333139871, train_acc: 0.543154756228129, val_loss: 0.5548256933689117, val_acc: 1.0
epoch: 292: train_loss: 0.7290520864677645, train_acc: 0.5327380895614624, val_loss: 0.5642680823802948, val_acc: 0.984375
epoch: 293: train_loss: 0.7286094808254111, train_acc: 0.5922619005044302, val_loss: 0.5520165860652924, val_acc: 1.0
epoch: 294: train_loss: 0.7285603279447824, train_acc: 0.4747023781140645, val_loss: 0.5518910586833954, val_acc: 1.0
epoch: 295: train_loss: 0.7284071060182812, train_acc: 0.5148809552192688, val_loss: 0.5722505748271942, val_acc: 0.96875
epoch: 296: train_loss: 0.7281461010878335, train_acc: 0.5952380895614624, val_loss: 0.5691733062267303, val_acc: 0.984375
epoch: 297: train_loss: 0.7280238514111879, train_acc: 0.4866071442763011, val_loss: 0.5671131312847137, val_acc: 0.984375
epoch: 298: train_loss: 0.728073890857739, train_acc: 0.4464285671710968, val_loss: 0.5595560073852539, val_acc: 0.984375
epoch: 299: train_loss: 0.7277331203222274, train_acc: 0.5625, val_loss: 0.5583506226539612, val_acc: 1.0
epoch: 300: train_loss: 0.7275303221356167, train_acc: 0.5357142885526022, val_loss: 0.5536210834980011, val_acc: 1.0
epoch: 301: train_loss: 0.7273038717163581, train_acc: 0.5654761989911398, val_loss: 0.5722076594829559, val_acc: 0.984375
epoch: 302: train_loss: 0.7268844221947084, train_acc: 0.5922619104385376, val_loss: 0.5755416452884674, val_acc: 0.984375
epoch: 303: train_loss: 0.7266442424111198, train_acc: 0.5491071343421936, val_loss: 0.611017495393753, val_acc: 0.9375
epoch: 304: train_loss: 0.7262922965112278, train_acc: 0.5580357114473978, val_loss: 0.5523746311664581, val_acc: 1.0
epoch: 305: train_loss: 0.7260893028156429, train_acc: 0.5386904776096344, val_loss: 0.5514847934246063, val_acc: 1.0
epoch: 306: train_loss: 0.7258727699356409, train_acc: 0.5342261989911398, val_loss: 0.5524260699748993, val_acc: 1.0
epoch: 307: train_loss: 0.7257382969384069, train_acc: 0.53125, val_loss: 0.5671806037425995, val_acc: 0.984375
epoch: 308: train_loss: 0.7253956329539627, train_acc: 0.605654756228129, val_loss: 0.5740387737751007, val_acc: 0.984375
epoch: 309: train_loss: 0.7253836012335233, train_acc: 0.4613095223903656, val_loss: 0.5561389327049255, val_acc: 1.0
epoch: 310: train_loss: 0.7251885524069509, train_acc: 0.5565476218859354, val_loss: 0.552710771560669, val_acc: 1.0
epoch: 311: train_loss: 0.7248738565697119, train_acc: 0.601190467675527, val_loss: 0.552295058965683, val_acc: 1.0
epoch: 312: train_loss: 0.724671177050303, train_acc: 0.5625, val_loss: 0.6156970858573914, val_acc: 0.9375
epoch: 313: train_loss: 0.7244046947062648, train_acc: 0.5684523781140646, val_loss: 0.5541049540042877, val_acc: 1.0
epoch: 314: train_loss: 0.7241745767454622, train_acc: 0.555059532324473, val_loss: 0.552423894405365, val_acc: 1.0
epoch: 315: train_loss: 0.7239114101111637, train_acc: 0.543154756228129, val_loss: 0.5652135908603668, val_acc: 0.984375
epoch: 316: train_loss: 0.7236896715392324, train_acc: 0.5520833333333334, val_loss: 0.5523858666419983, val_acc: 1.0
epoch: 317: train_loss: 0.7236775509403935, train_acc: 0.4523809552192688, val_loss: 0.5883805155754089, val_acc: 0.984375
epoch: 318: train_loss: 0.7237505497665863, train_acc: 0.4672619005044301, val_loss: 0.6385167241096497, val_acc: 0.90625
epoch: 319: train_loss: 0.7235973246706029, train_acc: 0.5357142885526022, val_loss: 0.6084692478179932, val_acc: 0.9375
epoch: 320: train_loss: 0.7234671387283725, train_acc: 0.53125, val_loss: 0.6259702146053314, val_acc: 0.921875
epoch: 321: train_loss: 0.723544828547454, train_acc: 0.53125, val_loss: 0.5760098099708557, val_acc: 0.96875
epoch: 322: train_loss: 0.723395320806956, train_acc: 0.5625, val_loss: 0.5673326253890991, val_acc: 0.984375
epoch: 323: train_loss: 0.7233071488792023, train_acc: 0.5193452338377634, val_loss: 0.6564857661724091, val_acc: 0.890625
epoch: 324: train_loss: 0.7233666532773237, train_acc: 0.4895833333333333, val_loss: 0.5791772603988647, val_acc: 0.96875
epoch: 325: train_loss: 0.7231971815015154, train_acc: 0.5446428656578064, val_loss: 0.5931120216846466, val_acc: 0.953125
epoch: 326: train_loss: 0.7228825686843146, train_acc: 0.5982142885526022, val_loss: 0.5998591482639313, val_acc: 0.953125
epoch: 327: train_loss: 0.7226953624285818, train_acc: 0.5416666666666666, val_loss: 0.5580322444438934, val_acc: 1.0
epoch: 328: train_loss: 0.7227491178713068, train_acc: 0.4404761989911397, val_loss: 0.5983803272247314, val_acc: 0.9375
epoch: 329: train_loss: 0.722471979200238, train_acc: 0.5758928656578064, val_loss: 0.5753944516181946, val_acc: 0.984375
epoch: 330: train_loss: 0.72217432114653, train_acc: 0.5982142885526022, val_loss: 0.5640658736228943, val_acc: 0.984375
epoch: 331: train_loss: 0.7220207873776735, train_acc: 0.5029761989911398, val_loss: 0.572880357503891, val_acc: 0.984375
epoch: 332: train_loss: 0.7218516957950784, train_acc: 0.538690467675527, val_loss: 0.6206183731555939, val_acc: 0.953125
epoch: 333: train_loss: 0.721957833734815, train_acc: 0.4776785671710968, val_loss: 0.5643168687820435, val_acc: 1.0
epoch: 334: train_loss: 0.7216860675396611, train_acc: 0.5669642885526022, val_loss: 0.5584286451339722, val_acc: 1.0
epoch: 335: train_loss: 0.7215285815357689, train_acc: 0.5580357114473978, val_loss: 0.6293691396713257, val_acc: 0.90625
epoch: 336: train_loss: 0.721404759425202, train_acc: 0.5297619005044302, val_loss: 0.58193439245224, val_acc: 0.96875
epoch: 337: train_loss: 0.7214386095485744, train_acc: 0.4479166666666667, val_loss: 0.5530569851398468, val_acc: 1.0
epoch: 338: train_loss: 0.7213809414892188, train_acc: 0.4791666666666667, val_loss: 0.5575571656227112, val_acc: 1.0
epoch: 339: train_loss: 0.7213617779752788, train_acc: 0.5148809552192688, val_loss: 0.5806319415569305, val_acc: 0.96875
epoch: 340: train_loss: 0.7211702483781739, train_acc: 0.5386904776096344, val_loss: 0.5613434612751007, val_acc: 1.0
epoch: 341: train_loss: 0.7212110538522171, train_acc: 0.4583333333333333, val_loss: 0.5804355144500732, val_acc: 0.96875
epoch: 342: train_loss: 0.7211588382199625, train_acc: 0.5, val_loss: 0.5564266443252563, val_acc: 1.0
epoch: 343: train_loss: 0.720968764737364, train_acc: 0.5580357114473978, val_loss: 0.5565356910228729, val_acc: 1.0
epoch: 344: train_loss: 0.7205960515328652, train_acc: 0.6086309552192688, val_loss: 0.5526273250579834, val_acc: 1.0
epoch: 345: train_loss: 0.7203558038826853, train_acc: 0.5848214228947958, val_loss: 0.5676094591617584, val_acc: 0.96875
epoch: 346: train_loss: 0.7200096239274104, train_acc: 0.601190467675527, val_loss: 0.5666977167129517, val_acc: 0.984375
epoch: 347: train_loss: 0.7198854367395013, train_acc: 0.5029761989911398, val_loss: 0.5828292667865753, val_acc: 0.984375
epoch: 348: train_loss: 0.7196798000659049, train_acc: 0.5788690447807312, val_loss: 0.5771135985851288, val_acc: 0.96875
epoch: 349: train_loss: 0.7196894099598838, train_acc: 0.507440467675527, val_loss: 0.5674488246440887, val_acc: 0.984375
epoch: 350: train_loss: 0.7195973810080902, train_acc: 0.5342261989911398, val_loss: 0.5564954578876495, val_acc: 1.0
epoch: 351: train_loss: 0.71931256336922, train_acc: 0.5997023781140646, val_loss: 0.5597100257873535, val_acc: 0.984375
epoch: 352: train_loss: 0.7191490457235811, train_acc: 0.5089285671710968, val_loss: 0.5567719340324402, val_acc: 1.0
epoch: 353: train_loss: 0.7192880833440806, train_acc: 0.4122023781140645, val_loss: 0.56629878282547, val_acc: 0.984375
epoch: 354: train_loss: 0.7192333831473695, train_acc: 0.5089285671710968, val_loss: 0.5990869104862213, val_acc: 0.953125
epoch: 355: train_loss: 0.7192314951384112, train_acc: 0.5342261989911398, val_loss: 0.5816746652126312, val_acc: 0.96875
epoch: 356: train_loss: 0.7190673357409701, train_acc: 0.5535714228947958, val_loss: 0.5675844550132751, val_acc: 0.984375
epoch: 357: train_loss: 0.7189604766542019, train_acc: 0.4970238109429677, val_loss: 0.5530417263507843, val_acc: 1.0
epoch: 358: train_loss: 0.71882514009458, train_acc: 0.5297619005044302, val_loss: 0.5715487897396088, val_acc: 0.984375
epoch: 359: train_loss: 0.7189320153108348, train_acc: 0.4761904776096344, val_loss: 0.5527483224868774, val_acc: 1.0
epoch: 360: train_loss: 0.7186568106970957, train_acc: 0.605654756228129, val_loss: 0.5957062244415283, val_acc: 0.96875
epoch: 361: train_loss: 0.7187193579985509, train_acc: 0.4285714228947957, val_loss: 0.5554563105106354, val_acc: 1.0
epoch: 362: train_loss: 0.7185507046180212, train_acc: 0.5446428656578064, val_loss: 0.5533373951911926, val_acc: 1.0
epoch: 363: train_loss: 0.7183695646760226, train_acc: 0.5580357114473978, val_loss: 0.555230975151062, val_acc: 1.0
epoch: 364: train_loss: 0.7183739694830488, train_acc: 0.4866071343421936, val_loss: 0.5582372546195984, val_acc: 1.0
epoch: 365: train_loss: 0.718442095813421, train_acc: 0.4136904776096344, val_loss: 0.5982998013496399, val_acc: 0.953125
epoch: 366: train_loss: 0.7181369078689871, train_acc: 0.5952380895614624, val_loss: 0.6245893836021423, val_acc: 0.9375
epoch: 367: train_loss: 0.7180331853096896, train_acc: 0.5223214228947958, val_loss: 0.6283421516418457, val_acc: 0.9375
epoch: 368: train_loss: 0.7178513765765827, train_acc: 0.550595243771871, val_loss: 0.552957683801651, val_acc: 1.0
epoch: 369: train_loss: 0.7176365538760346, train_acc: 0.543154756228129, val_loss: 0.5574364960193634, val_acc: 1.0
epoch: 370: train_loss: 0.7174823338237839, train_acc: 0.5639880895614624, val_loss: 0.5520754754543304, val_acc: 1.0
epoch: 371: train_loss: 0.7173457858703467, train_acc: 0.523809532324473, val_loss: 0.5518947839736938, val_acc: 1.0
epoch: 372: train_loss: 0.717094030307807, train_acc: 0.5967261989911398, val_loss: 0.5713644027709961, val_acc: 1.0
epoch: 373: train_loss: 0.7169432335677628, train_acc: 0.5327380895614624, val_loss: 0.5598333477973938, val_acc: 1.0
epoch: 374: train_loss: 0.7168125156296621, train_acc: 0.5669642885526022, val_loss: 0.6151120960712433, val_acc: 0.9375
epoch: 375: train_loss: 0.7165500078323882, train_acc: 0.586309532324473, val_loss: 0.5704334676265717, val_acc: 0.984375
epoch: 376: train_loss: 0.7165914594231187, train_acc: 0.4866071442763011, val_loss: 0.5706799626350403, val_acc: 0.984375
epoch: 377: train_loss: 0.7164930743830542, train_acc: 0.5193452338377634, val_loss: 0.5517444610595703, val_acc: 1.0
epoch: 378: train_loss: 0.7164035947991756, train_acc: 0.5372023781140646, val_loss: 0.5520095229148865, val_acc: 1.0
epoch: 379: train_loss: 0.7162551868903007, train_acc: 0.5446428656578064, val_loss: 0.6082920432090759, val_acc: 0.9375
epoch: 380: train_loss: 0.7160877760626823, train_acc: 0.5193452338377634, val_loss: 0.5607610642910004, val_acc: 0.984375
epoch: 381: train_loss: 0.7161743266420212, train_acc: 0.4479166666666667, val_loss: 0.5521106421947479, val_acc: 1.0
epoch: 382: train_loss: 0.7162483627117642, train_acc: 0.4464285671710968, val_loss: 0.5519174337387085, val_acc: 1.0
epoch: 383: train_loss: 0.7161670197318822, train_acc: 0.5104166666666666, val_loss: 0.5553088486194611, val_acc: 1.0
epoch: 384: train_loss: 0.7159808351363968, train_acc: 0.53125, val_loss: 0.5608988702297211, val_acc: 0.984375
epoch: 385: train_loss: 0.7159778433039601, train_acc: 0.4970238109429677, val_loss: 0.624244213104248, val_acc: 0.90625
epoch: 386: train_loss: 0.7158103851692926, train_acc: 0.5610119104385376, val_loss: 0.5803734064102173, val_acc: 0.96875
epoch: 387: train_loss: 0.7160393904984199, train_acc: 0.4300595223903656, val_loss: 0.6705731749534607, val_acc: 0.875
epoch: 388: train_loss: 0.7161018166055815, train_acc: 0.511904756228129, val_loss: 0.6361072361469269, val_acc: 0.921875
epoch: 389: train_loss: 0.7159332384411083, train_acc: 0.555059532324473, val_loss: 0.5582476854324341, val_acc: 1.0
epoch: 390: train_loss: 0.7157347272932272, train_acc: 0.5565476218859354, val_loss: 0.5525097250938416, val_acc: 1.0
epoch: 391: train_loss: 0.7155257389963074, train_acc: 0.586309532324473, val_loss: 0.5521992743015289, val_acc: 1.0
epoch: 392: train_loss: 0.7154634804721599, train_acc: 0.5446428656578064, val_loss: 0.5631222426891327, val_acc: 1.0
epoch: 393: train_loss: 0.7152357995005828, train_acc: 0.601190467675527, val_loss: 0.6024777591228485, val_acc: 0.9375
epoch: 394: train_loss: 0.7151641586661839, train_acc: 0.4925595323244731, val_loss: 0.5975308418273926, val_acc: 0.953125
epoch: 395: train_loss: 0.7150280281951529, train_acc: 0.5223214228947958, val_loss: 0.5789151191711426, val_acc: 0.96875
epoch: 396: train_loss: 0.7151408022837311, train_acc: 0.4970238109429677, val_loss: 0.5691235661506653, val_acc: 0.984375
epoch: 397: train_loss: 0.7151341462754321, train_acc: 0.5014880895614624, val_loss: 0.7130434811115265, val_acc: 0.828125
epoch: 398: train_loss: 0.7155460516113781, train_acc: 0.4434523781140645, val_loss: 0.6913580000400543, val_acc: 0.859375
epoch: 399: train_loss: 0.7154144159952796, train_acc: 0.6071428656578064, val_loss: 0.5524612367153168, val_acc: 1.0
epoch: 400: train_loss: 0.7153243935811744, train_acc: 0.5327380895614624, val_loss: 0.5674891769886017, val_acc: 0.984375
epoch: 401: train_loss: 0.7150403116868301, train_acc: 0.6532738010088602, val_loss: 0.5839136838912964, val_acc: 0.96875
epoch: 402: train_loss: 0.7149219315243316, train_acc: 0.5386904776096344, val_loss: 0.6377643644809723, val_acc: 0.921875
epoch: 403: train_loss: 0.7148655529561213, train_acc: 0.5074404776096344, val_loss: 0.5686998069286346, val_acc: 0.984375
epoch: 404: train_loss: 0.7147239665926236, train_acc: 0.5386904776096344, val_loss: 0.5788399577140808, val_acc: 0.96875
epoch: 405: train_loss: 0.714722541556961, train_acc: 0.46875, val_loss: 0.6050274968147278, val_acc: 0.9375
epoch: 406: train_loss: 0.7145524717177842, train_acc: 0.555059532324473, val_loss: 0.5672840476036072, val_acc: 0.984375
epoch: 407: train_loss: 0.7144106742895504, train_acc: 0.5625, val_loss: 0.5643405318260193, val_acc: 0.984375
epoch: 408: train_loss: 0.7144095696077757, train_acc: 0.4642857114473979, val_loss: 0.6400136053562164, val_acc: 0.90625
epoch: 409: train_loss: 0.7143801536017315, train_acc: 0.4836309552192688, val_loss: 0.598765105009079, val_acc: 0.953125
epoch: 410: train_loss: 0.7144391039282153, train_acc: 0.4836309552192688, val_loss: 0.6017457842826843, val_acc: 0.953125
epoch: 411: train_loss: 0.7144929753634534, train_acc: 0.4345238109429677, val_loss: 0.551737517118454, val_acc: 1.0
epoch: 412: train_loss: 0.7144300200171387, train_acc: 0.5, val_loss: 0.555252730846405, val_acc: 1.0
epoch: 413: train_loss: 0.7142690974925064, train_acc: 0.5372023781140646, val_loss: 0.552722156047821, val_acc: 1.0
epoch: 414: train_loss: 0.7142348808935843, train_acc: 0.507440467675527, val_loss: 0.588074117898941, val_acc: 0.921875
epoch: 415: train_loss: 0.7143509398954797, train_acc: 0.4315476218859355, val_loss: 0.5823251008987427, val_acc: 0.96875
epoch: 416: train_loss: 0.7142399229305821, train_acc: 0.5476190447807312, val_loss: 0.6336015462875366, val_acc: 0.921875
epoch: 417: train_loss: 0.714104184883823, train_acc: 0.5476190447807312, val_loss: 0.5582992434501648, val_acc: 1.0
epoch: 418: train_loss: 0.7139820190963042, train_acc: 0.5595238109429678, val_loss: 0.6148272454738617, val_acc: 0.9375
epoch: 419: train_loss: 0.7137899369001385, train_acc: 0.5639880895614624, val_loss: 0.551705539226532, val_acc: 1.0
epoch: 420: train_loss: 0.7136078383463857, train_acc: 0.5684523781140646, val_loss: 0.5666865706443787, val_acc: 0.984375
epoch: 421: train_loss: 0.7133815320088019, train_acc: 0.601190467675527, val_loss: 0.668694943189621, val_acc: 0.875
epoch: 422: train_loss: 0.7133251543210373, train_acc: 0.5193452338377634, val_loss: 0.5562833547592163, val_acc: 1.0
epoch: 423: train_loss: 0.7131289813034936, train_acc: 0.5461309552192688, val_loss: 0.5681436657905579, val_acc: 0.984375
epoch: 424: train_loss: 0.7130644410264254, train_acc: 0.5133928656578064, val_loss: 0.551693469285965, val_acc: 1.0
epoch: 425: train_loss: 0.7130100583433168, train_acc: 0.5104166666666666, val_loss: 0.5514552891254425, val_acc: 1.0
epoch: 426: train_loss: 0.7129191519411903, train_acc: 0.5163690447807312, val_loss: 0.5608652830123901, val_acc: 0.984375
epoch: 427: train_loss: 0.7128227544350783, train_acc: 0.5297619005044302, val_loss: 0.5749634504318237, val_acc: 1.0
epoch: 428: train_loss: 0.7126850555734104, train_acc: 0.5505952338377634, val_loss: 0.5632739365100861, val_acc: 0.984375
epoch: 429: train_loss: 0.7126134726890294, train_acc: 0.53125, val_loss: 0.6118055880069733, val_acc: 0.921875
epoch: 430: train_loss: 0.7125229096099188, train_acc: 0.5282738010088602, val_loss: 0.5682497918605804, val_acc: 0.984375
epoch: 431: train_loss: 0.7124161670605338, train_acc: 0.5267857114473978, val_loss: 0.5526592433452606, val_acc: 1.0
epoch: 432: train_loss: 0.7122718071460352, train_acc: 0.5327380895614624, val_loss: 0.5840509831905365, val_acc: 0.96875
epoch: 433: train_loss: 0.7120156912118787, train_acc: 0.6041666666666666, val_loss: 0.6294320821762085, val_acc: 0.921875
epoch: 434: train_loss: 0.7119215040371332, train_acc: 0.4985119005044301, val_loss: 0.5723091661930084, val_acc: 0.984375
epoch: 435: train_loss: 0.7117227757469226, train_acc: 0.586309532324473, val_loss: 0.5514907538890839, val_acc: 1.0
epoch: 436: train_loss: 0.7116369405927588, train_acc: 0.543154756228129, val_loss: 0.5528025031089783, val_acc: 1.0
epoch: 437: train_loss: 0.7115164691273296, train_acc: 0.543154756228129, val_loss: 0.55167356133461, val_acc: 1.0
epoch: 438: train_loss: 0.7114006831687599, train_acc: 0.5535714228947958, val_loss: 0.580739438533783, val_acc: 0.953125
epoch: 439: train_loss: 0.7114381544969295, train_acc: 0.4627976218859355, val_loss: 0.5546723306179047, val_acc: 1.0
epoch: 440: train_loss: 0.7114152982211523, train_acc: 0.4955357114473979, val_loss: 0.6262927353382111, val_acc: 0.921875
epoch: 441: train_loss: 0.711447831462411, train_acc: 0.4732142885526021, val_loss: 0.5804180800914764, val_acc: 0.96875
epoch: 442: train_loss: 0.7113355615696929, train_acc: 0.5252976218859354, val_loss: 0.5674638748168945, val_acc: 0.984375
epoch: 443: train_loss: 0.7113393842279013, train_acc: 0.5327380895614624, val_loss: 0.5979501307010651, val_acc: 0.9375
epoch: 444: train_loss: 0.7112523683001484, train_acc: 0.5297619005044302, val_loss: 0.5525667071342468, val_acc: 1.0
epoch: 445: train_loss: 0.7112893810097587, train_acc: 0.5059523781140646, val_loss: 0.5729871094226837, val_acc: 0.96875
epoch: 446: train_loss: 0.7110891455505249, train_acc: 0.5997023781140646, val_loss: 0.568146288394928, val_acc: 0.984375
epoch: 447: train_loss: 0.710993163331988, train_acc: 0.511904756228129, val_loss: 0.5819810628890991, val_acc: 0.96875
epoch: 448: train_loss: 0.7108711227276806, train_acc: 0.574404756228129, val_loss: 0.610054612159729, val_acc: 0.9375
epoch: 449: train_loss: 0.7107113907513792, train_acc: 0.569940467675527, val_loss: 0.5829643607139587, val_acc: 0.984375
epoch: 450: train_loss: 0.7107076189822823, train_acc: 0.4747023781140645, val_loss: 0.5848904848098755, val_acc: 0.984375
epoch: 451: train_loss: 0.7104912660466531, train_acc: 0.5982142885526022, val_loss: 0.5673049688339233, val_acc: 0.984375
epoch: 452: train_loss: 0.7102614580225292, train_acc: 0.5848214228947958, val_loss: 0.5610060393810272, val_acc: 0.984375
epoch: 453: train_loss: 0.710212884451674, train_acc: 0.5342261989911398, val_loss: 0.5573854446411133, val_acc: 1.0
epoch: 454: train_loss: 0.7101168735996704, train_acc: 0.5252976218859354, val_loss: 0.5515814423561096, val_acc: 1.0
epoch: 455: train_loss: 0.7100016523958643, train_acc: 0.5252976218859354, val_loss: 0.5521928071975708, val_acc: 1.0
epoch: 456: train_loss: 0.7098079141172327, train_acc: 0.5669642885526022, val_loss: 0.5521830320358276, val_acc: 1.0
epoch: 457: train_loss: 0.7095852678823328, train_acc: 0.5952380895614624, val_loss: 0.5690070688724518, val_acc: 0.984375
epoch: 458: train_loss: 0.7095393380265173, train_acc: 0.517857144276301, val_loss: 0.6385860443115234, val_acc: 0.90625
epoch: 459: train_loss: 0.7094213884161862, train_acc: 0.5595238109429678, val_loss: 0.630265086889267, val_acc: 0.921875
epoch: 460: train_loss: 0.7095836503343653, train_acc: 0.4002976218859355, val_loss: 0.5688194036483765, val_acc: 1.0
epoch: 461: train_loss: 0.7095523887511451, train_acc: 0.4910714228947957, val_loss: 0.6328224539756775, val_acc: 0.921875
epoch: 462: train_loss: 0.7093144348192931, train_acc: 0.6160714228947958, val_loss: 0.5762730240821838, val_acc: 0.96875
epoch: 463: train_loss: 0.7092487866208809, train_acc: 0.5223214228947958, val_loss: 0.574742466211319, val_acc: 0.96875
epoch: 464: train_loss: 0.7093108697390467, train_acc: 0.4404761890570323, val_loss: 0.5515901148319244, val_acc: 1.0
epoch: 465: train_loss: 0.7093235517989237, train_acc: 0.4672619005044301, val_loss: 0.5514636635780334, val_acc: 1.0
epoch: 466: train_loss: 0.7092278140472732, train_acc: 0.5089285671710968, val_loss: 0.5726195275783539, val_acc: 0.984375
epoch: 467: train_loss: 0.7091080113566155, train_acc: 0.5535714228947958, val_loss: 0.560147762298584, val_acc: 0.984375
epoch: 468: train_loss: 0.7090064109896795, train_acc: 0.53125, val_loss: 0.5799202620983124, val_acc: 0.96875
epoch: 469: train_loss: 0.7088865211035338, train_acc: 0.5580357114473978, val_loss: 0.6420494914054871, val_acc: 0.90625
epoch: 470: train_loss: 0.7090894070527098, train_acc: 0.4211309552192688, val_loss: 0.5716771185398102, val_acc: 0.984375
epoch: 471: train_loss: 0.7088882581115101, train_acc: 0.605654756228129, val_loss: 0.5848103165626526, val_acc: 0.96875
epoch: 472: train_loss: 0.7086775042877973, train_acc: 0.6041666666666666, val_loss: 0.5589000284671783, val_acc: 1.0
epoch: 473: train_loss: 0.7086967066789642, train_acc: 0.4717261989911397, val_loss: 0.5660540461540222, val_acc: 0.984375
epoch: 474: train_loss: 0.708480814423477, train_acc: 0.648809532324473, val_loss: 0.568035215139389, val_acc: 0.984375
epoch: 475: train_loss: 0.708255712600315, train_acc: 0.5997023781140646, val_loss: 0.5517375767230988, val_acc: 1.0
epoch: 476: train_loss: 0.7081709502877925, train_acc: 0.5223214228947958, val_loss: 0.585143506526947, val_acc: 0.96875
epoch: 477: train_loss: 0.7080389199729076, train_acc: 0.5773809552192688, val_loss: 0.5851440727710724, val_acc: 0.96875
epoch: 478: train_loss: 0.707973867177797, train_acc: 0.5133928656578064, val_loss: 0.6312746107578278, val_acc: 0.921875
epoch: 479: train_loss: 0.7080224783056309, train_acc: 0.4494047661622365, val_loss: 0.5558790862560272, val_acc: 1.0
epoch: 480: train_loss: 0.7079629425943018, train_acc: 0.5223214228947958, val_loss: 0.5717343389987946, val_acc: 0.984375
epoch: 481: train_loss: 0.7079724486850271, train_acc: 0.4925595323244731, val_loss: 0.5664273798465729, val_acc: 0.984375
epoch: 482: train_loss: 0.7081436006836923, train_acc: 0.4226190447807312, val_loss: 0.5751214325428009, val_acc: 0.96875
epoch: 483: train_loss: 0.7079019564570799, train_acc: 0.6220238010088602, val_loss: 0.5625261068344116, val_acc: 0.984375
epoch: 484: train_loss: 0.7079036271449212, train_acc: 0.5, val_loss: 0.558454304933548, val_acc: 1.0
epoch: 485: train_loss: 0.707866036270544, train_acc: 0.517857144276301, val_loss: 0.554300993680954, val_acc: 1.0
epoch: 486: train_loss: 0.7078238266694223, train_acc: 0.5, val_loss: 0.5553157925605774, val_acc: 1.0
epoch: 487: train_loss: 0.7077238454681923, train_acc: 0.5714285771052042, val_loss: 0.5515257716178894, val_acc: 1.0
epoch: 488: train_loss: 0.7078174610648251, train_acc: 0.4226190447807312, val_loss: 0.5670861601829529, val_acc: 0.984375
epoch: 489: train_loss: 0.7077966758183066, train_acc: 0.5223214228947958, val_loss: 0.5594436824321747, val_acc: 1.0
epoch: 490: train_loss: 0.7076348032339522, train_acc: 0.5788690447807312, val_loss: 0.6150100827217102, val_acc: 0.9375
epoch: 491: train_loss: 0.7076021091240203, train_acc: 0.5223214228947958, val_loss: 0.6140709519386292, val_acc: 0.921875
epoch: 492: train_loss: 0.7076289262861876, train_acc: 0.4553571442763011, val_loss: 0.6231769323348999, val_acc: 0.921875
epoch: 493: train_loss: 0.7076631899867781, train_acc: 0.46875, val_loss: 0.5515410006046295, val_acc: 1.0
epoch: 494: train_loss: 0.7077802172413574, train_acc: 0.4196428557236989, val_loss: 0.5523136556148529, val_acc: 1.0
epoch: 495: train_loss: 0.7077578783676185, train_acc: 0.4925595323244731, val_loss: 0.55507493019104, val_acc: 1.0
epoch: 496: train_loss: 0.7076701298006259, train_acc: 0.5104166666666666, val_loss: 0.589201420545578, val_acc: 0.9375
epoch: 497: train_loss: 0.7078059084284573, train_acc: 0.4107142885526021, val_loss: 0.6269316673278809, val_acc: 0.921875
epoch: 498: train_loss: 0.7078012147981796, train_acc: 0.4910714228947957, val_loss: 0.5595537424087524, val_acc: 1.0
epoch: 499: train_loss: 0.7077161366542176, train_acc: 0.5461309552192688, val_loss: 0.5916443467140198, val_acc: 0.9375
epoch: 500: train_loss: 0.7076198341286504, train_acc: 0.523809532324473, val_loss: 0.6313169002532959, val_acc: 0.9375
epoch: 501: train_loss: 0.7074670154022502, train_acc: 0.5654761989911398, val_loss: 0.5795005559921265, val_acc: 0.984375
epoch: 502: train_loss: 0.7073718466610207, train_acc: 0.5223214228947958, val_loss: 0.605605959892273, val_acc: 0.9375
epoch: 503: train_loss: 0.7074125472199977, train_acc: 0.4553571442763011, val_loss: 0.5664826035499573, val_acc: 0.984375
epoch: 504: train_loss: 0.7074052551792003, train_acc: 0.4821428656578064, val_loss: 0.5669465959072113, val_acc: 0.984375
epoch: 505: train_loss: 0.7073278961524031, train_acc: 0.5014880895614624, val_loss: 0.5515538156032562, val_acc: 1.0
epoch: 506: train_loss: 0.707255656980982, train_acc: 0.5401785771052042, val_loss: 0.5516241788864136, val_acc: 1.0
epoch: 507: train_loss: 0.7072007797677996, train_acc: 0.4985119005044301, val_loss: 0.5670191049575806, val_acc: 0.984375
epoch: 508: train_loss: 0.7071898045614797, train_acc: 0.5193452338377634, val_loss: 0.565585732460022, val_acc: 0.984375
epoch: 509: train_loss: 0.7071787249418642, train_acc: 0.5074404776096344, val_loss: 0.6151579320430756, val_acc: 0.9375
epoch: 510: train_loss: 0.7070442409353633, train_acc: 0.5476190447807312, val_loss: 0.5529773533344269, val_acc: 1.0
epoch: 511: train_loss: 0.7068926916690541, train_acc: 0.5684523781140646, val_loss: 0.5516313314437866, val_acc: 1.0
epoch: 512: train_loss: 0.7069398561338852, train_acc: 0.4627976218859355, val_loss: 0.5781759321689606, val_acc: 0.96875
epoch: 513: train_loss: 0.7069000194740664, train_acc: 0.5372023781140646, val_loss: 0.5585159063339233, val_acc: 1.0
epoch: 514: train_loss: 0.7069174754195227, train_acc: 0.4568452338377635, val_loss: 0.5725313425064087, val_acc: 0.96875
epoch: 515: train_loss: 0.7070356512331528, train_acc: 0.40625, val_loss: 0.5706903636455536, val_acc: 0.984375
epoch: 516: train_loss: 0.706997304559138, train_acc: 0.5223214228947958, val_loss: 0.5707946419715881, val_acc: 0.984375
epoch: 517: train_loss: 0.7069239788227251, train_acc: 0.5535714228947958, val_loss: 0.5584781765937805, val_acc: 1.0
epoch: 518: train_loss: 0.7069914629617099, train_acc: 0.4672619005044301, val_loss: 0.5525662302970886, val_acc: 1.0
epoch: 519: train_loss: 0.7068872051743359, train_acc: 0.5565476218859354, val_loss: 0.5696550905704498, val_acc: 1.0
epoch: 520: train_loss: 0.7069664694373604, train_acc: 0.4538690447807312, val_loss: 0.6022306382656097, val_acc: 0.921875
epoch: 521: train_loss: 0.706947900821118, train_acc: 0.4776785671710968, val_loss: 0.559344083070755, val_acc: 0.984375
epoch: 522: train_loss: 0.7066973890750706, train_acc: 0.6294642885526022, val_loss: 0.5732376575469971, val_acc: 0.984375
epoch: 523: train_loss: 0.7066091478401769, train_acc: 0.5654761989911398, val_loss: 0.5829356610774994, val_acc: 0.96875
epoch: 524: train_loss: 0.7066480598374018, train_acc: 0.4747023781140645, val_loss: 0.6041552424430847, val_acc: 0.9375
epoch: 525: train_loss: 0.7065033468456775, train_acc: 0.5625, val_loss: 0.6164797246456146, val_acc: 0.921875
epoch: 526: train_loss: 0.7063927333389634, train_acc: 0.5535714228947958, val_loss: 0.6127268075942993, val_acc: 0.921875
epoch: 527: train_loss: 0.706333199873416, train_acc: 0.5610119104385376, val_loss: 0.6793691217899323, val_acc: 0.859375
epoch: 528: train_loss: 0.7063574301573038, train_acc: 0.4910714228947957, val_loss: 0.6618088185787201, val_acc: 0.84375
epoch: 529: train_loss: 0.7063000348004155, train_acc: 0.5104166666666666, val_loss: 0.5566970109939575, val_acc: 1.0
epoch: 530: train_loss: 0.7062475811140535, train_acc: 0.5282738109429678, val_loss: 0.6371549367904663, val_acc: 0.90625
epoch: 531: train_loss: 0.7063211542845966, train_acc: 0.4568452338377635, val_loss: 0.5565944910049438, val_acc: 1.0
epoch: 532: train_loss: 0.7062087341201835, train_acc: 0.5580357114473978, val_loss: 0.5590548515319824, val_acc: 1.0
epoch: 533: train_loss: 0.7061968511261147, train_acc: 0.4642857114473979, val_loss: 0.6148456335067749, val_acc: 0.9375
epoch: 534: train_loss: 0.7061494707689849, train_acc: 0.517857144276301, val_loss: 0.5933407247066498, val_acc: 0.953125
epoch: 535: train_loss: 0.7061291907112396, train_acc: 0.5133928656578064, val_loss: 0.7238731682300568, val_acc: 0.8125
epoch: 536: train_loss: 0.7060725459913375, train_acc: 0.5163690447807312, val_loss: 0.5750903487205505, val_acc: 0.96875
epoch: 537: train_loss: 0.7060145018933873, train_acc: 0.5372023781140646, val_loss: 0.5580735504627228, val_acc: 1.0
epoch: 538: train_loss: 0.7059293745400068, train_acc: 0.5208333333333334, val_loss: 0.5577765703201294, val_acc: 1.0
epoch: 539: train_loss: 0.7059393461103791, train_acc: 0.4836309552192688, val_loss: 0.5514813959598541, val_acc: 1.0
epoch: 540: train_loss: 0.7058345403336331, train_acc: 0.5729166666666666, val_loss: 0.5518659055233002, val_acc: 1.0
epoch: 541: train_loss: 0.7058846876483414, train_acc: 0.4702380895614624, val_loss: 0.5526445806026459, val_acc: 1.0
epoch: 542: train_loss: 0.7057862995298746, train_acc: 0.5505952338377634, val_loss: 0.5515576004981995, val_acc: 1.0
epoch: 543: train_loss: 0.7059022213752362, train_acc: 0.4166666666666667, val_loss: 0.5749099254608154, val_acc: 0.984375
epoch: 544: train_loss: 0.7057854323941267, train_acc: 0.5520833333333334, val_loss: 0.551885575056076, val_acc: 1.0
epoch: 545: train_loss: 0.7057372946587819, train_acc: 0.543154756228129, val_loss: 0.5680297613143921, val_acc: 1.0
epoch: 546: train_loss: 0.7057990199323835, train_acc: 0.4300595223903656, val_loss: 0.5567394495010376, val_acc: 1.0
epoch: 547: train_loss: 0.7058813170214927, train_acc: 0.4270833333333333, val_loss: 0.6111775636672974, val_acc: 0.9375
epoch: 548: train_loss: 0.7059144105922834, train_acc: 0.4791666666666667, val_loss: 0.5923542976379395, val_acc: 0.921875
epoch: 549: train_loss: 0.7057759303215777, train_acc: 0.5833333333333334, val_loss: 0.5517378747463226, val_acc: 1.0
epoch: 550: train_loss: 0.7057469021495153, train_acc: 0.5, val_loss: 0.557179719209671, val_acc: 1.0
epoch: 551: train_loss: 0.7058125468879793, train_acc: 0.4464285671710968, val_loss: 0.5536787509918213, val_acc: 1.0
epoch: 552: train_loss: 0.7056223860233931, train_acc: 0.601190467675527, val_loss: 0.5514785945415497, val_acc: 1.0
epoch: 553: train_loss: 0.7054506205156821, train_acc: 0.5907738010088602, val_loss: 0.5565730929374695, val_acc: 1.0
epoch: 554: train_loss: 0.7053874033170419, train_acc: 0.5193452338377634, val_loss: 0.5685382783412933, val_acc: 0.984375
epoch: 555: train_loss: 0.7053474172663917, train_acc: 0.5148809552192688, val_loss: 0.5534458756446838, val_acc: 1.0
epoch: 556: train_loss: 0.7053095925753163, train_acc: 0.5267857114473978, val_loss: 0.5519475340843201, val_acc: 1.0
epoch: 557: train_loss: 0.7052194873669954, train_acc: 0.5505952338377634, val_loss: 0.5698634386062622, val_acc: 1.0
epoch: 558: train_loss: 0.7050747961854396, train_acc: 0.5729166666666666, val_loss: 0.5530258119106293, val_acc: 1.0
epoch: 559: train_loss: 0.705118259060241, train_acc: 0.4613095223903656, val_loss: 0.5671145915985107, val_acc: 0.984375
epoch: 560: train_loss: 0.7050266734596101, train_acc: 0.517857144276301, val_loss: 0.5669377744197845, val_acc: 0.984375
epoch: 561: train_loss: 0.7051287634410892, train_acc: 0.4479166666666667, val_loss: 0.6233794987201691, val_acc: 0.921875
epoch: 562: train_loss: 0.7050012096877349, train_acc: 0.574404756228129, val_loss: 0.6134721636772156, val_acc: 0.875
epoch: 563: train_loss: 0.704949825843598, train_acc: 0.513392855723699, val_loss: 0.590981513261795, val_acc: 0.921875
epoch: 564: train_loss: 0.7049248303811458, train_acc: 0.4985119005044301, val_loss: 0.5518465638160706, val_acc: 1.0
epoch: 565: train_loss: 0.7048688526885107, train_acc: 0.5267857114473978, val_loss: 0.5670768916606903, val_acc: 0.984375
epoch: 566: train_loss: 0.7049072711240677, train_acc: 0.4657738109429677, val_loss: 0.5531767308712006, val_acc: 1.0
epoch: 567: train_loss: 0.7048284386769985, train_acc: 0.543154756228129, val_loss: 0.5567246377468109, val_acc: 1.0
epoch: 568: train_loss: 0.7047633670300996, train_acc: 0.511904756228129, val_loss: 0.5597590804100037, val_acc: 0.984375
epoch: 569: train_loss: 0.7045931310855854, train_acc: 0.5818452338377634, val_loss: 0.5514871180057526, val_acc: 1.0
epoch: 570: train_loss: 0.7045219810782853, train_acc: 0.5372023781140646, val_loss: 0.5532457530498505, val_acc: 1.0
epoch: 571: train_loss: 0.7043527447225608, train_acc: 0.6160714228947958, val_loss: 0.5877437889575958, val_acc: 0.9375
epoch: 572: train_loss: 0.7044027423255314, train_acc: 0.4464285671710968, val_loss: 0.551634669303894, val_acc: 1.0
epoch: 573: train_loss: 0.7043729059476054, train_acc: 0.5223214228947958, val_loss: 0.5555805563926697, val_acc: 1.0
epoch: 574: train_loss: 0.7041936035950978, train_acc: 0.601190467675527, val_loss: 0.5517902076244354, val_acc: 1.0
epoch: 575: train_loss: 0.704212777023376, train_acc: 0.46875, val_loss: 0.5525614321231842, val_acc: 1.0
epoch: 576: train_loss: 0.7041209067553195, train_acc: 0.53125, val_loss: 0.5667937695980072, val_acc: 0.984375
epoch: 577: train_loss: 0.7038546010869047, train_acc: 0.648809532324473, val_loss: 0.5519504547119141, val_acc: 1.0
epoch: 578: train_loss: 0.7038680004641181, train_acc: 0.4821428557236989, val_loss: 0.6218466758728027, val_acc: 0.921875
epoch: 579: train_loss: 0.7038256769378979, train_acc: 0.5223214228947958, val_loss: 0.5516982078552246, val_acc: 1.0
epoch: 580: train_loss: 0.7037597814135624, train_acc: 0.5282738010088602, val_loss: 0.5558814108371735, val_acc: 1.0
epoch: 581: train_loss: 0.7038144230808452, train_acc: 0.4434523781140645, val_loss: 0.5587682127952576, val_acc: 0.984375
epoch: 582: train_loss: 0.7038623931375212, train_acc: 0.4598214228947957, val_loss: 0.5515896081924438, val_acc: 1.0
epoch: 583: train_loss: 0.7038510776375797, train_acc: 0.4895833333333333, val_loss: 0.5852758884429932, val_acc: 0.96875
epoch: 584: train_loss: 0.7037480904845429, train_acc: 0.5729166666666666, val_loss: 0.5520571768283844, val_acc: 1.0
epoch: 585: train_loss: 0.7038089176629317, train_acc: 0.4285714228947957, val_loss: 0.6095154583454132, val_acc: 0.921875
epoch: 586: train_loss: 0.7037236199062158, train_acc: 0.538690467675527, val_loss: 0.5515725910663605, val_acc: 1.0
epoch: 587: train_loss: 0.7036768746416584, train_acc: 0.5252976218859354, val_loss: 0.5520093441009521, val_acc: 1.0
epoch: 588: train_loss: 0.703561735342098, train_acc: 0.5610119005044302, val_loss: 0.5727421939373016, val_acc: 0.96875
epoch: 589: train_loss: 0.7033842213578142, train_acc: 0.6041666666666666, val_loss: 0.5587232112884521, val_acc: 1.0
epoch: 590: train_loss: 0.7033537293952935, train_acc: 0.5029761989911398, val_loss: 0.5672204792499542, val_acc: 0.984375
epoch: 591: train_loss: 0.7033741663940049, train_acc: 0.5104166666666666, val_loss: 0.5514755845069885, val_acc: 1.0
epoch: 592: train_loss: 0.7032410238585222, train_acc: 0.5982142885526022, val_loss: 0.5611733794212341, val_acc: 0.984375
epoch: 593: train_loss: 0.703232121270246, train_acc: 0.4806547562281291, val_loss: 0.5671474933624268, val_acc: 0.984375
epoch: 594: train_loss: 0.7033299282318403, train_acc: 0.4136904776096344, val_loss: 0.5684852004051208, val_acc: 0.984375
epoch: 595: train_loss: 0.7032561422694449, train_acc: 0.5565476218859354, val_loss: 0.5675864815711975, val_acc: 0.984375
epoch: 596: train_loss: 0.7030539409965193, train_acc: 0.636904756228129, val_loss: 0.5597096085548401, val_acc: 1.0
epoch: 597: train_loss: 0.7029620942430219, train_acc: 0.6026785771052042, val_loss: 0.5967361629009247, val_acc: 0.9375
epoch: 598: train_loss: 0.7029084167051926, train_acc: 0.5505952338377634, val_loss: 0.5555671751499176, val_acc: 1.0
epoch: 599: train_loss: 0.7029309087826145, train_acc: 0.4553571442763011, val_loss: 0.5535362958908081, val_acc: 1.0
epoch: 600: train_loss: 0.7030005178812538, train_acc: 0.4449404776096344, val_loss: 0.5515075623989105, val_acc: 1.0
epoch: 601: train_loss: 0.7028119228078942, train_acc: 0.625, val_loss: 0.5721710324287415, val_acc: 0.984375
epoch: 602: train_loss: 0.7027708907493773, train_acc: 0.5327380895614624, val_loss: 0.5521039664745331, val_acc: 1.0
epoch: 603: train_loss: 0.7027278255679487, train_acc: 0.523809532324473, val_loss: 0.5543285608291626, val_acc: 1.0
epoch: 604: train_loss: 0.702754638280422, train_acc: 0.4866071442763011, val_loss: 0.5539063513278961, val_acc: 1.0
epoch: 605: train_loss: 0.7027180537138835, train_acc: 0.5401785671710968, val_loss: 0.5671805739402771, val_acc: 0.984375
epoch: 606: train_loss: 0.7027538515328706, train_acc: 0.4880952338377635, val_loss: 0.5519925057888031, val_acc: 1.0
epoch: 607: train_loss: 0.7027303991806612, train_acc: 0.4776785771052043, val_loss: 0.5672892928123474, val_acc: 0.984375
epoch: 608: train_loss: 0.7027339338930043, train_acc: 0.5208333333333334, val_loss: 0.5524576604366302, val_acc: 1.0
epoch: 609: train_loss: 0.7026535374545009, train_acc: 0.6041666666666666, val_loss: 0.5817562341690063, val_acc: 0.96875
epoch: 610: train_loss: 0.7026724546448624, train_acc: 0.4910714228947957, val_loss: 0.5703668892383575, val_acc: 0.984375
epoch: 611: train_loss: 0.7026134052502564, train_acc: 0.5565476218859354, val_loss: 0.5559524893760681, val_acc: 1.0
epoch: 612: train_loss: 0.7025611991788979, train_acc: 0.4985119005044301, val_loss: 0.5548429489135742, val_acc: 1.0
epoch: 613: train_loss: 0.7024816883632854, train_acc: 0.5401785771052042, val_loss: 0.5517242848873138, val_acc: 1.0
epoch: 614: train_loss: 0.7024899859738544, train_acc: 0.4553571442763011, val_loss: 0.5557472109794617, val_acc: 1.0
epoch: 615: train_loss: 0.702493873574001, train_acc: 0.46875, val_loss: 0.5582140684127808, val_acc: 1.0
epoch: 616: train_loss: 0.7026091466588887, train_acc: 0.3928571442763011, val_loss: 0.56673663854599, val_acc: 0.984375
epoch: 617: train_loss: 0.702621340912394, train_acc: 0.4955357114473979, val_loss: 0.5552109777927399, val_acc: 1.0
epoch: 618: train_loss: 0.7026138340971069, train_acc: 0.5044642885526022, val_loss: 0.5605404078960419, val_acc: 0.984375
epoch: 619: train_loss: 0.7027005952853029, train_acc: 0.4434523781140645, val_loss: 0.5515236258506775, val_acc: 1.0
epoch: 620: train_loss: 0.7025785255803259, train_acc: 0.581845243771871, val_loss: 0.5646144449710846, val_acc: 0.984375
epoch: 621: train_loss: 0.7025049803469087, train_acc: 0.5892857114473978, val_loss: 0.5768943130970001, val_acc: 0.96875
epoch: 622: train_loss: 0.7023918994484643, train_acc: 0.5758928656578064, val_loss: 0.5824950337409973, val_acc: 0.96875
epoch: 623: train_loss: 0.7023693344149835, train_acc: 0.511904756228129, val_loss: 0.5541527271270752, val_acc: 1.0
epoch: 624: train_loss: 0.7023488050778708, train_acc: 0.5178571343421936, val_loss: 0.6079795956611633, val_acc: 0.9375
epoch: 625: train_loss: 0.7022620951469206, train_acc: 0.5461309552192688, val_loss: 0.5671327710151672, val_acc: 0.984375
epoch: 626: train_loss: 0.7022907723777159, train_acc: 0.4732142885526021, val_loss: 0.5527585744857788, val_acc: 1.0
epoch: 627: train_loss: 0.7022707911243865, train_acc: 0.5193452338377634, val_loss: 0.568540096282959, val_acc: 0.984375
epoch: 628: train_loss: 0.702176162756873, train_acc: 0.5669642885526022, val_loss: 0.5760034024715424, val_acc: 0.96875
epoch: 629: train_loss: 0.7021987063544138, train_acc: 0.4672619005044301, val_loss: 0.5517073571681976, val_acc: 1.0
epoch: 630: train_loss: 0.7021962242723456, train_acc: 0.5029761989911398, val_loss: 0.5566715002059937, val_acc: 1.0
epoch: 631: train_loss: 0.7022004598743806, train_acc: 0.4791666666666667, val_loss: 0.5670557618141174, val_acc: 0.984375
epoch: 632: train_loss: 0.7021483571482936, train_acc: 0.5208333333333334, val_loss: 0.5639858841896057, val_acc: 0.984375
epoch: 633: train_loss: 0.7021468220637048, train_acc: 0.5059523781140646, val_loss: 0.5826846659183502, val_acc: 0.96875
epoch: 634: train_loss: 0.7020708216456917, train_acc: 0.5282738109429678, val_loss: 0.5655922591686249, val_acc: 0.984375
epoch: 635: train_loss: 0.7019953720849517, train_acc: 0.5625, val_loss: 0.5808005928993225, val_acc: 1.0
epoch: 636: train_loss: 0.7019120708053367, train_acc: 0.5580357114473978, val_loss: 0.5514839291572571, val_acc: 1.0
epoch: 637: train_loss: 0.7020581782549279, train_acc: 0.4092261890570323, val_loss: 0.5656858086585999, val_acc: 0.984375
epoch: 638: train_loss: 0.7020906224870161, train_acc: 0.4375, val_loss: 0.5520209074020386, val_acc: 1.0
epoch: 639: train_loss: 0.7020531295798721, train_acc: 0.523809532324473, val_loss: 0.5519517958164215, val_acc: 1.0
epoch: 640: train_loss: 0.7020495773539592, train_acc: 0.4851190447807312, val_loss: 0.5528429448604584, val_acc: 1.0
epoch: 641: train_loss: 0.7019385226914199, train_acc: 0.555059532324473, val_loss: 0.5514902770519257, val_acc: 1.0
epoch: 642: train_loss: 0.7018933980312914, train_acc: 0.5297619005044302, val_loss: 0.5515808761119843, val_acc: 1.0
epoch: 643: train_loss: 0.7018248055045405, train_acc: 0.5491071343421936, val_loss: 0.5565993785858154, val_acc: 1.0
epoch: 644: train_loss: 0.7018177521629237, train_acc: 0.4880952338377635, val_loss: 0.5906427800655365, val_acc: 0.9375
epoch: 645: train_loss: 0.701666174658312, train_acc: 0.5877976218859354, val_loss: 0.5663990676403046, val_acc: 0.984375
epoch: 646: train_loss: 0.701750801802296, train_acc: 0.4449404776096344, val_loss: 0.551734209060669, val_acc: 1.0
epoch: 647: train_loss: 0.7017695424549376, train_acc: 0.5, val_loss: 0.5553269386291504, val_acc: 1.0
epoch: 648: train_loss: 0.7017586488079648, train_acc: 0.523809532324473, val_loss: 0.5789071321487427, val_acc: 0.9375
epoch: 649: train_loss: 0.7016749380490722, train_acc: 0.5595238010088602, val_loss: 0.5711471736431122, val_acc: 0.984375
epoch: 650: train_loss: 0.7017232019414188, train_acc: 0.4940476218859355, val_loss: 0.5522491335868835, val_acc: 1.0
epoch: 651: train_loss: 0.7016456852417542, train_acc: 0.5639880895614624, val_loss: 0.5514936149120331, val_acc: 1.0
epoch: 652: train_loss: 0.7015646581992989, train_acc: 0.5877976218859354, val_loss: 0.5567836165428162, val_acc: 1.0
epoch: 653: train_loss: 0.7015697375228528, train_acc: 0.4583333333333333, val_loss: 0.5518682897090912, val_acc: 1.0
epoch: 654: train_loss: 0.7015229304631555, train_acc: 0.511904756228129, val_loss: 0.5722631812095642, val_acc: 0.984375
epoch: 655: train_loss: 0.7014208631059992, train_acc: 0.5505952338377634, val_loss: 0.5520337224006653, val_acc: 1.0
epoch: 656: train_loss: 0.7014358671772369, train_acc: 0.5029761890570322, val_loss: 0.6138791441917419, val_acc: 0.9375
epoch: 657: train_loss: 0.7014251533733438, train_acc: 0.4866071442763011, val_loss: 0.5634787678718567, val_acc: 0.984375
epoch: 658: train_loss: 0.7014479585892638, train_acc: 0.4508928557236989, val_loss: 0.5547586381435394, val_acc: 1.0
epoch: 659: train_loss: 0.7014169329344628, train_acc: 0.517857144276301, val_loss: 0.59862419962883, val_acc: 0.953125
epoch: 660: train_loss: 0.7013437498475472, train_acc: 0.574404756228129, val_loss: 0.6451925039291382, val_acc: 0.90625
epoch: 661: train_loss: 0.7013669297294198, train_acc: 0.4508928557236989, val_loss: 0.6046721339225769, val_acc: 0.9375
epoch: 662: train_loss: 0.7012207374733381, train_acc: 0.6071428656578064, val_loss: 0.5577077269554138, val_acc: 1.0
epoch: 663: train_loss: 0.701160521482129, train_acc: 0.5342261989911398, val_loss: 0.5670807361602783, val_acc: 0.984375
epoch: 664: train_loss: 0.7009738726424697, train_acc: 0.617559532324473, val_loss: 0.5536078214645386, val_acc: 1.0
epoch: 665: train_loss: 0.700774343581768, train_acc: 0.6502976218859354, val_loss: 0.5682540833950043, val_acc: 0.984375
epoch: 666: train_loss: 0.7007677751085514, train_acc: 0.511904756228129, val_loss: 0.562294065952301, val_acc: 0.984375
epoch: 667: train_loss: 0.7008279946868771, train_acc: 0.4613095323244731, val_loss: 0.5657025277614594, val_acc: 0.96875
epoch: 668: train_loss: 0.7009013305865065, train_acc: 0.4806547562281291, val_loss: 0.5514757931232452, val_acc: 1.0
epoch: 669: train_loss: 0.7008729143818817, train_acc: 0.5267857114473978, val_loss: 0.5910457372665405, val_acc: 0.96875
epoch: 670: train_loss: 0.7009034577555407, train_acc: 0.53125, val_loss: 0.5821329355239868, val_acc: 0.96875
epoch: 671: train_loss: 0.7009920619191636, train_acc: 0.4717261989911397, val_loss: 0.5516434907913208, val_acc: 1.0
epoch: 672: train_loss: 0.701032139453161, train_acc: 0.4642857114473979, val_loss: 0.551520824432373, val_acc: 1.0
epoch: 673: train_loss: 0.7010037906681635, train_acc: 0.5401785671710968, val_loss: 0.5524028539657593, val_acc: 1.0
epoch: 674: train_loss: 0.7010602908664284, train_acc: 0.4776785671710968, val_loss: 0.5908370018005371, val_acc: 0.9375
epoch: 675: train_loss: 0.7009538349491609, train_acc: 0.5580357114473978, val_loss: 0.55173259973526, val_acc: 1.0
epoch: 676: train_loss: 0.7010176547462283, train_acc: 0.4568452338377635, val_loss: 0.5516974925994873, val_acc: 1.0
epoch: 677: train_loss: 0.7009960832321542, train_acc: 0.543154756228129, val_loss: 0.5525763630867004, val_acc: 1.0
epoch: 678: train_loss: 0.7010771213177089, train_acc: 0.4196428656578064, val_loss: 0.551761120557785, val_acc: 1.0
epoch: 679: train_loss: 0.700992465486714, train_acc: 0.5729166666666666, val_loss: 0.567976176738739, val_acc: 0.984375
epoch: 680: train_loss: 0.7010556053310069, train_acc: 0.4761904776096344, val_loss: 0.5670667886734009, val_acc: 0.984375
epoch: 681: train_loss: 0.7010069277978718, train_acc: 0.5014880895614624, val_loss: 0.5514979064464569, val_acc: 1.0
epoch: 682: train_loss: 0.7011211728572617, train_acc: 0.3928571442763011, val_loss: 0.6231819093227386, val_acc: 0.921875
epoch: 683: train_loss: 0.7011276637775861, train_acc: 0.4955357114473979, val_loss: 0.5514934957027435, val_acc: 1.0
epoch: 684: train_loss: 0.701144329822847, train_acc: 0.4821428656578064, val_loss: 0.5516105890274048, val_acc: 1.0
epoch: 685: train_loss: 0.7011274034184553, train_acc: 0.5104166666666666, val_loss: 0.5668026804924011, val_acc: 0.984375
epoch: 686: train_loss: 0.7011608263348451, train_acc: 0.4880952338377635, val_loss: 0.562809020280838, val_acc: 0.984375
epoch: 687: train_loss: 0.7011476875564392, train_acc: 0.4732142885526021, val_loss: 0.551464170217514, val_acc: 1.0
epoch: 688: train_loss: 0.7010195685805597, train_acc: 0.5877976218859354, val_loss: 0.5521170198917389, val_acc: 1.0
epoch: 689: train_loss: 0.7007861631791954, train_acc: 0.6622023781140646, val_loss: 0.5515031218528748, val_acc: 1.0
epoch: 690: train_loss: 0.7007643704648997, train_acc: 0.4910714228947957, val_loss: 0.5514751374721527, val_acc: 1.0
epoch: 691: train_loss: 0.7008112457675977, train_acc: 0.4255952338377635, val_loss: 0.5514498353004456, val_acc: 1.0
epoch: 692: train_loss: 0.700713186524361, train_acc: 0.5892857114473978, val_loss: 0.5521296262741089, val_acc: 1.0
epoch: 693: train_loss: 0.7007928772057402, train_acc: 0.4330357114473979, val_loss: 0.567129909992218, val_acc: 0.984375
epoch: 694: train_loss: 0.7007141468908009, train_acc: 0.549107144276301, val_loss: 0.5514473915100098, val_acc: 1.0
epoch: 695: train_loss: 0.7006370170020515, train_acc: 0.5952380895614624, val_loss: 0.5670818984508514, val_acc: 0.984375
epoch: 696: train_loss: 0.7005918127164386, train_acc: 0.5223214228947958, val_loss: 0.5540789663791656, val_acc: 1.0
epoch: 697: train_loss: 0.7005632328440609, train_acc: 0.5193452338377634, val_loss: 0.5514468848705292, val_acc: 1.0
epoch: 698: train_loss: 0.7005891071709788, train_acc: 0.4747023781140645, val_loss: 0.5514940023422241, val_acc: 1.0
epoch: 699: train_loss: 0.7005075682628729, train_acc: 0.5773809552192688, val_loss: 0.5646713376045227, val_acc: 0.984375
epoch: 700: train_loss: 0.7004864573818816, train_acc: 0.5133928656578064, val_loss: 0.5518607497215271, val_acc: 1.0
epoch: 701: train_loss: 0.7003995473907889, train_acc: 0.5773809552192688, val_loss: 0.552829772233963, val_acc: 1.0
epoch: 702: train_loss: 0.7003840903299807, train_acc: 0.5089285671710968, val_loss: 0.5672515332698822, val_acc: 0.984375
epoch: 703: train_loss: 0.7002656237419813, train_acc: 0.5729166666666666, val_loss: 0.5753730833530426, val_acc: 0.984375
epoch: 704: train_loss: 0.7002649752524451, train_acc: 0.4880952338377635, val_loss: 0.5676008462905884, val_acc: 0.984375
epoch: 705: train_loss: 0.7002266839892382, train_acc: 0.5401785671710968, val_loss: 0.5675366520881653, val_acc: 0.984375
epoch: 706: train_loss: 0.700170860291652, train_acc: 0.523809532324473, val_loss: 0.579833984375, val_acc: 0.96875
epoch: 707: train_loss: 0.7001913380712669, train_acc: 0.5282738109429678, val_loss: 0.6346802115440369, val_acc: 0.921875
epoch: 708: train_loss: 0.7000935616126691, train_acc: 0.5803571343421936, val_loss: 0.5635852217674255, val_acc: 0.984375
epoch: 709: train_loss: 0.700152519345284, train_acc: 0.4672619005044301, val_loss: 0.5759161412715912, val_acc: 0.984375
epoch: 710: train_loss: 0.7000694058755172, train_acc: 0.5877976218859354, val_loss: 0.6142659783363342, val_acc: 0.9375
epoch: 711: train_loss: 0.6999474441988419, train_acc: 0.6532738010088602, val_loss: 0.5779107809066772, val_acc: 0.96875
epoch: 712: train_loss: 0.6999222611283079, train_acc: 0.5401785671710968, val_loss: 0.5659529566764832, val_acc: 0.984375
epoch: 713: train_loss: 0.6998109525186124, train_acc: 0.6383928656578064, val_loss: 0.592844545841217, val_acc: 0.953125
epoch: 714: train_loss: 0.6999052596397894, train_acc: 0.4642857114473979, val_loss: 0.6452197134494781, val_acc: 0.90625
epoch: 715: train_loss: 0.6998665241916535, train_acc: 0.5729166666666666, val_loss: 0.5693570077419281, val_acc: 0.984375
epoch: 716: train_loss: 0.6997455269416955, train_acc: 0.601190467675527, val_loss: 0.5801337659358978, val_acc: 0.953125
epoch: 717: train_loss: 0.699722541477134, train_acc: 0.5252976218859354, val_loss: 0.5724444389343262, val_acc: 0.96875
epoch: 718: train_loss: 0.6996948577404911, train_acc: 0.4880952338377635, val_loss: 0.562042772769928, val_acc: 0.984375
epoch: 719: train_loss: 0.6996204282536556, train_acc: 0.569940467675527, val_loss: 0.5735195279121399, val_acc: 0.96875
epoch: 720: train_loss: 0.6995813761300865, train_acc: 0.586309532324473, val_loss: 0.5678039789199829, val_acc: 0.984375
epoch: 721: train_loss: 0.6995577570721719, train_acc: 0.5461309552192688, val_loss: 0.5762549042701721, val_acc: 0.96875
epoch: 722: train_loss: 0.6996536952743562, train_acc: 0.4553571442763011, val_loss: 0.5545603632926941, val_acc: 1.0
epoch: 723: train_loss: 0.6995572314818925, train_acc: 0.5580357114473978, val_loss: 0.5661208629608154, val_acc: 0.984375
epoch: 724: train_loss: 0.6995703557030913, train_acc: 0.4925595323244731, val_loss: 0.5905435383319855, val_acc: 0.953125
epoch: 725: train_loss: 0.6996510083709324, train_acc: 0.5208333333333334, val_loss: 0.6297115683555603, val_acc: 0.921875
epoch: 726: train_loss: 0.6996326627588124, train_acc: 0.5372023781140646, val_loss: 0.5602525472640991, val_acc: 1.0
epoch: 727: train_loss: 0.6995575163144993, train_acc: 0.613095243771871, val_loss: 0.6377322971820831, val_acc: 0.90625
epoch: 728: train_loss: 0.6995561224143152, train_acc: 0.5729166666666666, val_loss: 0.6858136057853699, val_acc: 0.859375
epoch: 729: train_loss: 0.6997312978658506, train_acc: 0.4657738109429677, val_loss: 0.6442282497882843, val_acc: 0.921875
epoch: 730: train_loss: 0.6997977283057485, train_acc: 0.4985119005044301, val_loss: 0.5662951171398163, val_acc: 1.0
epoch: 731: train_loss: 0.6997610071507747, train_acc: 0.4955357114473979, val_loss: 0.5840920805931091, val_acc: 0.96875
epoch: 732: train_loss: 0.6998984422695648, train_acc: 0.4226190447807312, val_loss: 0.6761439740657806, val_acc: 0.890625
epoch: 733: train_loss: 0.6998576234000912, train_acc: 0.5684523781140646, val_loss: 0.6645325422286987, val_acc: 0.859375
epoch: 734: train_loss: 0.6997866058430711, train_acc: 0.5505952338377634, val_loss: 0.6649342179298401, val_acc: 0.859375
epoch: 735: train_loss: 0.6998053494219986, train_acc: 0.5327380895614624, val_loss: 0.5685044825077057, val_acc: 0.984375
epoch: 736: train_loss: 0.6998156995349538, train_acc: 0.4657738109429677, val_loss: 0.6139837205410004, val_acc: 0.9375
epoch: 737: train_loss: 0.6997613786385749, train_acc: 0.555059532324473, val_loss: 0.631680816411972, val_acc: 0.90625
epoch: 738: train_loss: 0.6997268181504251, train_acc: 0.5297619005044302, val_loss: 0.6450716853141785, val_acc: 0.90625
epoch: 739: train_loss: 0.6996683170532327, train_acc: 0.5625, val_loss: 0.644262820482254, val_acc: 0.90625
epoch: 740: train_loss: 0.6996682277012135, train_acc: 0.5208333333333334, val_loss: 0.5670191943645477, val_acc: 0.984375
epoch: 741: train_loss: 0.6994340688338291, train_acc: 0.6889880895614624, val_loss: 0.5514518320560455, val_acc: 1.0
epoch: 742: train_loss: 0.6993708492876336, train_acc: 0.5401785671710968, val_loss: 0.5651788115501404, val_acc: 0.984375
epoch: 743: train_loss: 0.6994684944496794, train_acc: 0.4345238109429677, val_loss: 0.5554089248180389, val_acc: 1.0
epoch: 744: train_loss: 0.6994759030256769, train_acc: 0.517857144276301, val_loss: 0.5674005746841431, val_acc: 0.984375
epoch: 745: train_loss: 0.699529777583105, train_acc: 0.5014880895614624, val_loss: 0.5961021482944489, val_acc: 0.953125
epoch: 746: train_loss: 0.6994911970064509, train_acc: 0.5535714228947958, val_loss: 0.5715791881084442, val_acc: 0.984375
epoch: 747: train_loss: 0.6995342848507059, train_acc: 0.4598214328289032, val_loss: 0.5677451193332672, val_acc: 0.984375
epoch: 748: train_loss: 0.699516186275958, train_acc: 0.5297619005044302, val_loss: 0.553202360868454, val_acc: 1.0
epoch: 749: train_loss: 0.6995250346925529, train_acc: 0.5342261989911398, val_loss: 0.5525867640972137, val_acc: 1.0
epoch: 750: train_loss: 0.6995815209055608, train_acc: 0.4583333333333333, val_loss: 0.5514506697654724, val_acc: 1.0
epoch: 751: train_loss: 0.6994958616502536, train_acc: 0.550595243771871, val_loss: 0.5673179924488068, val_acc: 0.984375
epoch: 752: train_loss: 0.699533731510493, train_acc: 0.4880952338377635, val_loss: 0.566745400428772, val_acc: 0.984375
epoch: 753: train_loss: 0.6994205477304111, train_acc: 0.5639880895614624, val_loss: 0.5670633614063263, val_acc: 0.984375
epoch: 754: train_loss: 0.6993773613149763, train_acc: 0.5491071343421936, val_loss: 0.5774078071117401, val_acc: 0.96875
epoch: 755: train_loss: 0.6993542818559547, train_acc: 0.53125, val_loss: 0.6394093334674835, val_acc: 0.90625
epoch: 756: train_loss: 0.6993596558133085, train_acc: 0.523809532324473, val_loss: 0.66293004155159, val_acc: 0.875
epoch: 757: train_loss: 0.6992971969599675, train_acc: 0.5669642885526022, val_loss: 0.5540547668933868, val_acc: 1.0
epoch: 758: train_loss: 0.6992336973502131, train_acc: 0.5669642885526022, val_loss: 0.5530238449573517, val_acc: 1.0
epoch: 759: train_loss: 0.699173997068092, train_acc: 0.5327380895614624, val_loss: 0.5555670261383057, val_acc: 1.0
epoch: 760: train_loss: 0.6992218051753549, train_acc: 0.4895833333333333, val_loss: 0.5662519633769989, val_acc: 0.984375
epoch: 761: train_loss: 0.6991956684402705, train_acc: 0.4880952338377635, val_loss: 0.5671557188034058, val_acc: 0.984375
epoch: 762: train_loss: 0.6992235292691553, train_acc: 0.4747023781140645, val_loss: 0.6026792824268341, val_acc: 0.953125
epoch: 763: train_loss: 0.6992533192506641, train_acc: 0.4985119005044301, val_loss: 0.5827010273933411, val_acc: 0.96875
epoch: 764: train_loss: 0.6992289196302175, train_acc: 0.5297619005044302, val_loss: 0.5670391321182251, val_acc: 0.984375
epoch: 765: train_loss: 0.6991545943279083, train_acc: 0.5535714228947958, val_loss: 0.6057363450527191, val_acc: 0.921875
epoch: 766: train_loss: 0.6990774459991189, train_acc: 0.586309532324473, val_loss: 0.6183342635631561, val_acc: 0.9375
epoch: 767: train_loss: 0.6991987641926656, train_acc: 0.4136904776096344, val_loss: 0.5926852822303772, val_acc: 0.921875
epoch: 768: train_loss: 0.6990902372056608, train_acc: 0.6071428656578064, val_loss: 0.5909664928913116, val_acc: 0.9375
epoch: 769: train_loss: 0.6989808993035073, train_acc: 0.5982142885526022, val_loss: 0.6301606297492981, val_acc: 0.921875
epoch: 770: train_loss: 0.698862236645436, train_acc: 0.6071428656578064, val_loss: 0.5572472810745239, val_acc: 1.0
epoch: 771: train_loss: 0.6988286054427768, train_acc: 0.5535714228947958, val_loss: 0.5782173573970795, val_acc: 0.96875
epoch: 772: train_loss: 0.6987735953064722, train_acc: 0.538690467675527, val_loss: 0.5670830607414246, val_acc: 0.984375
epoch: 773: train_loss: 0.6986778642241046, train_acc: 0.5729166666666666, val_loss: 0.580663412809372, val_acc: 0.96875
epoch: 774: train_loss: 0.698638053850462, train_acc: 0.543154756228129, val_loss: 0.5968080163002014, val_acc: 0.921875
epoch: 775: train_loss: 0.6986314237399415, train_acc: 0.5029761989911398, val_loss: 0.6333001554012299, val_acc: 0.90625
epoch: 776: train_loss: 0.6985173959401695, train_acc: 0.6116071343421936, val_loss: 0.6160830557346344, val_acc: 0.9375
epoch: 777: train_loss: 0.6984436911528408, train_acc: 0.5892857114473978, val_loss: 0.5518545806407928, val_acc: 1.0
epoch: 778: train_loss: 0.6984602243384092, train_acc: 0.4732142885526021, val_loss: 0.6302128732204437, val_acc: 0.921875
epoch: 779: train_loss: 0.6984117170047566, train_acc: 0.5223214228947958, val_loss: 0.5855257511138916, val_acc: 0.921875
epoch: 780: train_loss: 0.6983905698542822, train_acc: 0.5208333333333334, val_loss: 0.5514649152755737, val_acc: 1.0
epoch: 781: train_loss: 0.6983383737332994, train_acc: 0.5133928656578064, val_loss: 0.553485244512558, val_acc: 1.0
epoch: 782: train_loss: 0.6983077565296104, train_acc: 0.5163690447807312, val_loss: 0.5670654773712158, val_acc: 0.984375
epoch: 783: train_loss: 0.6983260199503639, train_acc: 0.4732142885526021, val_loss: 0.5689639151096344, val_acc: 0.984375
epoch: 784: train_loss: 0.6983504845205141, train_acc: 0.5208333333333334, val_loss: 0.5915390849113464, val_acc: 0.9375
epoch: 785: train_loss: 0.6983784586696739, train_acc: 0.4747023781140645, val_loss: 0.6262800097465515, val_acc: 0.921875
epoch: 786: train_loss: 0.698333110926971, train_acc: 0.5401785671710968, val_loss: 0.5527462959289551, val_acc: 1.0
epoch: 787: train_loss: 0.6982970514449782, train_acc: 0.5565476218859354, val_loss: 0.5733318030834198, val_acc: 0.96875
epoch: 788: train_loss: 0.6983654030048256, train_acc: 0.4404761890570323, val_loss: 0.5751315653324127, val_acc: 0.984375
epoch: 789: train_loss: 0.698328369063668, train_acc: 0.5416666666666666, val_loss: 0.5780119299888611, val_acc: 0.96875
epoch: 790: train_loss: 0.698312743889146, train_acc: 0.5282738109429678, val_loss: 0.5514718294143677, val_acc: 1.0
epoch: 791: train_loss: 0.6982871475018043, train_acc: 0.523809532324473, val_loss: 0.55524942278862, val_acc: 1.0
epoch: 792: train_loss: 0.6982883578867116, train_acc: 0.4970238109429677, val_loss: 0.5562033951282501, val_acc: 1.0
epoch: 793: train_loss: 0.6983127401943079, train_acc: 0.5104166666666666, val_loss: 0.5514984130859375, val_acc: 1.0
epoch: 794: train_loss: 0.6982132909557862, train_acc: 0.5788690447807312, val_loss: 0.5516905784606934, val_acc: 1.0
epoch: 795: train_loss: 0.6982363175532135, train_acc: 0.4880952338377635, val_loss: 0.5685688853263855, val_acc: 0.984375
epoch: 796: train_loss: 0.6981663188017954, train_acc: 0.5758928656578064, val_loss: 0.5514514446258545, val_acc: 1.0
epoch: 797: train_loss: 0.6981428611771513, train_acc: 0.5327380895614624, val_loss: 0.5515677034854889, val_acc: 1.0
epoch: 798: train_loss: 0.698060089578518, train_acc: 0.5639880895614624, val_loss: 0.5515007972717285, val_acc: 1.0
epoch: 799: train_loss: 0.6980861913040289, train_acc: 0.4642857114473979, val_loss: 0.5534546971321106, val_acc: 1.0
epoch: 800: train_loss: 0.6979765624266794, train_acc: 0.5892857114473978, val_loss: 0.5515090823173523, val_acc: 1.0
epoch: 801: train_loss: 0.697921407923834, train_acc: 0.5386904776096344, val_loss: 0.573754221200943, val_acc: 0.96875
epoch: 802: train_loss: 0.6979155268670125, train_acc: 0.4955357114473979, val_loss: 0.559955507516861, val_acc: 0.984375
epoch: 803: train_loss: 0.697758818752632, train_acc: 0.65625, val_loss: 0.5551863610744476, val_acc: 1.0
epoch: 804: train_loss: 0.6977581280482243, train_acc: 0.511904756228129, val_loss: 0.5522875189781189, val_acc: 1.0
epoch: 805: train_loss: 0.697779677999326, train_acc: 0.5029761989911398, val_loss: 0.5525718033313751, val_acc: 1.0
epoch: 806: train_loss: 0.6977267757839831, train_acc: 0.5714285671710968, val_loss: 0.5592151582241058, val_acc: 0.984375
epoch: 807: train_loss: 0.6977230807022498, train_acc: 0.5029761989911398, val_loss: 0.5515421330928802, val_acc: 1.0
epoch: 808: train_loss: 0.697730356238731, train_acc: 0.4717261989911397, val_loss: 0.5514914691448212, val_acc: 1.0
epoch: 809: train_loss: 0.6977315719848806, train_acc: 0.4985119005044301, val_loss: 0.5514562129974365, val_acc: 1.0
epoch: 810: train_loss: 0.6976994034369892, train_acc: 0.5282738109429678, val_loss: 0.5659438073635101, val_acc: 0.984375
epoch: 811: train_loss: 0.6976751814946565, train_acc: 0.5461309552192688, val_loss: 0.5516976416110992, val_acc: 1.0
epoch: 812: train_loss: 0.6977012274132354, train_acc: 0.4494047562281291, val_loss: 0.5634620487689972, val_acc: 0.984375
epoch: 813: train_loss: 0.6976629361986244, train_acc: 0.5342261989911398, val_loss: 0.5605752170085907, val_acc: 0.984375
epoch: 814: train_loss: 0.6975476674385118, train_acc: 0.6294642885526022, val_loss: 0.5514862835407257, val_acc: 1.0
epoch: 815: train_loss: 0.6974890198363791, train_acc: 0.5669642885526022, val_loss: 0.5657562017440796, val_acc: 0.984375
epoch: 816: train_loss: 0.6975142742871566, train_acc: 0.511904756228129, val_loss: 0.5514678061008453, val_acc: 1.0
epoch: 817: train_loss: 0.6975453088778067, train_acc: 0.4776785671710968, val_loss: 0.5548956096172333, val_acc: 1.0
epoch: 818: train_loss: 0.6974486319646158, train_acc: 0.5833333333333334, val_loss: 0.5973784923553467, val_acc: 0.953125
epoch: 819: train_loss: 0.6974613386562211, train_acc: 0.5044642885526022, val_loss: 0.6410920917987823, val_acc: 0.890625
epoch: 820: train_loss: 0.6973690314235805, train_acc: 0.5892857114473978, val_loss: 0.5827688872814178, val_acc: 0.96875
epoch: 821: train_loss: 0.6973012796597866, train_acc: 0.555059532324473, val_loss: 0.5791646540164948, val_acc: 0.96875
epoch: 822: train_loss: 0.6971986417757843, train_acc: 0.586309532324473, val_loss: 0.551699310541153, val_acc: 1.0
epoch: 823: train_loss: 0.6971708686726968, train_acc: 0.5059523781140646, val_loss: 0.5529705584049225, val_acc: 1.0
epoch: 824: train_loss: 0.6970701933268356, train_acc: 0.574404756228129, val_loss: 0.5516393482685089, val_acc: 1.0
epoch: 825: train_loss: 0.6971110212673008, train_acc: 0.4702380895614624, val_loss: 0.5535902678966522, val_acc: 1.0
epoch: 826: train_loss: 0.6970277507066834, train_acc: 0.5788690447807312, val_loss: 0.5516727566719055, val_acc: 1.0
epoch: 827: train_loss: 0.6970109804360967, train_acc: 0.5178571343421936, val_loss: 0.6095492243766785, val_acc: 0.9375
epoch: 828: train_loss: 0.6969668408043194, train_acc: 0.543154756228129, val_loss: 0.5524927079677582, val_acc: 1.0
epoch: 829: train_loss: 0.6969886891454111, train_acc: 0.4791666666666667, val_loss: 0.5670605599880219, val_acc: 0.984375
epoch: 830: train_loss: 0.697045722557177, train_acc: 0.4672619005044301, val_loss: 0.5678420066833496, val_acc: 0.984375
epoch: 831: train_loss: 0.6969819477138433, train_acc: 0.555059532324473, val_loss: 0.5671248137950897, val_acc: 0.984375
epoch: 832: train_loss: 0.6969684020740228, train_acc: 0.5252976218859354, val_loss: 0.5605683922767639, val_acc: 0.984375
epoch: 833: train_loss: 0.696941093253576, train_acc: 0.5267857114473978, val_loss: 0.5557864904403687, val_acc: 1.0
epoch: 834: train_loss: 0.6969475151654977, train_acc: 0.511904756228129, val_loss: 0.5587345957756042, val_acc: 0.984375
epoch: 835: train_loss: 0.6968524614185629, train_acc: 0.5952380895614624, val_loss: 0.5534152090549469, val_acc: 1.0
epoch: 836: train_loss: 0.6968213823130303, train_acc: 0.5342261989911398, val_loss: 0.602177768945694, val_acc: 0.953125
epoch: 837: train_loss: 0.6967769356573407, train_acc: 0.5372023781140646, val_loss: 0.6314347386360168, val_acc: 0.921875
epoch: 838: train_loss: 0.6968064027430296, train_acc: 0.5089285671710968, val_loss: 0.5983225405216217, val_acc: 0.953125
epoch: 839: train_loss: 0.6967370280079437, train_acc: 0.5461309552192688, val_loss: 0.5669946372509003, val_acc: 0.984375
epoch: 840: train_loss: 0.6967006094555899, train_acc: 0.538690467675527, val_loss: 0.6191885471343994, val_acc: 0.921875
epoch: 841: train_loss: 0.6966308718834817, train_acc: 0.5684523781140646, val_loss: 0.5934171974658966, val_acc: 0.96875
epoch: 842: train_loss: 0.6966124048413106, train_acc: 0.5223214228947958, val_loss: 0.6291870474815369, val_acc: 0.875
epoch: 843: train_loss: 0.6965633068251394, train_acc: 0.5803571343421936, val_loss: 0.5531603991985321, val_acc: 1.0
epoch: 844: train_loss: 0.6965186897231758, train_acc: 0.5342261989911398, val_loss: 0.5982059836387634, val_acc: 0.953125
epoch: 845: train_loss: 0.6965030656380397, train_acc: 0.5386904776096344, val_loss: 0.5665943324565887, val_acc: 0.984375
epoch: 846: train_loss: 0.6963639311295999, train_acc: 0.6205357114473978, val_loss: 0.5816422402858734, val_acc: 0.96875
epoch: 847: train_loss: 0.6963347712145108, train_acc: 0.5386904776096344, val_loss: 0.553210437297821, val_acc: 1.0
epoch: 848: train_loss: 0.6963077903519354, train_acc: 0.5461309552192688, val_loss: 0.5565856397151947, val_acc: 1.0
epoch: 849: train_loss: 0.6962954592587911, train_acc: 0.5327380895614624, val_loss: 0.5515952408313751, val_acc: 1.0
epoch: 850: train_loss: 0.6962229660619169, train_acc: 0.5922619104385376, val_loss: 0.6257637143135071, val_acc: 0.921875
epoch: 851: train_loss: 0.6961945658418504, train_acc: 0.5684523781140646, val_loss: 0.5831433236598969, val_acc: 0.96875
epoch: 852: train_loss: 0.6962033435673218, train_acc: 0.4925595323244731, val_loss: 0.5826886892318726, val_acc: 0.96875
epoch: 853: train_loss: 0.6961961450239916, train_acc: 0.5327380895614624, val_loss: 0.5827884376049042, val_acc: 0.96875
epoch: 854: train_loss: 0.6961985350352291, train_acc: 0.4985119005044301, val_loss: 0.5671048760414124, val_acc: 0.984375
epoch: 855: train_loss: 0.6961732605164678, train_acc: 0.4925595323244731, val_loss: 0.5528947114944458, val_acc: 1.0
epoch: 856: train_loss: 0.696086594632794, train_acc: 0.5773809552192688, val_loss: 0.5677750110626221, val_acc: 0.984375
epoch: 857: train_loss: 0.6960470334401692, train_acc: 0.5461309552192688, val_loss: 0.5531069040298462, val_acc: 1.0
epoch: 858: train_loss: 0.6960350362931669, train_acc: 0.5357142885526022, val_loss: 0.5528723299503326, val_acc: 1.0
epoch: 859: train_loss: 0.6961175718741832, train_acc: 0.4151785671710968, val_loss: 0.5742333829402924, val_acc: 0.984375
epoch: 860: train_loss: 0.6961099170133959, train_acc: 0.543154756228129, val_loss: 0.5957594513893127, val_acc: 0.953125
epoch: 861: train_loss: 0.6961540196030442, train_acc: 0.4404761890570323, val_loss: 0.5672471225261688, val_acc: 0.984375
epoch: 862: train_loss: 0.6961274845989674, train_acc: 0.4970238109429677, val_loss: 0.5514754354953766, val_acc: 1.0
epoch: 863: train_loss: 0.6960976988905015, train_acc: 0.5089285671710968, val_loss: 0.5515649616718292, val_acc: 1.0
epoch: 864: train_loss: 0.6961751674641083, train_acc: 0.4122023781140645, val_loss: 0.5789774656295776, val_acc: 0.96875
epoch: 865: train_loss: 0.6960921724820899, train_acc: 0.605654756228129, val_loss: 0.5514568388462067, val_acc: 1.0
epoch: 866: train_loss: 0.6960934548367, train_acc: 0.5223214228947958, val_loss: 0.5672309696674347, val_acc: 0.984375
epoch: 867: train_loss: 0.6961185655881375, train_acc: 0.4866071442763011, val_loss: 0.5825247168540955, val_acc: 0.96875
epoch: 868: train_loss: 0.6960330242894835, train_acc: 0.5758928656578064, val_loss: 0.6450250148773193, val_acc: 0.890625
epoch: 869: train_loss: 0.696066759806484, train_acc: 0.4449404776096344, val_loss: 0.5655686259269714, val_acc: 0.984375
epoch: 870: train_loss: 0.6959874111256569, train_acc: 0.5803571343421936, val_loss: 0.5557122826576233, val_acc: 1.0
epoch: 871: train_loss: 0.6959096395185607, train_acc: 0.5907738010088602, val_loss: 0.5514523386955261, val_acc: 1.0
epoch: 872: train_loss: 0.6959652001960634, train_acc: 0.4970238109429677, val_loss: 0.5596849322319031, val_acc: 0.984375
epoch: 873: train_loss: 0.6959168859604757, train_acc: 0.517857144276301, val_loss: 0.5719153881072998, val_acc: 0.984375
epoch: 874: train_loss: 0.6959315064521071, train_acc: 0.4866071442763011, val_loss: 0.5836531519889832, val_acc: 0.96875
epoch: 875: train_loss: 0.6958884550783014, train_acc: 0.5461309552192688, val_loss: 0.5868862867355347, val_acc: 0.96875
epoch: 876: train_loss: 0.6959123407104367, train_acc: 0.4895833333333333, val_loss: 0.5714899897575378, val_acc: 0.984375
epoch: 877: train_loss: 0.6959225634733844, train_acc: 0.4806547562281291, val_loss: 0.582657516002655, val_acc: 0.96875
epoch: 878: train_loss: 0.6959596398114534, train_acc: 0.4910714228947957, val_loss: 0.5663046538829803, val_acc: 0.984375
epoch: 879: train_loss: 0.6958753440642004, train_acc: 0.5654761989911398, val_loss: 0.57383993268013, val_acc: 0.96875
epoch: 880: train_loss: 0.6957491626353421, train_acc: 0.5982142885526022, val_loss: 0.5670462548732758, val_acc: 0.984375
epoch: 881: train_loss: 0.6957087812104741, train_acc: 0.53125, val_loss: 0.5701256096363068, val_acc: 0.984375
epoch: 882: train_loss: 0.6957539208163049, train_acc: 0.4330357114473979, val_loss: 0.5826616287231445, val_acc: 0.96875
epoch: 883: train_loss: 0.6957513438945091, train_acc: 0.5104166666666666, val_loss: 0.6284536123275757, val_acc: 0.921875
epoch: 884: train_loss: 0.6957413236075659, train_acc: 0.523809532324473, val_loss: 0.5516092777252197, val_acc: 1.0
epoch: 885: train_loss: 0.6957036272038912, train_acc: 0.53125, val_loss: 0.6673192977905273, val_acc: 0.890625
epoch: 886: train_loss: 0.6959221981289431, train_acc: 0.4166666666666667, val_loss: 0.6079185009002686, val_acc: 0.953125
epoch: 887: train_loss: 0.695816571222949, train_acc: 0.636904756228129, val_loss: 0.5515959560871124, val_acc: 1.0
epoch: 888: train_loss: 0.6957721591085192, train_acc: 0.5461309552192688, val_loss: 0.5582623481750488, val_acc: 0.984375
epoch: 889: train_loss: 0.6957737726665176, train_acc: 0.4880952338377635, val_loss: 0.5745813250541687, val_acc: 0.984375
epoch: 890: train_loss: 0.695849740143979, train_acc: 0.4776785671710968, val_loss: 0.6424739360809326, val_acc: 0.90625
epoch: 891: train_loss: 0.6958782679564223, train_acc: 0.4836309552192688, val_loss: 0.5987748503684998, val_acc: 0.953125
epoch: 892: train_loss: 0.6959249246872471, train_acc: 0.4910714228947957, val_loss: 0.697380781173706, val_acc: 0.84375
epoch: 893: train_loss: 0.6958114764834418, train_acc: 0.6086309552192688, val_loss: 0.582335889339447, val_acc: 0.96875
epoch: 894: train_loss: 0.6956959075545917, train_acc: 0.601190467675527, val_loss: 0.5521278083324432, val_acc: 1.0
epoch: 895: train_loss: 0.6957172769387926, train_acc: 0.4732142885526021, val_loss: 0.5924981236457825, val_acc: 0.953125
epoch: 896: train_loss: 0.6957083701733016, train_acc: 0.5297619005044302, val_loss: 0.5517345666885376, val_acc: 1.0
epoch: 897: train_loss: 0.6957375612981077, train_acc: 0.5074404776096344, val_loss: 0.5537363886833191, val_acc: 1.0
epoch: 898: train_loss: 0.695689425806139, train_acc: 0.5505952338377634, val_loss: 0.5619930028915405, val_acc: 0.984375
epoch: 899: train_loss: 0.6957125206346874, train_acc: 0.511904756228129, val_loss: 0.5820978283882141, val_acc: 0.984375
epoch: 900: train_loss: 0.6956836999984224, train_acc: 0.5104166666666666, val_loss: 0.5718235075473785, val_acc: 0.984375
epoch: 901: train_loss: 0.6956941504348063, train_acc: 0.4970238109429677, val_loss: 0.6137329936027527, val_acc: 0.9375
epoch: 902: train_loss: 0.6956539071983593, train_acc: 0.5342261989911398, val_loss: 0.582788497209549, val_acc: 0.96875
epoch: 903: train_loss: 0.6957117464162614, train_acc: 0.4761904776096344, val_loss: 0.6139456629753113, val_acc: 0.9375
epoch: 904: train_loss: 0.6956554815035945, train_acc: 0.5505952338377634, val_loss: 0.5866649150848389, val_acc: 0.984375
epoch: 905: train_loss: 0.695685797464857, train_acc: 0.4657738109429677, val_loss: 0.6148440837860107, val_acc: 0.9375
epoch: 906: train_loss: 0.6956832416352878, train_acc: 0.5029761989911398, val_loss: 0.5517307817935944, val_acc: 1.0
epoch: 907: train_loss: 0.6957053677144737, train_acc: 0.4747023781140645, val_loss: 0.5514513552188873, val_acc: 1.0
epoch: 908: train_loss: 0.695740869882827, train_acc: 0.4553571442763011, val_loss: 0.5515153110027313, val_acc: 1.0
epoch: 909: train_loss: 0.6957228164533126, train_acc: 0.4985119005044301, val_loss: 0.5655986368656158, val_acc: 0.984375
epoch: 910: train_loss: 0.6956798325211329, train_acc: 0.5386904776096344, val_loss: 0.5615932643413544, val_acc: 0.984375
epoch: 911: train_loss: 0.6956909428083764, train_acc: 0.5133928656578064, val_loss: 0.6295267045497894, val_acc: 0.921875
epoch: 912: train_loss: 0.6957542103283079, train_acc: 0.4434523781140645, val_loss: 0.5718766152858734, val_acc: 0.984375
epoch: 913: train_loss: 0.6957267190008428, train_acc: 0.543154756228129, val_loss: 0.628974050283432, val_acc: 0.921875
epoch: 914: train_loss: 0.6956813342367155, train_acc: 0.538690467675527, val_loss: 0.5755643248558044, val_acc: 0.96875
epoch: 915: train_loss: 0.6956465236267849, train_acc: 0.5327380895614624, val_loss: 0.6136434972286224, val_acc: 0.9375
epoch: 916: train_loss: 0.6956349470449861, train_acc: 0.5074404776096344, val_loss: 0.5915973484516144, val_acc: 0.921875
epoch: 917: train_loss: 0.6956179267310895, train_acc: 0.5104166666666666, val_loss: 0.5516482591629028, val_acc: 1.0
epoch: 918: train_loss: 0.6955389823652418, train_acc: 0.5848214228947958, val_loss: 0.554061084985733, val_acc: 1.0
epoch: 919: train_loss: 0.6955685920905382, train_acc: 0.4925595323244731, val_loss: 0.5534333884716034, val_acc: 1.0
epoch: 920: train_loss: 0.6955675903843925, train_acc: 0.5029761989911398, val_loss: 0.6250163614749908, val_acc: 0.921875
epoch: 921: train_loss: 0.6955763910097749, train_acc: 0.4925595323244731, val_loss: 0.6235790848731995, val_acc: 0.921875
epoch: 922: train_loss: 0.69552967954345, train_acc: 0.5401785671710968, val_loss: 0.5549803376197815, val_acc: 1.0
epoch: 923: train_loss: 0.6956145066719558, train_acc: 0.4419642885526021, val_loss: 0.5514452755451202, val_acc: 1.0
epoch: 924: train_loss: 0.6955501927341434, train_acc: 0.6071428656578064, val_loss: 0.5670648813247681, val_acc: 0.984375
epoch: 925: train_loss: 0.6955989453756534, train_acc: 0.4494047562281291, val_loss: 0.5514782667160034, val_acc: 1.0
epoch: 926: train_loss: 0.6955277954704775, train_acc: 0.5461309552192688, val_loss: 0.6131492853164673, val_acc: 0.9375
epoch: 927: train_loss: 0.6954386845316702, train_acc: 0.5892857114473978, val_loss: 0.5514473915100098, val_acc: 1.0
epoch: 928: train_loss: 0.6954563090696024, train_acc: 0.4851190447807312, val_loss: 0.5952556431293488, val_acc: 0.921875
epoch: 929: train_loss: 0.6954206842889076, train_acc: 0.555059532324473, val_loss: 0.5631190538406372, val_acc: 0.984375
epoch: 930: train_loss: 0.6954239731700548, train_acc: 0.4895833333333333, val_loss: 0.5520395040512085, val_acc: 1.0
epoch: 931: train_loss: 0.6954583411607286, train_acc: 0.4910714228947957, val_loss: 0.5948843657970428, val_acc: 0.953125
epoch: 932: train_loss: 0.6953363538895736, train_acc: 0.632440467675527, val_loss: 0.6228235065937042, val_acc: 0.921875
epoch: 933: train_loss: 0.6953738439857413, train_acc: 0.4747023781140645, val_loss: 0.5515004992485046, val_acc: 1.0
epoch: 934: train_loss: 0.6952555395277645, train_acc: 0.6205357114473978, val_loss: 0.5514571368694305, val_acc: 1.0
epoch: 935: train_loss: 0.6952413979418943, train_acc: 0.5252976218859354, val_loss: 0.5524865686893463, val_acc: 1.0
epoch: 936: train_loss: 0.6953045217136782, train_acc: 0.4553571442763011, val_loss: 0.5518929362297058, val_acc: 1.0
epoch: 937: train_loss: 0.6952411509949692, train_acc: 0.5610119005044302, val_loss: 0.5801034867763519, val_acc: 0.96875
epoch: 938: train_loss: 0.6950730712311386, train_acc: 0.6860119104385376, val_loss: 0.5594300329685211, val_acc: 1.0
epoch: 939: train_loss: 0.695061828439118, train_acc: 0.5416666666666666, val_loss: 0.552136242389679, val_acc: 1.0
epoch: 940: train_loss: 0.6950464428125699, train_acc: 0.5193452338377634, val_loss: 0.5670116245746613, val_acc: 0.984375
epoch: 941: train_loss: 0.6949062403304652, train_acc: 0.6666666666666666, val_loss: 0.6129853129386902, val_acc: 0.9375
epoch: 942: train_loss: 0.6948314044702266, train_acc: 0.5625, val_loss: 0.5560308396816254, val_acc: 1.0
epoch: 943: train_loss: 0.6948813332592034, train_acc: 0.4270833333333333, val_loss: 0.5604649484157562, val_acc: 1.0
epoch: 944: train_loss: 0.6949313757701322, train_acc: 0.4583333333333333, val_loss: 0.5671530663967133, val_acc: 0.984375
epoch: 945: train_loss: 0.6949040481454481, train_acc: 0.5446428656578064, val_loss: 0.5891216695308685, val_acc: 0.984375
epoch: 946: train_loss: 0.6948846502230234, train_acc: 0.5208333333333334, val_loss: 0.551579087972641, val_acc: 1.0
epoch: 947: train_loss: 0.6948088835269932, train_acc: 0.5639880895614624, val_loss: 0.5812733471393585, val_acc: 0.96875
epoch: 948: train_loss: 0.6947208626001483, train_acc: 0.5758928656578064, val_loss: 0.5664360523223877, val_acc: 1.0
epoch: 949: train_loss: 0.6948135874564193, train_acc: 0.4776785671710968, val_loss: 0.5808840990066528, val_acc: 0.96875
epoch: 950: train_loss: 0.6948981715259831, train_acc: 0.4196428557236989, val_loss: 0.5538278520107269, val_acc: 1.0
epoch: 951: train_loss: 0.6948428559286592, train_acc: 0.53125, val_loss: 0.5798884928226471, val_acc: 0.96875
epoch: 952: train_loss: 0.694826572268274, train_acc: 0.5, val_loss: 0.5765343308448792, val_acc: 0.96875
epoch: 953: train_loss: 0.6948860003388408, train_acc: 0.4255952338377635, val_loss: 0.6289760172367096, val_acc: 0.921875
epoch: 954: train_loss: 0.6948514788354676, train_acc: 0.5327380895614624, val_loss: 0.5524490773677826, val_acc: 1.0
epoch: 955: train_loss: 0.6948231846676051, train_acc: 0.5014880895614624, val_loss: 0.5757604539394379, val_acc: 0.96875
epoch: 956: train_loss: 0.694847049405043, train_acc: 0.4761904776096344, val_loss: 0.6275768578052521, val_acc: 0.921875
epoch: 957: train_loss: 0.694853461361131, train_acc: 0.5148809552192688, val_loss: 0.5515129268169403, val_acc: 1.0
epoch: 958: train_loss: 0.694891158619564, train_acc: 0.4523809552192688, val_loss: 0.562034398317337, val_acc: 0.984375
epoch: 959: train_loss: 0.6949341226576108, train_acc: 0.4449404776096344, val_loss: 0.570111870765686, val_acc: 0.984375
epoch: 960: train_loss: 0.6949196588525833, train_acc: 0.5208333333333334, val_loss: 0.5781213045120239, val_acc: 0.96875
epoch: 961: train_loss: 0.6948823896820816, train_acc: 0.5684523781140646, val_loss: 0.5674132406711578, val_acc: 0.984375
epoch: 962: train_loss: 0.6948743532503634, train_acc: 0.5014880895614624, val_loss: 0.5929365754127502, val_acc: 0.953125
epoch: 963: train_loss: 0.6948991582354079, train_acc: 0.4925595323244731, val_loss: 0.5982689261436462, val_acc: 0.953125
epoch: 964: train_loss: 0.6948713512000647, train_acc: 0.5461309552192688, val_loss: 0.6119773685932159, val_acc: 0.9375
epoch: 965: train_loss: 0.6948631777073948, train_acc: 0.5223214228947958, val_loss: 0.5514990389347076, val_acc: 1.0
epoch: 966: train_loss: 0.694898355504391, train_acc: 0.4776785671710968, val_loss: 0.5671467781066895, val_acc: 0.984375
epoch: 967: train_loss: 0.6949528797886264, train_acc: 0.4434523781140645, val_loss: 0.5701353847980499, val_acc: 0.984375
epoch: 968: train_loss: 0.694969320711419, train_acc: 0.4791666666666667, val_loss: 0.5514465570449829, val_acc: 1.0
epoch: 969: train_loss: 0.695006909546574, train_acc: 0.4702380895614624, val_loss: 0.5670765042304993, val_acc: 0.984375
epoch: 970: train_loss: 0.6950146342895308, train_acc: 0.5267857114473978, val_loss: 0.566391259431839, val_acc: 0.984375
epoch: 971: train_loss: 0.6949998411990002, train_acc: 0.517857144276301, val_loss: 0.5515271127223969, val_acc: 1.0
epoch: 972: train_loss: 0.6950510751102513, train_acc: 0.4375, val_loss: 0.5514473617076874, val_acc: 1.0
epoch: 973: train_loss: 0.6951648467497664, train_acc: 0.3854166666666667, val_loss: 0.5816937983036041, val_acc: 0.9375
epoch: 974: train_loss: 0.6952010412909032, train_acc: 0.4494047562281291, val_loss: 0.6151042878627777, val_acc: 0.9375
epoch: 975: train_loss: 0.6951926275273496, train_acc: 0.4895833333333333, val_loss: 0.5671536028385162, val_acc: 0.984375
epoch: 976: train_loss: 0.6951891797185152, train_acc: 0.5297619005044302, val_loss: 0.5845076143741608, val_acc: 0.96875
epoch: 977: train_loss: 0.6951612921173917, train_acc: 0.5148809552192688, val_loss: 0.6268169581890106, val_acc: 0.921875
epoch: 978: train_loss: 0.6951678388980044, train_acc: 0.53125, val_loss: 0.6150846183300018, val_acc: 0.9375
epoch: 979: train_loss: 0.6951712198403422, train_acc: 0.4985119005044301, val_loss: 0.5516247749328613, val_acc: 1.0
epoch: 980: train_loss: 0.6951654944499258, train_acc: 0.5104166666666666, val_loss: 0.6758382320404053, val_acc: 0.875
epoch: 981: train_loss: 0.6951246094679401, train_acc: 0.5714285771052042, val_loss: 0.5588656961917877, val_acc: 0.984375
epoch: 982: train_loss: 0.6950604105933561, train_acc: 0.5639880895614624, val_loss: 0.5764373540878296, val_acc: 0.96875
epoch: 983: train_loss: 0.6950288098154035, train_acc: 0.5505952338377634, val_loss: 0.551445871591568, val_acc: 1.0
epoch: 984: train_loss: 0.695079051480076, train_acc: 0.4345238109429677, val_loss: 0.5514478087425232, val_acc: 1.0
epoch: 985: train_loss: 0.6951188045711924, train_acc: 0.4553571442763011, val_loss: 0.5514785945415497, val_acc: 1.0
epoch: 986: train_loss: 0.6950776228547223, train_acc: 0.5372023781140646, val_loss: 0.5654840469360352, val_acc: 0.984375
epoch: 987: train_loss: 0.6950590713867137, train_acc: 0.523809532324473, val_loss: 0.6139600574970245, val_acc: 0.9375
epoch: 988: train_loss: 0.6949940241263061, train_acc: 0.555059532324473, val_loss: 0.5771768093109131, val_acc: 0.96875
epoch: 989: train_loss: 0.6950027148731636, train_acc: 0.5014880895614624, val_loss: 0.5670705139636993, val_acc: 0.984375
epoch: 990: train_loss: 0.6949352758162524, train_acc: 0.5997023781140646, val_loss: 0.5670793652534485, val_acc: 0.984375
epoch: 991: train_loss: 0.6949298304695923, train_acc: 0.4985119005044301, val_loss: 0.568262368440628, val_acc: 0.984375
epoch: 992: train_loss: 0.6947892554136849, train_acc: 0.6547619104385376, val_loss: 0.5809965133666992, val_acc: 0.96875
epoch: 993: train_loss: 0.6947389791109359, train_acc: 0.549107144276301, val_loss: 0.5849568247795105, val_acc: 0.96875
epoch: 994: train_loss: 0.6947714814388978, train_acc: 0.4479166666666667, val_loss: 0.58202263712883, val_acc: 0.96875
epoch: 995: train_loss: 0.6947829070659373, train_acc: 0.4791666666666667, val_loss: 0.5514478981494904, val_acc: 1.0
epoch: 996: train_loss: 0.6947199868063836, train_acc: 0.5535714228947958, val_loss: 0.6140242218971252, val_acc: 0.9375
epoch: 997: train_loss: 0.6946642978357017, train_acc: 0.5461309552192688, val_loss: 0.5633067488670349, val_acc: 0.984375
epoch: 998: train_loss: 0.6945534516144568, train_acc: 0.6577380895614624, val_loss: 0.5514448881149292, val_acc: 1.0
epoch: 999: train_loss: 0.6944749175508823, train_acc: 0.5997023781140646, val_loss: 0.5521049201488495, val_acc: 1.0
1.030704289674759 0.5260416567325592
GroundTruth:  Alligator Cracks Alligator Cracks Alligator Cracks Alligator Cracks
Accuracy of the network on the test images: 46 %
Accuracy for class: Alligator Cracks is 0.0 %
Accuracy for class: Longitudinal Cracks is 66.7 %
Accuracy for class: Transverse Cracks is 40.0 %
