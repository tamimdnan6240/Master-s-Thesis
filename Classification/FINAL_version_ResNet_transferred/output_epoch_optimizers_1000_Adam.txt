cpu
Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Alligator Cracks Alligator Cracks Longitudinal Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.2345792452494304, train_acc: 0.4002976218859355, val_loss: 1.0353633761405945, val_acc: 0.5
epoch: 1: train_loss: 1.2004906634489696, train_acc: 0.3794642885526021, val_loss: 1.0721932649612427, val_acc: 0.453125
epoch: 2: train_loss: 1.1910863121350606, train_acc: 0.3244047661622365, val_loss: 1.047416865825653, val_acc: 0.421875
epoch: 3: train_loss: 1.1441837400197983, train_acc: 0.4002976218859355, val_loss: 1.0084187984466553, val_acc: 0.453125
epoch: 4: train_loss: 1.119126804669698, train_acc: 0.3377976218859355, val_loss: 1.0440976321697235, val_acc: 0.390625
epoch: 5: train_loss: 1.0943080451753404, train_acc: 0.4122023781140645, val_loss: 0.9595897197723389, val_acc: 0.640625
epoch: 6: train_loss: 1.0751360115550814, train_acc: 0.3958333333333333, val_loss: 0.8941104710102081, val_acc: 0.796875
epoch: 7: train_loss: 1.0598090688387554, train_acc: 0.4255952338377635, val_loss: 0.9763067960739136, val_acc: 0.65625
epoch: 8: train_loss: 1.0388021954783688, train_acc: 0.5163690447807312, val_loss: 0.9277500510215759, val_acc: 0.625
epoch: 9: train_loss: 1.02454105814298, train_acc: 0.4523809552192688, val_loss: 0.816213071346283, val_acc: 0.78125
epoch: 10: train_loss: 1.0108333175832576, train_acc: 0.4568452338377635, val_loss: 0.8416608273983002, val_acc: 0.765625
epoch: 11: train_loss: 1.0009457386202283, train_acc: 0.4657738109429677, val_loss: 0.991229236125946, val_acc: 0.53125
epoch: 12: train_loss: 0.9890923973841546, train_acc: 0.4821428656578064, val_loss: 0.9230433404445648, val_acc: 0.5625
epoch: 13: train_loss: 0.9822340820516858, train_acc: 0.4523809552192688, val_loss: 0.849888265132904, val_acc: 0.71875
epoch: 14: train_loss: 0.9738518092367384, train_acc: 0.4226190447807312, val_loss: 0.8222645223140717, val_acc: 0.734375
epoch: 15: train_loss: 0.9619899317622185, train_acc: 0.555059532324473, val_loss: 0.8068491816520691, val_acc: 0.75
epoch: 16: train_loss: 0.9567350756888295, train_acc: 0.4523809552192688, val_loss: 0.943527102470398, val_acc: 0.625
epoch: 17: train_loss: 0.9530273168175307, train_acc: 0.4434523781140645, val_loss: 0.8134841620922089, val_acc: 0.765625
epoch: 18: train_loss: 0.947461047716308, train_acc: 0.4241071442763011, val_loss: 0.791358083486557, val_acc: 0.78125
epoch: 19: train_loss: 0.9435446749130885, train_acc: 0.4836309552192688, val_loss: 0.927547425031662, val_acc: 0.640625
epoch: 20: train_loss: 0.9389164286946493, train_acc: 0.4880952338377635, val_loss: 0.942577451467514, val_acc: 0.625
epoch: 21: train_loss: 0.9392059264761028, train_acc: 0.3586309552192688, val_loss: 0.8665337264537811, val_acc: 0.671875
epoch: 22: train_loss: 0.9371020318805307, train_acc: 0.3824404776096344, val_loss: 0.9558554291725159, val_acc: 0.59375
epoch: 23: train_loss: 0.9333091643121506, train_acc: 0.4464285671710968, val_loss: 0.8348790109157562, val_acc: 0.734375
epoch: 24: train_loss: 0.9294979786872862, train_acc: 0.4836309552192688, val_loss: 0.9128392040729523, val_acc: 0.640625
epoch: 25: train_loss: 0.928156751852769, train_acc: 0.4092261890570323, val_loss: 0.9454193413257599, val_acc: 0.609375
epoch: 26: train_loss: 0.9264238903551925, train_acc: 0.4568452338377635, val_loss: 0.8252213597297668, val_acc: 0.734375
epoch: 27: train_loss: 0.9236627306256974, train_acc: 0.511904756228129, val_loss: 0.8338368237018585, val_acc: 0.71875
epoch: 28: train_loss: 0.924683767488633, train_acc: 0.4107142885526021, val_loss: 0.7765417098999023, val_acc: 0.78125
epoch: 29: train_loss: 0.921671093834771, train_acc: 0.5059523781140646, val_loss: 0.8269695341587067, val_acc: 0.75
epoch: 30: train_loss: 0.921780390124167, train_acc: 0.4553571442763011, val_loss: 0.7786263227462769, val_acc: 0.78125
epoch: 31: train_loss: 0.9189061963309844, train_acc: 0.5193452338377634, val_loss: 0.8579520583152771, val_acc: 0.6875
epoch: 32: train_loss: 0.9152759054694513, train_acc: 0.5386904776096344, val_loss: 0.837924987077713, val_acc: 0.71875
epoch: 33: train_loss: 0.9128269389563916, train_acc: 0.4419642885526021, val_loss: 0.7681969702243805, val_acc: 0.78125
epoch: 34: train_loss: 0.911581502074287, train_acc: 0.4538690447807312, val_loss: 0.8115669190883636, val_acc: 0.734375
epoch: 35: train_loss: 0.9101927076224927, train_acc: 0.4255952338377635, val_loss: 0.8096114695072174, val_acc: 0.734375
epoch: 36: train_loss: 0.9084277512790919, train_acc: 0.4791666666666667, val_loss: 0.7676756381988525, val_acc: 0.78125
epoch: 37: train_loss: 0.9085720991879178, train_acc: 0.3809523781140645, val_loss: 0.8380291163921356, val_acc: 0.71875
epoch: 38: train_loss: 0.9088555854609888, train_acc: 0.4017857114473979, val_loss: 0.7212885916233063, val_acc: 0.828125
epoch: 39: train_loss: 0.9073840354879696, train_acc: 0.4761904776096344, val_loss: 0.8100247085094452, val_acc: 0.734375
epoch: 40: train_loss: 0.9071596289068702, train_acc: 0.4345238109429677, val_loss: 0.9019640684127808, val_acc: 0.640625
epoch: 41: train_loss: 0.9039807447365352, train_acc: 0.5491071343421936, val_loss: 0.8082623779773712, val_acc: 0.734375
epoch: 42: train_loss: 0.9024476852527884, train_acc: 0.4434523781140645, val_loss: 0.7222236692905426, val_acc: 0.828125
epoch: 43: train_loss: 0.9020894494923678, train_acc: 0.4226190447807312, val_loss: 0.952844649553299, val_acc: 0.578125
epoch: 44: train_loss: 0.9031081036285117, train_acc: 0.4017857114473979, val_loss: 0.8229940533638, val_acc: 0.734375
epoch: 45: train_loss: 0.9011132121086121, train_acc: 0.5, val_loss: 0.8082743287086487, val_acc: 0.734375
epoch: 46: train_loss: 0.8993937952298644, train_acc: 0.5342261989911398, val_loss: 0.8131963014602661, val_acc: 0.71875
epoch: 47: train_loss: 0.8978128685719436, train_acc: 0.5252976218859354, val_loss: 0.8946260213851929, val_acc: 0.640625
epoch: 48: train_loss: 0.8960101734213277, train_acc: 0.5133928656578064, val_loss: 0.7662039697170258, val_acc: 0.78125
epoch: 49: train_loss: 0.8962993649641672, train_acc: 0.3913690447807312, val_loss: 0.7214174270629883, val_acc: 0.828125
epoch: 50: train_loss: 0.8953336640900256, train_acc: 0.5014880895614624, val_loss: 0.8053630292415619, val_acc: 0.734375
epoch: 51: train_loss: 0.8944115459154813, train_acc: 0.4553571442763011, val_loss: 0.7380799651145935, val_acc: 0.796875
epoch: 52: train_loss: 0.8933873716390357, train_acc: 0.5, val_loss: 0.7453950643539429, val_acc: 0.71875
epoch: 53: train_loss: 0.8935044952380804, train_acc: 0.40625, val_loss: 0.74796462059021, val_acc: 0.71875
epoch: 54: train_loss: 0.8925445130377104, train_acc: 0.5044642885526022, val_loss: 0.9134091138839722, val_acc: 0.640625
epoch: 55: train_loss: 0.8919484884965987, train_acc: 0.4166666666666667, val_loss: 0.8178297877311707, val_acc: 0.734375
epoch: 56: train_loss: 0.8920475526859885, train_acc: 0.3705357114473979, val_loss: 0.7773658335208893, val_acc: 0.78125
epoch: 57: train_loss: 0.891081948389952, train_acc: 0.507440467675527, val_loss: 0.762291669845581, val_acc: 0.78125
epoch: 58: train_loss: 0.8903453457826946, train_acc: 0.4553571442763011, val_loss: 0.9590065181255341, val_acc: 0.578125
epoch: 59: train_loss: 0.8916320704751542, train_acc: 0.3630952338377635, val_loss: 0.9425399601459503, val_acc: 0.59375
epoch: 60: train_loss: 0.8917552594278677, train_acc: 0.4122023781140645, val_loss: 0.9315932989120483, val_acc: 0.609375
epoch: 61: train_loss: 0.8914540877906222, train_acc: 0.4553571442763011, val_loss: 0.8265476822853088, val_acc: 0.703125
epoch: 62: train_loss: 0.8904360841821739, train_acc: 0.5193452338377634, val_loss: 0.8318733274936676, val_acc: 0.734375
epoch: 63: train_loss: 0.891093403411408, train_acc: 0.3616071442763011, val_loss: 0.868967592716217, val_acc: 0.6875
epoch: 64: train_loss: 0.8904926089140082, train_acc: 0.5089285671710968, val_loss: 0.7639244496822357, val_acc: 0.78125
epoch: 65: train_loss: 0.8900851902335578, train_acc: 0.4270833333333333, val_loss: 0.8441297113895416, val_acc: 0.6875
epoch: 66: train_loss: 0.8900445228192341, train_acc: 0.4404761989911397, val_loss: 0.8531237542629242, val_acc: 0.703125
epoch: 67: train_loss: 0.8908862173557278, train_acc: 0.3779761890570323, val_loss: 0.7702522873878479, val_acc: 0.78125
epoch: 68: train_loss: 0.8900604106953751, train_acc: 0.4449404776096344, val_loss: 0.7946079969406128, val_acc: 0.78125
epoch: 69: train_loss: 0.8901678332260674, train_acc: 0.4494047562281291, val_loss: 0.8184103071689606, val_acc: 0.734375
epoch: 70: train_loss: 0.88957714139016, train_acc: 0.4538690447807312, val_loss: 0.7669346034526825, val_acc: 0.78125
epoch: 71: train_loss: 0.8898299634456632, train_acc: 0.4077380994955699, val_loss: 0.8154497444629669, val_acc: 0.734375
epoch: 72: train_loss: 0.8891197557318699, train_acc: 0.4761904776096344, val_loss: 0.8060488104820251, val_acc: 0.734375
epoch: 73: train_loss: 0.890099269312781, train_acc: 0.3229166666666667, val_loss: 0.8125467300415039, val_acc: 0.734375
epoch: 74: train_loss: 0.8902634037865531, train_acc: 0.4270833333333333, val_loss: 0.8517732620239258, val_acc: 0.6875
epoch: 75: train_loss: 0.8893942995029582, train_acc: 0.4880952338377635, val_loss: 0.8193221092224121, val_acc: 0.734375
epoch: 76: train_loss: 0.8895234986817165, train_acc: 0.4151785671710968, val_loss: 0.8182019591331482, val_acc: 0.734375
epoch: 77: train_loss: 0.8885336974237718, train_acc: 0.4761904676755269, val_loss: 0.8267127573490143, val_acc: 0.734375
epoch: 78: train_loss: 0.887803706187236, train_acc: 0.5014880994955698, val_loss: 0.8782705366611481, val_acc: 0.65625
epoch: 79: train_loss: 0.88780184139808, train_acc: 0.4732142885526021, val_loss: 0.8346302211284637, val_acc: 0.71875
epoch: 80: train_loss: 0.8871680326422545, train_acc: 0.4791666666666667, val_loss: 0.8552275002002716, val_acc: 0.6875
epoch: 81: train_loss: 0.8878878657895374, train_acc: 0.3377976218859355, val_loss: 0.8443114161491394, val_acc: 0.671875
epoch: 82: train_loss: 0.8881992178269658, train_acc: 0.4017857114473979, val_loss: 0.7955678999423981, val_acc: 0.78125
epoch: 83: train_loss: 0.8881850994768595, train_acc: 0.4375, val_loss: 0.8454344272613525, val_acc: 0.6875
epoch: 84: train_loss: 0.8860668306257209, train_acc: 0.5505952338377634, val_loss: 0.6491661667823792, val_acc: 0.9375
epoch: 85: train_loss: 0.8855095972386441, train_acc: 0.4598214228947957, val_loss: 0.599848210811615, val_acc: 0.984375
epoch: 86: train_loss: 0.884656463317944, train_acc: 0.4419642885526021, val_loss: 0.781527191400528, val_acc: 0.71875
epoch: 87: train_loss: 0.8834898442481502, train_acc: 0.4851190447807312, val_loss: 0.6155610084533691, val_acc: 0.921875
epoch: 88: train_loss: 0.8823874074868048, train_acc: 0.5148809552192688, val_loss: 0.8667354583740234, val_acc: 0.703125
epoch: 89: train_loss: 0.8817981673611534, train_acc: 0.511904756228129, val_loss: 0.6490151584148407, val_acc: 0.90625
epoch: 90: train_loss: 0.8808146590714925, train_acc: 0.4360119005044301, val_loss: 0.6745122075080872, val_acc: 0.84375
epoch: 91: train_loss: 0.8797430663868999, train_acc: 0.4895833333333333, val_loss: 0.697162926197052, val_acc: 0.796875
epoch: 92: train_loss: 0.8793295136916592, train_acc: 0.4077380994955699, val_loss: 0.6580645143985748, val_acc: 0.859375
epoch: 93: train_loss: 0.8782173752361999, train_acc: 0.4464285671710968, val_loss: 0.5887337327003479, val_acc: 0.984375
epoch: 94: train_loss: 0.8770109109711227, train_acc: 0.4761904776096344, val_loss: 0.6036246716976166, val_acc: 0.921875
epoch: 95: train_loss: 0.8751841048813526, train_acc: 0.5252976218859354, val_loss: 0.5895214378833771, val_acc: 0.96875
epoch: 96: train_loss: 0.8731828788711441, train_acc: 0.5416666666666666, val_loss: 0.5865033566951752, val_acc: 0.984375
epoch: 97: train_loss: 0.8717679961198039, train_acc: 0.4970238109429677, val_loss: 0.563155472278595, val_acc: 1.0
epoch: 98: train_loss: 0.8699424595945208, train_acc: 0.5297619005044302, val_loss: 0.6517935991287231, val_acc: 0.921875
epoch: 99: train_loss: 0.8683035800854363, train_acc: 0.5386904776096344, val_loss: 0.5788635611534119, val_acc: 0.96875
epoch: 100: train_loss: 0.8666971996672473, train_acc: 0.4880952338377635, val_loss: 0.5709400475025177, val_acc: 1.0
epoch: 101: train_loss: 0.864867134811052, train_acc: 0.5223214228947958, val_loss: 0.5765613317489624, val_acc: 0.984375
epoch: 102: train_loss: 0.8629351439599464, train_acc: 0.5788690447807312, val_loss: 0.5995035469532013, val_acc: 0.9375
epoch: 103: train_loss: 0.8609949891001748, train_acc: 0.5803571343421936, val_loss: 0.5711438357830048, val_acc: 0.96875
epoch: 104: train_loss: 0.8595001559408882, train_acc: 0.5133928656578064, val_loss: 0.5754445791244507, val_acc: 0.984375
epoch: 105: train_loss: 0.8576759248034758, train_acc: 0.555059532324473, val_loss: 0.5598685443401337, val_acc: 1.0
epoch: 106: train_loss: 0.8558095784573538, train_acc: 0.5758928656578064, val_loss: 0.5742233097553253, val_acc: 0.984375
epoch: 107: train_loss: 0.8548005188320887, train_acc: 0.4970238109429677, val_loss: 0.6254813969135284, val_acc: 0.9375
epoch: 108: train_loss: 0.8537403383386242, train_acc: 0.4747023781140645, val_loss: 0.5914770364761353, val_acc: 0.96875
epoch: 109: train_loss: 0.8526618117635898, train_acc: 0.5267857114473978, val_loss: 0.6112761199474335, val_acc: 0.90625
epoch: 110: train_loss: 0.8508619809293888, train_acc: 0.5877976218859354, val_loss: 0.5850873589515686, val_acc: 0.984375
epoch: 111: train_loss: 0.8490953735475026, train_acc: 0.5625, val_loss: 0.5817891359329224, val_acc: 1.0
epoch: 112: train_loss: 0.8482365679424418, train_acc: 0.4449404776096344, val_loss: 0.5745580196380615, val_acc: 0.984375
epoch: 113: train_loss: 0.8476037967797605, train_acc: 0.4151785671710968, val_loss: 0.5604372024536133, val_acc: 1.0
epoch: 114: train_loss: 0.8467883788157198, train_acc: 0.4836309552192688, val_loss: 0.5742315649986267, val_acc: 1.0
epoch: 115: train_loss: 0.8455918916854362, train_acc: 0.4925595323244731, val_loss: 0.5556004643440247, val_acc: 1.0
epoch: 116: train_loss: 0.8442039030399754, train_acc: 0.5357142885526022, val_loss: 0.6622986793518066, val_acc: 0.921875
epoch: 117: train_loss: 0.8426421903452629, train_acc: 0.5267857114473978, val_loss: 0.6182468831539154, val_acc: 0.9375
epoch: 118: train_loss: 0.8415348104903008, train_acc: 0.511904756228129, val_loss: 0.5763026177883148, val_acc: 0.984375
epoch: 119: train_loss: 0.8404050062927932, train_acc: 0.4985119005044301, val_loss: 0.5565332472324371, val_acc: 1.0
epoch: 120: train_loss: 0.8391231212241588, train_acc: 0.4955357114473979, val_loss: 0.5886141061782837, val_acc: 0.984375
epoch: 121: train_loss: 0.8381294773413182, train_acc: 0.4851190447807312, val_loss: 0.5689280033111572, val_acc: 0.984375
epoch: 122: train_loss: 0.8370766333609736, train_acc: 0.5074404776096344, val_loss: 0.5544783174991608, val_acc: 1.0
epoch: 123: train_loss: 0.83573769986309, train_acc: 0.53125, val_loss: 0.5629752576351166, val_acc: 1.0
epoch: 124: train_loss: 0.834944947004318, train_acc: 0.5029761989911398, val_loss: 0.5715824663639069, val_acc: 0.984375
epoch: 125: train_loss: 0.8337160152575324, train_acc: 0.5044642885526022, val_loss: 0.5629971325397491, val_acc: 1.0
epoch: 126: train_loss: 0.8322402640121187, train_acc: 0.5342261989911398, val_loss: 0.6163314580917358, val_acc: 0.9375
epoch: 127: train_loss: 0.8319087830216932, train_acc: 0.3883928557236989, val_loss: 0.5716201961040497, val_acc: 1.0
epoch: 128: train_loss: 0.8308229035955373, train_acc: 0.4895833333333333, val_loss: 0.5705533921718597, val_acc: 0.984375
epoch: 129: train_loss: 0.8293860009847541, train_acc: 0.5773809552192688, val_loss: 0.6030169129371643, val_acc: 0.9375
epoch: 130: train_loss: 0.828912275513923, train_acc: 0.4300595323244731, val_loss: 0.582184910774231, val_acc: 0.96875
epoch: 131: train_loss: 0.8279521788160004, train_acc: 0.5029761989911398, val_loss: 0.6251930594444275, val_acc: 0.9375
epoch: 132: train_loss: 0.8268033955479622, train_acc: 0.5327380895614624, val_loss: 0.6222296059131622, val_acc: 0.953125
epoch: 133: train_loss: 0.8260501179084253, train_acc: 0.5282738109429678, val_loss: 0.5573378205299377, val_acc: 1.0
epoch: 134: train_loss: 0.8247745672125872, train_acc: 0.574404756228129, val_loss: 0.5813241004943848, val_acc: 1.0
epoch: 135: train_loss: 0.8238176572702676, train_acc: 0.5, val_loss: 0.560936450958252, val_acc: 1.0
epoch: 136: train_loss: 0.8228531548721655, train_acc: 0.4955357114473979, val_loss: 0.5665794610977173, val_acc: 1.0
epoch: 137: train_loss: 0.8218261881007086, train_acc: 0.4880952338377635, val_loss: 0.5576260089874268, val_acc: 1.0
epoch: 138: train_loss: 0.8209832002647772, train_acc: 0.4910714228947957, val_loss: 0.5688237547874451, val_acc: 1.0
epoch: 139: train_loss: 0.8202376543765973, train_acc: 0.4866071442763011, val_loss: 0.5668406188488007, val_acc: 1.0
epoch: 140: train_loss: 0.8191994098202272, train_acc: 0.5401785671710968, val_loss: 0.5580404996871948, val_acc: 1.0
epoch: 141: train_loss: 0.818268611047749, train_acc: 0.5014880895614624, val_loss: 0.5594620406627655, val_acc: 1.0
epoch: 142: train_loss: 0.816591478241629, train_acc: 0.6220238010088602, val_loss: 0.5565979182720184, val_acc: 1.0
epoch: 143: train_loss: 0.8155843004998231, train_acc: 0.523809532324473, val_loss: 0.5662883818149567, val_acc: 0.984375
epoch: 144: train_loss: 0.8147802265896192, train_acc: 0.523809532324473, val_loss: 0.5666700303554535, val_acc: 0.984375
epoch: 145: train_loss: 0.8135101752330177, train_acc: 0.5833333333333334, val_loss: 0.5573452413082123, val_acc: 1.0
epoch: 146: train_loss: 0.812122307329221, train_acc: 0.5922619104385376, val_loss: 0.5600138604640961, val_acc: 1.0
epoch: 147: train_loss: 0.8113640751521866, train_acc: 0.4806547562281291, val_loss: 0.5892851948738098, val_acc: 0.921875
epoch: 148: train_loss: 0.8108032322436636, train_acc: 0.4598214228947957, val_loss: 0.5887100994586945, val_acc: 0.984375
epoch: 149: train_loss: 0.8100994749201668, train_acc: 0.4836309552192688, val_loss: 0.6017922759056091, val_acc: 0.953125
epoch: 150: train_loss: 0.8096050508895982, train_acc: 0.4821428656578064, val_loss: 0.5792995095252991, val_acc: 0.96875
epoch: 151: train_loss: 0.8086832866987637, train_acc: 0.550595243771871, val_loss: 0.5636049807071686, val_acc: 0.984375
epoch: 152: train_loss: 0.8080018037414757, train_acc: 0.4970238010088603, val_loss: 0.5634029805660248, val_acc: 0.984375
epoch: 153: train_loss: 0.8065925981704289, train_acc: 0.6220238010088602, val_loss: 0.5703288912773132, val_acc: 0.984375
epoch: 154: train_loss: 0.8059563152892615, train_acc: 0.5223214228947958, val_loss: 0.6246627867221832, val_acc: 0.921875
epoch: 155: train_loss: 0.805461895198394, train_acc: 0.4880952338377635, val_loss: 0.6156791150569916, val_acc: 0.9375
epoch: 156: train_loss: 0.8051659699584773, train_acc: 0.4241071442763011, val_loss: 0.5586011409759521, val_acc: 1.0
epoch: 157: train_loss: 0.8045238275945437, train_acc: 0.5104166666666666, val_loss: 0.5626210272312164, val_acc: 1.0
epoch: 158: train_loss: 0.8040478912414494, train_acc: 0.4672619005044301, val_loss: 0.5563107430934906, val_acc: 1.0
epoch: 159: train_loss: 0.8024499041338761, train_acc: 0.6547619104385376, val_loss: 0.6329269409179688, val_acc: 0.921875
epoch: 160: train_loss: 0.8019017477213224, train_acc: 0.4747023781140645, val_loss: 0.563884049654007, val_acc: 0.984375
epoch: 161: train_loss: 0.8013600397993015, train_acc: 0.46875, val_loss: 0.5557339787483215, val_acc: 1.0
epoch: 162: train_loss: 0.8003327828243465, train_acc: 0.5982142885526022, val_loss: 0.5796795785427094, val_acc: 0.984375
epoch: 163: train_loss: 0.7995339954287054, train_acc: 0.5446428656578064, val_loss: 0.5607400238513947, val_acc: 1.0
epoch: 164: train_loss: 0.7981601951098198, train_acc: 0.6413690447807312, val_loss: 0.5547689199447632, val_acc: 1.0
epoch: 165: train_loss: 0.7972339796253953, train_acc: 0.5610119005044302, val_loss: 0.5660929679870605, val_acc: 0.984375
epoch: 166: train_loss: 0.7963830396800696, train_acc: 0.550595243771871, val_loss: 0.5671935081481934, val_acc: 1.0
epoch: 167: train_loss: 0.7959746989229368, train_acc: 0.5089285671710968, val_loss: 0.6162577569484711, val_acc: 0.921875
epoch: 168: train_loss: 0.7948080890277432, train_acc: 0.6116071343421936, val_loss: 0.572829008102417, val_acc: 0.96875
epoch: 169: train_loss: 0.7941726136441322, train_acc: 0.5401785671710968, val_loss: 0.5613782703876495, val_acc: 1.0
epoch: 170: train_loss: 0.7934016695729006, train_acc: 0.543154756228129, val_loss: 0.579545259475708, val_acc: 1.0
epoch: 171: train_loss: 0.7931978912778601, train_acc: 0.5089285671710968, val_loss: 0.5665695667266846, val_acc: 0.984375
epoch: 172: train_loss: 0.7923266247977179, train_acc: 0.5654761989911398, val_loss: 0.5653820931911469, val_acc: 1.0
epoch: 173: train_loss: 0.7918125868300365, train_acc: 0.4955357114473979, val_loss: 0.557759016752243, val_acc: 1.0
epoch: 174: train_loss: 0.7908776400202794, train_acc: 0.5535714228947958, val_loss: 0.5569429397583008, val_acc: 1.0
epoch: 175: train_loss: 0.7904887185855342, train_acc: 0.4880952338377635, val_loss: 0.5609391629695892, val_acc: 0.984375
epoch: 176: train_loss: 0.7901457843358455, train_acc: 0.4732142885526021, val_loss: 0.5686482191085815, val_acc: 1.0
epoch: 177: train_loss: 0.7899441395359535, train_acc: 0.4851190447807312, val_loss: 0.587946742773056, val_acc: 0.984375
epoch: 178: train_loss: 0.7886878532848318, train_acc: 0.632440467675527, val_loss: 0.5729095637798309, val_acc: 0.984375
epoch: 179: train_loss: 0.787577554804307, train_acc: 0.6071428656578064, val_loss: 0.5631685256958008, val_acc: 1.0
epoch: 180: train_loss: 0.7870184396072861, train_acc: 0.4925595323244731, val_loss: 0.5668276846408844, val_acc: 1.0
epoch: 181: train_loss: 0.786657000199342, train_acc: 0.4776785671710968, val_loss: 0.580445796251297, val_acc: 1.0
epoch: 182: train_loss: 0.7856122809029664, train_acc: 0.5982142885526022, val_loss: 0.5612115561962128, val_acc: 1.0
epoch: 183: train_loss: 0.7850340198779449, train_acc: 0.5059523781140646, val_loss: 0.563104897737503, val_acc: 0.984375
epoch: 184: train_loss: 0.7845428202603312, train_acc: 0.5133928656578064, val_loss: 0.5631047785282135, val_acc: 1.0
epoch: 185: train_loss: 0.7837915956760391, train_acc: 0.5535714228947958, val_loss: 0.5857302248477936, val_acc: 0.9375
epoch: 186: train_loss: 0.7832002061786072, train_acc: 0.5193452338377634, val_loss: 0.6034075617790222, val_acc: 0.9375
epoch: 187: train_loss: 0.7823980292107195, train_acc: 0.5654761989911398, val_loss: 0.562550961971283, val_acc: 0.984375
epoch: 188: train_loss: 0.7820344952048447, train_acc: 0.4598214228947957, val_loss: 0.5560374557971954, val_acc: 1.0
epoch: 189: train_loss: 0.7814036894262881, train_acc: 0.5297619005044302, val_loss: 0.574363648891449, val_acc: 0.96875
epoch: 190: train_loss: 0.7810753182382899, train_acc: 0.5104166666666666, val_loss: 0.5565997958183289, val_acc: 1.0
epoch: 191: train_loss: 0.780218414030969, train_acc: 0.5982142885526022, val_loss: 0.5544922351837158, val_acc: 1.0
epoch: 192: train_loss: 0.7796662761757409, train_acc: 0.5163690447807312, val_loss: 0.5701201260089874, val_acc: 1.0
epoch: 193: train_loss: 0.7788360610450664, train_acc: 0.59375, val_loss: 0.5666549205780029, val_acc: 0.984375
epoch: 194: train_loss: 0.7782296900056366, train_acc: 0.5342261989911398, val_loss: 0.5572513341903687, val_acc: 1.0
epoch: 195: train_loss: 0.7780651723851961, train_acc: 0.4464285671710968, val_loss: 0.5533559620380402, val_acc: 1.0
epoch: 196: train_loss: 0.7776174690517675, train_acc: 0.5, val_loss: 0.6049869358539581, val_acc: 0.9375
epoch: 197: train_loss: 0.7773438085008549, train_acc: 0.4851190447807312, val_loss: 0.5681085884571075, val_acc: 0.984375
epoch: 198: train_loss: 0.7769192047853964, train_acc: 0.5297619005044302, val_loss: 0.6200428903102875, val_acc: 0.9375
epoch: 199: train_loss: 0.7766386994719504, train_acc: 0.4761904776096344, val_loss: 0.5655083954334259, val_acc: 1.0
epoch: 200: train_loss: 0.7764849279255019, train_acc: 0.4479166666666667, val_loss: 0.5540256500244141, val_acc: 1.0
epoch: 201: train_loss: 0.7759275903599488, train_acc: 0.5461309552192688, val_loss: 0.5596989989280701, val_acc: 1.0
epoch: 202: train_loss: 0.7752253171454117, train_acc: 0.555059532324473, val_loss: 0.5774985253810883, val_acc: 0.96875
epoch: 203: train_loss: 0.7747827355378591, train_acc: 0.5267857114473978, val_loss: 0.6083569526672363, val_acc: 0.9375
epoch: 204: train_loss: 0.7745884152931893, train_acc: 0.511904756228129, val_loss: 0.5540208220481873, val_acc: 1.0
epoch: 205: train_loss: 0.7741839315706085, train_acc: 0.5089285671710968, val_loss: 0.5527580380439758, val_acc: 1.0
epoch: 206: train_loss: 0.7737404258738776, train_acc: 0.5089285671710968, val_loss: 0.5633865594863892, val_acc: 1.0
epoch: 207: train_loss: 0.7734523341059684, train_acc: 0.4657738109429677, val_loss: 0.5628495812416077, val_acc: 0.984375
epoch: 208: train_loss: 0.7725346731987485, train_acc: 0.6220238010088602, val_loss: 0.5686517357826233, val_acc: 0.984375
epoch: 209: train_loss: 0.7723814719253115, train_acc: 0.4553571442763011, val_loss: 0.5590941905975342, val_acc: 1.0
epoch: 210: train_loss: 0.7721414797678942, train_acc: 0.4449404776096344, val_loss: 0.5645824670791626, val_acc: 1.0
epoch: 211: train_loss: 0.7719573866833679, train_acc: 0.4613095223903656, val_loss: 0.5645541846752167, val_acc: 0.984375
epoch: 212: train_loss: 0.7716537245748933, train_acc: 0.5089285671710968, val_loss: 0.566329687833786, val_acc: 0.984375
epoch: 213: train_loss: 0.772049906283524, train_acc: 0.3199404776096344, val_loss: 0.561377078294754, val_acc: 0.984375
epoch: 214: train_loss: 0.771851570754088, train_acc: 0.4389880994955699, val_loss: 0.5697031319141388, val_acc: 0.96875
epoch: 215: train_loss: 0.7716568941135463, train_acc: 0.4657738109429677, val_loss: 0.6004427671432495, val_acc: 0.953125
epoch: 216: train_loss: 0.7713028306052796, train_acc: 0.5342261989911398, val_loss: 0.5945565104484558, val_acc: 0.953125
epoch: 217: train_loss: 0.7707957098607986, train_acc: 0.5461309552192688, val_loss: 0.5677633285522461, val_acc: 0.984375
epoch: 218: train_loss: 0.7702368949646272, train_acc: 0.5625, val_loss: 0.5712611675262451, val_acc: 1.0
epoch: 219: train_loss: 0.7696800181359954, train_acc: 0.5699404776096344, val_loss: 0.5670779943466187, val_acc: 0.984375
epoch: 220: train_loss: 0.7691628585933378, train_acc: 0.5535714228947958, val_loss: 0.5537244379520416, val_acc: 1.0
epoch: 221: train_loss: 0.7684937276102757, train_acc: 0.586309532324473, val_loss: 0.5689485669136047, val_acc: 0.984375
epoch: 222: train_loss: 0.7680993361679782, train_acc: 0.517857144276301, val_loss: 0.5771372020244598, val_acc: 0.984375
epoch: 223: train_loss: 0.7676313076877876, train_acc: 0.586309532324473, val_loss: 0.5826728940010071, val_acc: 0.96875
epoch: 224: train_loss: 0.767254712934847, train_acc: 0.5133928656578064, val_loss: 0.5523955523967743, val_acc: 1.0
epoch: 225: train_loss: 0.7667737825606071, train_acc: 0.5446428656578064, val_loss: 0.58197420835495, val_acc: 0.96875
epoch: 226: train_loss: 0.7660217023455972, train_acc: 0.6086309552192688, val_loss: 0.5583397746086121, val_acc: 1.0
epoch: 227: train_loss: 0.7655926229138121, train_acc: 0.5372023781140646, val_loss: 0.5661522448062897, val_acc: 0.984375
epoch: 228: train_loss: 0.7653434968931704, train_acc: 0.4970238109429677, val_loss: 0.6060323715209961, val_acc: 0.921875
epoch: 229: train_loss: 0.7651527853979578, train_acc: 0.517857144276301, val_loss: 0.6021080017089844, val_acc: 0.9375
epoch: 230: train_loss: 0.764593388353075, train_acc: 0.5818452338377634, val_loss: 0.5876761972904205, val_acc: 0.96875
epoch: 231: train_loss: 0.7642722279689775, train_acc: 0.4985119005044301, val_loss: 0.5835379660129547, val_acc: 0.96875
epoch: 232: train_loss: 0.7637666150906224, train_acc: 0.538690467675527, val_loss: 0.5555072128772736, val_acc: 1.0
epoch: 233: train_loss: 0.7634195505384025, train_acc: 0.5074404776096344, val_loss: 0.5526942610740662, val_acc: 1.0
epoch: 234: train_loss: 0.7633527089518011, train_acc: 0.4196428557236989, val_loss: 0.5537622272968292, val_acc: 1.0
epoch: 235: train_loss: 0.7633781988741986, train_acc: 0.4434523781140645, val_loss: 0.5530275702476501, val_acc: 1.0
epoch: 236: train_loss: 0.7627126932898651, train_acc: 0.6130952338377634, val_loss: 0.5668500661849976, val_acc: 0.984375
epoch: 237: train_loss: 0.7626897006058223, train_acc: 0.4880952338377635, val_loss: 0.5609699189662933, val_acc: 1.0
epoch: 238: train_loss: 0.762595133344166, train_acc: 0.4791666666666667, val_loss: 0.5920529961585999, val_acc: 1.0
epoch: 239: train_loss: 0.7622894829759994, train_acc: 0.5044642885526022, val_loss: 0.5644963085651398, val_acc: 0.984375
epoch: 240: train_loss: 0.7621643589822418, train_acc: 0.46875, val_loss: 0.5546201169490814, val_acc: 1.0
epoch: 241: train_loss: 0.7620337319357664, train_acc: 0.4479166666666667, val_loss: 0.5586675405502319, val_acc: 1.0
epoch: 242: train_loss: 0.7614699800400413, train_acc: 0.5907738010088602, val_loss: 0.5547280609607697, val_acc: 1.0
epoch: 243: train_loss: 0.7612690408943128, train_acc: 0.5029761989911398, val_loss: 0.6106158494949341, val_acc: 0.9375
epoch: 244: train_loss: 0.760872682138365, train_acc: 0.5491071343421936, val_loss: 0.5564039349555969, val_acc: 1.0
epoch: 245: train_loss: 0.7610128015925889, train_acc: 0.3869047661622365, val_loss: 0.5744224190711975, val_acc: 0.96875
epoch: 246: train_loss: 0.7605025324663812, train_acc: 0.5416666666666666, val_loss: 0.5784538686275482, val_acc: 0.984375
epoch: 247: train_loss: 0.760130934297077, train_acc: 0.53125, val_loss: 0.5630770623683929, val_acc: 0.984375
epoch: 248: train_loss: 0.7595113730095475, train_acc: 0.574404756228129, val_loss: 0.5589947998523712, val_acc: 1.0
epoch: 249: train_loss: 0.7592393828630446, train_acc: 0.5327380895614624, val_loss: 0.5616313517093658, val_acc: 1.0
epoch: 250: train_loss: 0.7589545389649719, train_acc: 0.5342261989911398, val_loss: 0.5621587634086609, val_acc: 1.0
epoch: 251: train_loss: 0.758850722204125, train_acc: 0.4672619005044301, val_loss: 0.5579617321491241, val_acc: 1.0
epoch: 252: train_loss: 0.7583746255543548, train_acc: 0.569940467675527, val_loss: 0.5811779499053955, val_acc: 0.96875
epoch: 253: train_loss: 0.7578998933548688, train_acc: 0.5714285771052042, val_loss: 0.5715388357639313, val_acc: 0.984375
epoch: 254: train_loss: 0.7575709891864675, train_acc: 0.5297619005044302, val_loss: 0.6014605164527893, val_acc: 0.9375
epoch: 255: train_loss: 0.7572628487444791, train_acc: 0.517857144276301, val_loss: 0.5535094141960144, val_acc: 1.0
epoch: 256: train_loss: 0.7570131737666369, train_acc: 0.5208333333333334, val_loss: 0.555779904127121, val_acc: 1.0
epoch: 257: train_loss: 0.7566388637905587, train_acc: 0.543154756228129, val_loss: 0.5540588200092316, val_acc: 1.0
epoch: 258: train_loss: 0.7563098845104451, train_acc: 0.5342261989911398, val_loss: 0.5522839426994324, val_acc: 1.0
epoch: 259: train_loss: 0.7560421678118214, train_acc: 0.5193452338377634, val_loss: 0.5688255429267883, val_acc: 0.984375
epoch: 260: train_loss: 0.7555102300217627, train_acc: 0.581845243771871, val_loss: 0.5573922991752625, val_acc: 1.0
epoch: 261: train_loss: 0.7551009714148423, train_acc: 0.5580357114473978, val_loss: 0.6095565855503082, val_acc: 0.9375
epoch: 262: train_loss: 0.7546367241855807, train_acc: 0.586309532324473, val_loss: 0.6140521466732025, val_acc: 0.9375
epoch: 263: train_loss: 0.7540898955229554, train_acc: 0.6071428656578064, val_loss: 0.5660494863986969, val_acc: 0.984375
epoch: 264: train_loss: 0.7536243692134158, train_acc: 0.5773809552192688, val_loss: 0.5641413331031799, val_acc: 0.984375
epoch: 265: train_loss: 0.7530768644391441, train_acc: 0.605654756228129, val_loss: 0.5536490678787231, val_acc: 1.0
epoch: 266: train_loss: 0.7523625402266014, train_acc: 0.6428571343421936, val_loss: 0.5609737634658813, val_acc: 1.0
epoch: 267: train_loss: 0.7522260825432351, train_acc: 0.5, val_loss: 0.6114145219326019, val_acc: 0.9375
epoch: 268: train_loss: 0.7518933893284091, train_acc: 0.5327380895614624, val_loss: 0.5564078092575073, val_acc: 1.0
epoch: 269: train_loss: 0.7516906594052724, train_acc: 0.523809532324473, val_loss: 0.6381267309188843, val_acc: 0.9375
epoch: 270: train_loss: 0.7510700726567743, train_acc: 0.6354166666666666, val_loss: 0.5980088114738464, val_acc: 0.9375
epoch: 271: train_loss: 0.7508012112595284, train_acc: 0.5282738109429678, val_loss: 0.5663004517555237, val_acc: 1.0
epoch: 272: train_loss: 0.7506518641962087, train_acc: 0.5, val_loss: 0.58453568816185, val_acc: 0.96875
epoch: 273: train_loss: 0.7503270883194717, train_acc: 0.5401785671710968, val_loss: 0.5519622266292572, val_acc: 1.0
epoch: 274: train_loss: 0.7499444145867314, train_acc: 0.5520833333333334, val_loss: 0.5518829524517059, val_acc: 1.0
epoch: 275: train_loss: 0.7494131260448027, train_acc: 0.5997023781140646, val_loss: 0.5553715527057648, val_acc: 1.0
epoch: 276: train_loss: 0.7493007977086279, train_acc: 0.4791666666666667, val_loss: 0.5518632233142853, val_acc: 1.0
epoch: 277: train_loss: 0.7489137482300074, train_acc: 0.5639880895614624, val_loss: 0.5618934333324432, val_acc: 0.984375
epoch: 278: train_loss: 0.7485610987977738, train_acc: 0.5520833333333334, val_loss: 0.622301459312439, val_acc: 0.921875
epoch: 279: train_loss: 0.748312218700136, train_acc: 0.5178571343421936, val_loss: 0.5674321949481964, val_acc: 0.984375
epoch: 280: train_loss: 0.7478655255703451, train_acc: 0.5907738010088602, val_loss: 0.5693557262420654, val_acc: 0.96875
epoch: 281: train_loss: 0.7478223458531331, train_acc: 0.4598214328289032, val_loss: 0.5675937235355377, val_acc: 0.984375
epoch: 282: train_loss: 0.7475986253246396, train_acc: 0.5565476218859354, val_loss: 0.5674687623977661, val_acc: 1.0
epoch: 283: train_loss: 0.7474472803968772, train_acc: 0.4791666666666667, val_loss: 0.5556874573230743, val_acc: 1.0
epoch: 284: train_loss: 0.7473038402914297, train_acc: 0.4940476218859355, val_loss: 0.6069974303245544, val_acc: 0.9375
epoch: 285: train_loss: 0.747170838929, train_acc: 0.4761904776096344, val_loss: 0.5656309723854065, val_acc: 1.0
epoch: 286: train_loss: 0.7470621288722671, train_acc: 0.4776785671710968, val_loss: 0.5518929064273834, val_acc: 1.0
epoch: 287: train_loss: 0.7465731590572324, train_acc: 0.5967261989911398, val_loss: 0.5559511780738831, val_acc: 1.0
epoch: 288: train_loss: 0.7462398948828923, train_acc: 0.543154756228129, val_loss: 0.5530411005020142, val_acc: 1.0
epoch: 289: train_loss: 0.7458681610808969, train_acc: 0.5491071343421936, val_loss: 0.5551916062831879, val_acc: 1.0
epoch: 290: train_loss: 0.7454354243453283, train_acc: 0.5684523781140646, val_loss: 0.6114465594291687, val_acc: 0.9375
epoch: 291: train_loss: 0.7451483107865122, train_acc: 0.5208333333333334, val_loss: 0.5602339208126068, val_acc: 1.0
epoch: 292: train_loss: 0.7450385528348541, train_acc: 0.4821428656578064, val_loss: 0.561063677072525, val_acc: 0.984375
epoch: 293: train_loss: 0.7443847114942506, train_acc: 0.6726190447807312, val_loss: 0.5525822043418884, val_acc: 1.0
epoch: 294: train_loss: 0.7438518518781926, train_acc: 0.6190476218859354, val_loss: 0.5521106123924255, val_acc: 1.0
epoch: 295: train_loss: 0.7438213022442546, train_acc: 0.4464285671710968, val_loss: 0.5720930397510529, val_acc: 0.984375
epoch: 296: train_loss: 0.7436133700722926, train_acc: 0.5208333333333334, val_loss: 0.5537095367908478, val_acc: 1.0
epoch: 297: train_loss: 0.7434745046916419, train_acc: 0.4910714228947957, val_loss: 0.554474264383316, val_acc: 1.0
epoch: 298: train_loss: 0.7432150565660912, train_acc: 0.5163690447807312, val_loss: 0.6181089282035828, val_acc: 0.9375
epoch: 299: train_loss: 0.7429996061987343, train_acc: 0.511904756228129, val_loss: 0.5817810297012329, val_acc: 0.984375
epoch: 300: train_loss: 0.7426054906079403, train_acc: 0.6086309552192688, val_loss: 0.5545459687709808, val_acc: 1.0
epoch: 301: train_loss: 0.7424129777265169, train_acc: 0.4985119005044301, val_loss: 0.5517109334468842, val_acc: 1.0
epoch: 302: train_loss: 0.7425401490120206, train_acc: 0.4092261890570323, val_loss: 0.5596062242984772, val_acc: 1.0
epoch: 303: train_loss: 0.7424461942231442, train_acc: 0.4955357114473979, val_loss: 0.5570625960826874, val_acc: 1.0
epoch: 304: train_loss: 0.7420523271534608, train_acc: 0.5773809552192688, val_loss: 0.7479027807712555, val_acc: 0.765625
epoch: 305: train_loss: 0.7418896459156648, train_acc: 0.5133928656578064, val_loss: 0.5924980342388153, val_acc: 0.96875
epoch: 306: train_loss: 0.741492401828724, train_acc: 0.581845243771871, val_loss: 0.5693352520465851, val_acc: 0.984375
epoch: 307: train_loss: 0.7413408411787699, train_acc: 0.4702380895614624, val_loss: 0.5713377594947815, val_acc: 0.984375
epoch: 308: train_loss: 0.7411018837617049, train_acc: 0.5491071343421936, val_loss: 0.6054822504520416, val_acc: 0.921875
epoch: 309: train_loss: 0.7410038653881316, train_acc: 0.4851190447807312, val_loss: 0.5536302328109741, val_acc: 1.0
epoch: 310: train_loss: 0.7409689915780722, train_acc: 0.4598214228947957, val_loss: 0.5580870509147644, val_acc: 1.0
epoch: 311: train_loss: 0.7406925543760638, train_acc: 0.53125, val_loss: 0.5951388776302338, val_acc: 0.921875
epoch: 312: train_loss: 0.7404971795594981, train_acc: 0.523809532324473, val_loss: 0.6075223386287689, val_acc: 0.953125
epoch: 313: train_loss: 0.7402830294742702, train_acc: 0.5401785771052042, val_loss: 0.5579606294631958, val_acc: 1.0
epoch: 314: train_loss: 0.7400110833228577, train_acc: 0.555059532324473, val_loss: 0.5709814131259918, val_acc: 0.984375
epoch: 315: train_loss: 0.7397475070339714, train_acc: 0.5267857114473978, val_loss: 0.5642709732055664, val_acc: 0.984375
epoch: 316: train_loss: 0.7394934420079209, train_acc: 0.5223214228947958, val_loss: 0.6233503818511963, val_acc: 0.921875
epoch: 317: train_loss: 0.7391277900281938, train_acc: 0.5952380895614624, val_loss: 0.5634580254554749, val_acc: 0.984375
epoch: 318: train_loss: 0.7387250138051579, train_acc: 0.5907738010088602, val_loss: 0.552972823381424, val_acc: 1.0
epoch: 319: train_loss: 0.7384977116559939, train_acc: 0.5505952338377634, val_loss: 0.5695208013057709, val_acc: 1.0
epoch: 320: train_loss: 0.7385736582558966, train_acc: 0.4345238109429677, val_loss: 0.5517795979976654, val_acc: 1.0
epoch: 321: train_loss: 0.7383900158276959, train_acc: 0.5297619005044302, val_loss: 0.5583035051822662, val_acc: 1.0
epoch: 322: train_loss: 0.7380366644007991, train_acc: 0.5535714228947958, val_loss: 0.5518283545970917, val_acc: 1.0
epoch: 323: train_loss: 0.7377128904616388, train_acc: 0.5461309552192688, val_loss: 0.5520680844783783, val_acc: 1.0
epoch: 324: train_loss: 0.737705769600012, train_acc: 0.4479166666666667, val_loss: 0.5659468173980713, val_acc: 1.0
epoch: 325: train_loss: 0.737471169672129, train_acc: 0.5416666666666666, val_loss: 0.5558270812034607, val_acc: 1.0
epoch: 326: train_loss: 0.7371377004395928, train_acc: 0.569940467675527, val_loss: 0.5546795725822449, val_acc: 1.0
epoch: 327: train_loss: 0.7369887395118307, train_acc: 0.5014880895614624, val_loss: 0.5713620781898499, val_acc: 0.984375
epoch: 328: train_loss: 0.7368867926322215, train_acc: 0.4836309552192688, val_loss: 0.5863902270793915, val_acc: 0.96875
epoch: 329: train_loss: 0.7367519297383045, train_acc: 0.5, val_loss: 0.5517091453075409, val_acc: 1.0
epoch: 330: train_loss: 0.7366183599436508, train_acc: 0.523809532324473, val_loss: 0.5576433837413788, val_acc: 1.0
epoch: 331: train_loss: 0.7364800180416985, train_acc: 0.4985119005044301, val_loss: 0.5674121081829071, val_acc: 0.984375
epoch: 332: train_loss: 0.7364626819784335, train_acc: 0.4672619005044301, val_loss: 0.5523298382759094, val_acc: 1.0
epoch: 333: train_loss: 0.7362738947668472, train_acc: 0.555059532324473, val_loss: 0.5617176592350006, val_acc: 0.984375
epoch: 334: train_loss: 0.7359336832269504, train_acc: 0.6026785771052042, val_loss: 0.5548350512981415, val_acc: 1.0
epoch: 335: train_loss: 0.7359435783019139, train_acc: 0.4791666666666667, val_loss: 0.5603381991386414, val_acc: 1.0
epoch: 336: train_loss: 0.7356202433771948, train_acc: 0.5833333333333334, val_loss: 0.5558713972568512, val_acc: 1.0
epoch: 337: train_loss: 0.7352663961035257, train_acc: 0.6190476218859354, val_loss: 0.5568243265151978, val_acc: 1.0
epoch: 338: train_loss: 0.7350573358048142, train_acc: 0.5729166666666666, val_loss: 0.5592415034770966, val_acc: 1.0
epoch: 339: train_loss: 0.7350879984159093, train_acc: 0.4434523781140645, val_loss: 0.5558121204376221, val_acc: 1.0
epoch: 340: train_loss: 0.7351711348704226, train_acc: 0.46875, val_loss: 0.5556739866733551, val_acc: 1.0
epoch: 341: train_loss: 0.7350235638911261, train_acc: 0.5267857114473978, val_loss: 0.5695810616016388, val_acc: 1.0
epoch: 342: train_loss: 0.7350443791022451, train_acc: 0.4270833333333333, val_loss: 0.5650378167629242, val_acc: 0.984375
epoch: 343: train_loss: 0.7349457170150074, train_acc: 0.511904756228129, val_loss: 0.5737349092960358, val_acc: 0.984375
epoch: 344: train_loss: 0.7347314216088555, train_acc: 0.5223214228947958, val_loss: 0.6282934546470642, val_acc: 0.890625
epoch: 345: train_loss: 0.7345614413190666, train_acc: 0.4955357114473979, val_loss: 0.6286059617996216, val_acc: 0.90625
epoch: 346: train_loss: 0.7344563156223204, train_acc: 0.4970238109429677, val_loss: 0.5769068896770477, val_acc: 0.96875
epoch: 347: train_loss: 0.7342134208857326, train_acc: 0.5357142885526022, val_loss: 0.5642034411430359, val_acc: 0.984375
epoch: 348: train_loss: 0.7340539203104567, train_acc: 0.5059523781140646, val_loss: 0.5550631582736969, val_acc: 1.0
epoch: 349: train_loss: 0.7339160696097781, train_acc: 0.5029761989911398, val_loss: 0.5529523193836212, val_acc: 1.0
epoch: 350: train_loss: 0.7335845648619749, train_acc: 0.5788690447807312, val_loss: 0.5840549468994141, val_acc: 0.96875
epoch: 351: train_loss: 0.7335872979213792, train_acc: 0.46875, val_loss: 0.5521264970302582, val_acc: 1.0
epoch: 352: train_loss: 0.7335674372678437, train_acc: 0.5, val_loss: 0.5822198688983917, val_acc: 0.96875
epoch: 353: train_loss: 0.7336377284284363, train_acc: 0.4330357114473979, val_loss: 0.5559328198432922, val_acc: 1.0
epoch: 354: train_loss: 0.7335440631203806, train_acc: 0.4880952338377635, val_loss: 0.5543841421604156, val_acc: 1.0
epoch: 355: train_loss: 0.7334354911069296, train_acc: 0.4880952338377635, val_loss: 0.5520566701889038, val_acc: 1.0
epoch: 356: train_loss: 0.7332838263943524, train_acc: 0.5133928656578064, val_loss: 0.5698263347148895, val_acc: 0.984375
epoch: 357: train_loss: 0.7331055408305516, train_acc: 0.5208333333333334, val_loss: 0.56565260887146, val_acc: 0.984375
epoch: 358: train_loss: 0.7328074506398361, train_acc: 0.5833333333333334, val_loss: 0.5629792511463165, val_acc: 0.984375
epoch: 359: train_loss: 0.7327233802113267, train_acc: 0.5148809552192688, val_loss: 0.5567629337310791, val_acc: 1.0
epoch: 360: train_loss: 0.7325864954613361, train_acc: 0.5044642885526022, val_loss: 0.5545355677604675, val_acc: 1.0
epoch: 361: train_loss: 0.7325264339234069, train_acc: 0.4880952338377635, val_loss: 0.5529800057411194, val_acc: 1.0
epoch: 362: train_loss: 0.7323711360945845, train_acc: 0.5029761989911398, val_loss: 0.5542557239532471, val_acc: 1.0
epoch: 363: train_loss: 0.7320742260241682, train_acc: 0.5803571343421936, val_loss: 0.5692147314548492, val_acc: 0.984375
epoch: 364: train_loss: 0.7319168678973906, train_acc: 0.5014880895614624, val_loss: 0.5519826114177704, val_acc: 1.0
epoch: 365: train_loss: 0.7317308049872924, train_acc: 0.5401785671710968, val_loss: 0.6026773750782013, val_acc: 0.96875
epoch: 366: train_loss: 0.7316940933276477, train_acc: 0.4910714228947957, val_loss: 0.5694769322872162, val_acc: 0.96875
epoch: 367: train_loss: 0.7315391875749478, train_acc: 0.5208333333333334, val_loss: 0.5569711923599243, val_acc: 1.0
epoch: 368: train_loss: 0.7313580679645811, train_acc: 0.5386904776096344, val_loss: 0.5681815147399902, val_acc: 0.984375
epoch: 369: train_loss: 0.7310962785203176, train_acc: 0.5535714228947958, val_loss: 0.5534304082393646, val_acc: 1.0
epoch: 370: train_loss: 0.7309840589574296, train_acc: 0.4985119005044301, val_loss: 0.5636751055717468, val_acc: 1.0
epoch: 371: train_loss: 0.7308783424103557, train_acc: 0.5342261989911398, val_loss: 0.5519614517688751, val_acc: 1.0
epoch: 372: train_loss: 0.7305763215492406, train_acc: 0.5729166666666666, val_loss: 0.5520381331443787, val_acc: 1.0
epoch: 373: train_loss: 0.7305083131673287, train_acc: 0.5223214228947958, val_loss: 0.5892232656478882, val_acc: 0.984375
epoch: 374: train_loss: 0.7304779982831741, train_acc: 0.4955357114473979, val_loss: 0.5534020960330963, val_acc: 1.0
epoch: 375: train_loss: 0.730370105396137, train_acc: 0.511904756228129, val_loss: 0.5515687465667725, val_acc: 1.0
epoch: 376: train_loss: 0.7301105152548781, train_acc: 0.5758928656578064, val_loss: 0.5521002113819122, val_acc: 1.0
epoch: 377: train_loss: 0.7299187931066257, train_acc: 0.5401785671710968, val_loss: 0.5516451001167297, val_acc: 1.0
epoch: 378: train_loss: 0.7297267928758836, train_acc: 0.5580357114473978, val_loss: 0.5532259941101074, val_acc: 1.0
epoch: 379: train_loss: 0.7293842858128379, train_acc: 0.586309532324473, val_loss: 0.5572201609611511, val_acc: 1.0
epoch: 380: train_loss: 0.729321062069448, train_acc: 0.5, val_loss: 0.552908718585968, val_acc: 1.0
epoch: 381: train_loss: 0.7290568729085654, train_acc: 0.5595238109429678, val_loss: 0.5529148280620575, val_acc: 1.0
epoch: 382: train_loss: 0.7289146210080751, train_acc: 0.5252976218859354, val_loss: 0.5524828732013702, val_acc: 1.0
epoch: 383: train_loss: 0.7286925524628408, train_acc: 0.574404756228129, val_loss: 0.5534120500087738, val_acc: 1.0
epoch: 384: train_loss: 0.728464813640107, train_acc: 0.5610119005044302, val_loss: 0.5520845055580139, val_acc: 1.0
epoch: 385: train_loss: 0.7284900508241947, train_acc: 0.4791666666666667, val_loss: 0.5527053475379944, val_acc: 1.0
epoch: 386: train_loss: 0.7284228998612574, train_acc: 0.4940476218859355, val_loss: 0.5523200035095215, val_acc: 1.0
epoch: 387: train_loss: 0.7283841442969654, train_acc: 0.4776785671710968, val_loss: 0.5533744692802429, val_acc: 1.0
epoch: 388: train_loss: 0.7282350793442564, train_acc: 0.4985119005044301, val_loss: 0.5517561137676239, val_acc: 1.0
epoch: 389: train_loss: 0.7280856582089364, train_acc: 0.5208333333333334, val_loss: 0.5516076982021332, val_acc: 1.0
epoch: 390: train_loss: 0.7279929047401196, train_acc: 0.4970238109429677, val_loss: 0.5709280967712402, val_acc: 0.984375
epoch: 391: train_loss: 0.7277676154206802, train_acc: 0.5684523781140646, val_loss: 0.5818296670913696, val_acc: 0.96875
epoch: 392: train_loss: 0.7275440723993091, train_acc: 0.601190467675527, val_loss: 0.5773054957389832, val_acc: 0.96875
epoch: 393: train_loss: 0.7275338335035213, train_acc: 0.4464285671710968, val_loss: 0.6331550180912018, val_acc: 0.90625
epoch: 394: train_loss: 0.727328595431042, train_acc: 0.5416666666666666, val_loss: 0.6054649353027344, val_acc: 0.9375
epoch: 395: train_loss: 0.7271869831815713, train_acc: 0.5461309552192688, val_loss: 0.5542812347412109, val_acc: 1.0
epoch: 396: train_loss: 0.7270058870415642, train_acc: 0.5833333333333334, val_loss: 0.5630482733249664, val_acc: 1.0
epoch: 397: train_loss: 0.7269780282898364, train_acc: 0.5104166666666666, val_loss: 0.5517848134040833, val_acc: 1.0
epoch: 398: train_loss: 0.7270448734686586, train_acc: 0.4255952338377635, val_loss: 0.5532791614532471, val_acc: 1.0
epoch: 399: train_loss: 0.7269610710442065, train_acc: 0.517857144276301, val_loss: 0.5525259077548981, val_acc: 1.0
epoch: 400: train_loss: 0.7269507010381416, train_acc: 0.5089285671710968, val_loss: 0.5548804402351379, val_acc: 1.0
epoch: 401: train_loss: 0.7268260085760656, train_acc: 0.5297619005044302, val_loss: 0.5713481605052948, val_acc: 0.984375
epoch: 402: train_loss: 0.7266404560225574, train_acc: 0.53125, val_loss: 0.562957227230072, val_acc: 0.984375
epoch: 403: train_loss: 0.7263232616603177, train_acc: 0.617559532324473, val_loss: 0.6251629590988159, val_acc: 0.921875
epoch: 404: train_loss: 0.7262807842635323, train_acc: 0.5639880895614624, val_loss: 0.5579699873924255, val_acc: 1.0
epoch: 405: train_loss: 0.7263573730128935, train_acc: 0.4092261890570323, val_loss: 0.5522284507751465, val_acc: 1.0
epoch: 406: train_loss: 0.7263781583084618, train_acc: 0.4717261989911397, val_loss: 0.5605293810367584, val_acc: 0.984375
epoch: 407: train_loss: 0.7262660580522873, train_acc: 0.5104166666666666, val_loss: 0.5518671870231628, val_acc: 1.0
epoch: 408: train_loss: 0.7262035071509779, train_acc: 0.5372023781140646, val_loss: 0.5514691472053528, val_acc: 1.0
epoch: 409: train_loss: 0.7260269069574712, train_acc: 0.5401785671710968, val_loss: 0.5523468554019928, val_acc: 1.0
epoch: 410: train_loss: 0.7258381622647813, train_acc: 0.5565476218859354, val_loss: 0.5732140839099884, val_acc: 0.96875
epoch: 411: train_loss: 0.7259338094482143, train_acc: 0.4375, val_loss: 0.5839704275131226, val_acc: 0.96875
epoch: 412: train_loss: 0.7257007193045811, train_acc: 0.5505952338377634, val_loss: 0.6272867619991302, val_acc: 0.921875
epoch: 413: train_loss: 0.725447491603197, train_acc: 0.5729166666666666, val_loss: 0.554050475358963, val_acc: 1.0
epoch: 414: train_loss: 0.7253012483857241, train_acc: 0.5193452338377634, val_loss: 0.5645769238471985, val_acc: 0.984375
epoch: 415: train_loss: 0.7252363777026913, train_acc: 0.5029761890570322, val_loss: 0.5820423662662506, val_acc: 0.96875
epoch: 416: train_loss: 0.7250699083582101, train_acc: 0.5476190447807312, val_loss: 0.5772934556007385, val_acc: 0.984375
epoch: 417: train_loss: 0.7249254840507839, train_acc: 0.5372023781140646, val_loss: 0.5539563596248627, val_acc: 1.0
epoch: 418: train_loss: 0.724732258060003, train_acc: 0.5401785671710968, val_loss: 0.576192319393158, val_acc: 0.96875
epoch: 419: train_loss: 0.7246401779235353, train_acc: 0.5178571343421936, val_loss: 0.5523580610752106, val_acc: 1.0
epoch: 420: train_loss: 0.7245083874591454, train_acc: 0.5535714228947958, val_loss: 0.5518316626548767, val_acc: 1.0
epoch: 421: train_loss: 0.7243836289992639, train_acc: 0.5193452338377634, val_loss: 0.5516727864742279, val_acc: 1.0
epoch: 422: train_loss: 0.7240389437070504, train_acc: 0.6622023781140646, val_loss: 0.5530882477760315, val_acc: 1.0
epoch: 423: train_loss: 0.7238773078944696, train_acc: 0.523809532324473, val_loss: 0.5541521608829498, val_acc: 1.0
epoch: 424: train_loss: 0.7239141280510843, train_acc: 0.4508928656578064, val_loss: 0.5533704459667206, val_acc: 1.0
epoch: 425: train_loss: 0.7237713323885657, train_acc: 0.5491071343421936, val_loss: 0.5775355100631714, val_acc: 0.984375
epoch: 426: train_loss: 0.7235361703851089, train_acc: 0.5773809552192688, val_loss: 0.7040810585021973, val_acc: 0.796875
epoch: 427: train_loss: 0.7233878712973489, train_acc: 0.523809532324473, val_loss: 0.5593912601470947, val_acc: 1.0
epoch: 428: train_loss: 0.7233012093530667, train_acc: 0.5252976218859354, val_loss: 0.5520778894424438, val_acc: 1.0
epoch: 429: train_loss: 0.7231600535008333, train_acc: 0.549107144276301, val_loss: 0.5587877631187439, val_acc: 1.0
epoch: 430: train_loss: 0.7229945326810278, train_acc: 0.5520833333333334, val_loss: 0.5521769523620605, val_acc: 1.0
epoch: 431: train_loss: 0.7229832396555094, train_acc: 0.4910714228947957, val_loss: 0.5550825595855713, val_acc: 1.0
epoch: 432: train_loss: 0.7228703372656519, train_acc: 0.5104166666666666, val_loss: 0.5669838786125183, val_acc: 0.984375
epoch: 433: train_loss: 0.7227703205176762, train_acc: 0.53125, val_loss: 0.5660108625888824, val_acc: 0.984375
epoch: 434: train_loss: 0.722772116313949, train_acc: 0.4747023781140645, val_loss: 0.6077560484409332, val_acc: 0.921875
epoch: 435: train_loss: 0.7226038859524858, train_acc: 0.5788690447807312, val_loss: 0.5602087378501892, val_acc: 0.984375
epoch: 436: train_loss: 0.7222690344493322, train_acc: 0.6532738010088602, val_loss: 0.5642845034599304, val_acc: 0.984375
epoch: 437: train_loss: 0.7221506624914922, train_acc: 0.543154756228129, val_loss: 0.5515629649162292, val_acc: 1.0
epoch: 438: train_loss: 0.7220010735722502, train_acc: 0.5357142885526022, val_loss: 0.551568329334259, val_acc: 1.0
epoch: 439: train_loss: 0.7221172856110516, train_acc: 0.3958333333333333, val_loss: 0.554510086774826, val_acc: 1.0
epoch: 440: train_loss: 0.7220407084632984, train_acc: 0.5104166666666666, val_loss: 0.5646899044513702, val_acc: 1.0
epoch: 441: train_loss: 0.7219096664030269, train_acc: 0.5372023781140646, val_loss: 0.5515764355659485, val_acc: 1.0
epoch: 442: train_loss: 0.7216898637036797, train_acc: 0.5982142885526022, val_loss: 0.577400267124176, val_acc: 0.96875
epoch: 443: train_loss: 0.7216825065043595, train_acc: 0.4642857114473979, val_loss: 0.5827285945415497, val_acc: 0.96875
epoch: 444: train_loss: 0.721535725361399, train_acc: 0.5565476218859354, val_loss: 0.5726821422576904, val_acc: 0.984375
epoch: 445: train_loss: 0.7213557223568048, train_acc: 0.5714285771052042, val_loss: 0.552156001329422, val_acc: 1.0
epoch: 446: train_loss: 0.7211792602812861, train_acc: 0.5342261989911398, val_loss: 0.5532870292663574, val_acc: 1.0
epoch: 447: train_loss: 0.7211879835951895, train_acc: 0.4702380895614624, val_loss: 0.5522041022777557, val_acc: 1.0
epoch: 448: train_loss: 0.7210732116996577, train_acc: 0.5416666666666666, val_loss: 0.5523433089256287, val_acc: 1.0
epoch: 449: train_loss: 0.7209776659365053, train_acc: 0.5520833333333334, val_loss: 0.55422443151474, val_acc: 1.0
epoch: 450: train_loss: 0.721009774949933, train_acc: 0.4761904776096344, val_loss: 0.5517250001430511, val_acc: 1.0
epoch: 451: train_loss: 0.7209009066619704, train_acc: 0.5520833333333334, val_loss: 0.5749501287937164, val_acc: 0.96875
epoch: 452: train_loss: 0.7207799437205699, train_acc: 0.523809532324473, val_loss: 0.5772985816001892, val_acc: 0.96875
epoch: 453: train_loss: 0.7206747343274115, train_acc: 0.5104166666666666, val_loss: 0.5626419186592102, val_acc: 1.0
epoch: 454: train_loss: 0.7205786837762965, train_acc: 0.5193452338377634, val_loss: 0.5515279769897461, val_acc: 1.0
epoch: 455: train_loss: 0.7205359584145379, train_acc: 0.4866071442763011, val_loss: 0.5520265400409698, val_acc: 1.0
epoch: 456: train_loss: 0.7204191329529649, train_acc: 0.5401785671710968, val_loss: 0.5530159771442413, val_acc: 1.0
epoch: 457: train_loss: 0.7202624303371452, train_acc: 0.5580357114473978, val_loss: 0.5753935277462006, val_acc: 1.0
epoch: 458: train_loss: 0.7202196141993038, train_acc: 0.4880952338377635, val_loss: 0.5888839662075043, val_acc: 0.96875
epoch: 459: train_loss: 0.7199997082568598, train_acc: 0.5907738109429678, val_loss: 0.5678550601005554, val_acc: 0.984375
epoch: 460: train_loss: 0.7198922750058592, train_acc: 0.5104166666666666, val_loss: 0.5524694919586182, val_acc: 1.0
epoch: 461: train_loss: 0.7199637777619547, train_acc: 0.4360119005044301, val_loss: 0.5519854426383972, val_acc: 1.0
epoch: 462: train_loss: 0.7197365724328099, train_acc: 0.5892857114473978, val_loss: 0.5918370187282562, val_acc: 0.9375
epoch: 463: train_loss: 0.7195148035113154, train_acc: 0.5669642885526022, val_loss: 0.5522664189338684, val_acc: 1.0
epoch: 464: train_loss: 0.7192943053647181, train_acc: 0.5714285771052042, val_loss: 0.5555627346038818, val_acc: 1.0
epoch: 465: train_loss: 0.7190590715075768, train_acc: 0.605654756228129, val_loss: 0.5529927909374237, val_acc: 1.0
epoch: 466: train_loss: 0.7190944192709028, train_acc: 0.4776785671710968, val_loss: 0.5520954430103302, val_acc: 1.0
epoch: 467: train_loss: 0.7190671464444226, train_acc: 0.4791666666666667, val_loss: 0.5665116310119629, val_acc: 0.984375
epoch: 468: train_loss: 0.7189416013335029, train_acc: 0.53125, val_loss: 0.5601839125156403, val_acc: 0.984375
epoch: 469: train_loss: 0.7189334600741135, train_acc: 0.4776785671710968, val_loss: 0.5630274713039398, val_acc: 1.0
epoch: 470: train_loss: 0.7187765812038615, train_acc: 0.5520833333333334, val_loss: 0.5688086450099945, val_acc: 1.0
epoch: 471: train_loss: 0.7185386345000926, train_acc: 0.6264880895614624, val_loss: 0.5789060592651367, val_acc: 0.9375
epoch: 472: train_loss: 0.7183586242461052, train_acc: 0.574404756228129, val_loss: 0.5602206289768219, val_acc: 1.0
epoch: 473: train_loss: 0.7181788609794086, train_acc: 0.5684523781140646, val_loss: 0.5515851974487305, val_acc: 1.0
epoch: 474: train_loss: 0.7180509383427468, train_acc: 0.5252976218859354, val_loss: 0.5518330335617065, val_acc: 1.0
epoch: 475: train_loss: 0.7178136896895093, train_acc: 0.586309532324473, val_loss: 0.5514855682849884, val_acc: 1.0
epoch: 476: train_loss: 0.7176184763156975, train_acc: 0.5833333333333334, val_loss: 0.5554366409778595, val_acc: 1.0
epoch: 477: train_loss: 0.7173785216264644, train_acc: 0.5773809552192688, val_loss: 0.5573889315128326, val_acc: 1.0
epoch: 478: train_loss: 0.7173547937003952, train_acc: 0.4851190447807312, val_loss: 0.554935485124588, val_acc: 1.0
epoch: 479: train_loss: 0.7173741649215419, train_acc: 0.4672619005044301, val_loss: 0.5626620352268219, val_acc: 1.0
epoch: 480: train_loss: 0.7172709329419059, train_acc: 0.5372023781140646, val_loss: 0.5654132664203644, val_acc: 0.984375
epoch: 481: train_loss: 0.71711532276943, train_acc: 0.5520833333333334, val_loss: 0.5902053117752075, val_acc: 0.9375
epoch: 482: train_loss: 0.7170221043957603, train_acc: 0.5625, val_loss: 0.5516052842140198, val_acc: 1.0
epoch: 483: train_loss: 0.7167275845799235, train_acc: 0.648809532324473, val_loss: 0.5529024302959442, val_acc: 1.0
epoch: 484: train_loss: 0.7164872627078052, train_acc: 0.5952380895614624, val_loss: 0.5681090354919434, val_acc: 1.0
epoch: 485: train_loss: 0.7165506715555412, train_acc: 0.4553571442763011, val_loss: 0.553993821144104, val_acc: 1.0
epoch: 486: train_loss: 0.7163598087452437, train_acc: 0.555059532324473, val_loss: 0.5674827098846436, val_acc: 0.984375
epoch: 487: train_loss: 0.7161762975359873, train_acc: 0.6101190447807312, val_loss: 0.5525819957256317, val_acc: 1.0
epoch: 488: train_loss: 0.7160390753199959, train_acc: 0.5907738010088602, val_loss: 0.587412029504776, val_acc: 0.96875
epoch: 489: train_loss: 0.7160476179755462, train_acc: 0.4925595323244731, val_loss: 0.5992671847343445, val_acc: 0.9375
epoch: 490: train_loss: 0.7159351817378361, train_acc: 0.543154756228129, val_loss: 0.5515648126602173, val_acc: 1.0
epoch: 491: train_loss: 0.71570233655412, train_acc: 0.6026785771052042, val_loss: 0.5694510638713837, val_acc: 0.984375
epoch: 492: train_loss: 0.7155229613698759, train_acc: 0.5848214228947958, val_loss: 0.5901026725769043, val_acc: 0.96875
epoch: 493: train_loss: 0.7155917814546909, train_acc: 0.4821428656578064, val_loss: 0.5843597650527954, val_acc: 0.96875
epoch: 494: train_loss: 0.7154654503270028, train_acc: 0.5535714228947958, val_loss: 0.5659375786781311, val_acc: 0.984375
epoch: 495: train_loss: 0.7155443702333714, train_acc: 0.4523809552192688, val_loss: 0.5524149835109711, val_acc: 1.0
epoch: 496: train_loss: 0.7153537035548069, train_acc: 0.5788690447807312, val_loss: 0.605059951543808, val_acc: 0.9375
epoch: 497: train_loss: 0.7153590245339443, train_acc: 0.5, val_loss: 0.5702499151229858, val_acc: 0.984375
epoch: 498: train_loss: 0.7153424968222576, train_acc: 0.5133928656578064, val_loss: 0.5515016615390778, val_acc: 1.0
epoch: 499: train_loss: 0.7151577186187107, train_acc: 0.5803571343421936, val_loss: 0.5656812489032745, val_acc: 0.984375
epoch: 500: train_loss: 0.7149963894052179, train_acc: 0.5758928656578064, val_loss: 0.5875396430492401, val_acc: 0.953125
epoch: 501: train_loss: 0.7150514001944466, train_acc: 0.4627976218859355, val_loss: 0.680530309677124, val_acc: 0.84375
epoch: 502: train_loss: 0.71516521412304, train_acc: 0.4583333333333333, val_loss: 0.652092307806015, val_acc: 0.890625
epoch: 503: train_loss: 0.7151740335282825, train_acc: 0.4836309552192688, val_loss: 0.5669883191585541, val_acc: 0.984375
epoch: 504: train_loss: 0.7149587343234826, train_acc: 0.6205357114473978, val_loss: 0.5515329837799072, val_acc: 1.0
epoch: 505: train_loss: 0.71483393759517, train_acc: 0.5401785671710968, val_loss: 0.552471399307251, val_acc: 1.0
epoch: 506: train_loss: 0.7146563856598579, train_acc: 0.5848214228947958, val_loss: 0.5638211667537689, val_acc: 0.984375
epoch: 507: train_loss: 0.7144620965120047, train_acc: 0.5639880895614624, val_loss: 0.554654061794281, val_acc: 1.0
epoch: 508: train_loss: 0.7143156175802325, train_acc: 0.5669642885526022, val_loss: 0.5516667664051056, val_acc: 1.0
epoch: 509: train_loss: 0.7141595594828424, train_acc: 0.5625, val_loss: 0.5518395602703094, val_acc: 1.0
epoch: 510: train_loss: 0.7140915628865273, train_acc: 0.5208333333333334, val_loss: 0.5566359758377075, val_acc: 1.0
epoch: 511: train_loss: 0.7140269706142134, train_acc: 0.511904756228129, val_loss: 0.5685812532901764, val_acc: 0.984375
epoch: 512: train_loss: 0.7139126395660222, train_acc: 0.53125, val_loss: 0.5688754916191101, val_acc: 0.984375
epoch: 513: train_loss: 0.7139034973778766, train_acc: 0.4642857114473979, val_loss: 0.581648200750351, val_acc: 0.96875
epoch: 514: train_loss: 0.7138611088485778, train_acc: 0.523809532324473, val_loss: 0.5694969296455383, val_acc: 0.984375
epoch: 515: train_loss: 0.7138543008256327, train_acc: 0.4747023781140645, val_loss: 0.5524232983589172, val_acc: 1.0
epoch: 516: train_loss: 0.713767716936109, train_acc: 0.5342261989911398, val_loss: 0.5617000758647919, val_acc: 0.984375
epoch: 517: train_loss: 0.7136617364071817, train_acc: 0.523809532324473, val_loss: 0.5542437434196472, val_acc: 1.0
epoch: 518: train_loss: 0.7136699632910788, train_acc: 0.4553571442763011, val_loss: 0.5707511007785797, val_acc: 1.0
epoch: 519: train_loss: 0.713625455303834, train_acc: 0.5520833333333334, val_loss: 0.5537599325180054, val_acc: 1.0
epoch: 520: train_loss: 0.7133618164771807, train_acc: 0.6413690447807312, val_loss: 0.5590148270130157, val_acc: 1.0
epoch: 521: train_loss: 0.7133690825405401, train_acc: 0.4627976218859355, val_loss: 0.6041375696659088, val_acc: 0.953125
epoch: 522: train_loss: 0.7135377093019418, train_acc: 0.4345238109429677, val_loss: 0.577118843793869, val_acc: 0.96875
epoch: 523: train_loss: 0.7133832204015806, train_acc: 0.59375, val_loss: 0.5546726286411285, val_acc: 1.0
epoch: 524: train_loss: 0.7132225755472031, train_acc: 0.580357144276301, val_loss: 0.5522505044937134, val_acc: 1.0
epoch: 525: train_loss: 0.7130976972584487, train_acc: 0.5357142885526022, val_loss: 0.5515559017658234, val_acc: 1.0
epoch: 526: train_loss: 0.7129899757448288, train_acc: 0.523809532324473, val_loss: 0.5527660250663757, val_acc: 1.0
epoch: 527: train_loss: 0.7128348410167175, train_acc: 0.555059532324473, val_loss: 0.5516742169857025, val_acc: 1.0
epoch: 528: train_loss: 0.712725599212472, train_acc: 0.538690467675527, val_loss: 0.5639781653881073, val_acc: 0.984375
epoch: 529: train_loss: 0.7126997204134298, train_acc: 0.4776785671710968, val_loss: 0.5739205479621887, val_acc: 0.96875
epoch: 530: train_loss: 0.7125577831036326, train_acc: 0.5625, val_loss: 0.5745801627635956, val_acc: 0.96875
epoch: 531: train_loss: 0.7124591701871767, train_acc: 0.5327380895614624, val_loss: 0.6209661066532135, val_acc: 0.921875
epoch: 532: train_loss: 0.7124109086466996, train_acc: 0.5029761989911398, val_loss: 0.5757080614566803, val_acc: 0.96875
epoch: 533: train_loss: 0.7123838253644817, train_acc: 0.5223214228947958, val_loss: 0.5525263845920563, val_acc: 1.0
epoch: 534: train_loss: 0.7123889137094264, train_acc: 0.4583333333333333, val_loss: 0.573477029800415, val_acc: 0.984375
epoch: 535: train_loss: 0.7122605124405069, train_acc: 0.5505952338377634, val_loss: 0.5943925380706787, val_acc: 0.9375
epoch: 536: train_loss: 0.7122957808012401, train_acc: 0.4598214228947957, val_loss: 0.5514881312847137, val_acc: 1.0
epoch: 537: train_loss: 0.7121466427598299, train_acc: 0.5639880895614624, val_loss: 0.552418053150177, val_acc: 1.0
epoch: 538: train_loss: 0.7120534768241675, train_acc: 0.5372023781140646, val_loss: 0.5637391209602356, val_acc: 0.984375
epoch: 539: train_loss: 0.7121177149223691, train_acc: 0.4211309552192688, val_loss: 0.553785502910614, val_acc: 1.0
epoch: 540: train_loss: 0.7120406280053079, train_acc: 0.5297619005044302, val_loss: 0.5532737970352173, val_acc: 1.0
epoch: 541: train_loss: 0.7120653680293174, train_acc: 0.4449404776096344, val_loss: 0.5695401132106781, val_acc: 0.96875
epoch: 542: train_loss: 0.7120475216721228, train_acc: 0.4821428557236989, val_loss: 0.570003867149353, val_acc: 0.984375
epoch: 543: train_loss: 0.711932880749159, train_acc: 0.5758928656578064, val_loss: 0.6330850124359131, val_acc: 0.921875
epoch: 544: train_loss: 0.7118381924403187, train_acc: 0.5669642885526022, val_loss: 0.5756050944328308, val_acc: 0.96875
epoch: 545: train_loss: 0.7118984461740404, train_acc: 0.4434523781140645, val_loss: 0.5527405738830566, val_acc: 1.0
epoch: 546: train_loss: 0.7118689361721331, train_acc: 0.4940476218859355, val_loss: 0.5661254525184631, val_acc: 0.984375
epoch: 547: train_loss: 0.7118182673760283, train_acc: 0.5089285671710968, val_loss: 0.5614352226257324, val_acc: 1.0
epoch: 548: train_loss: 0.7118462736525532, train_acc: 0.4657738109429677, val_loss: 0.5582265257835388, val_acc: 1.0
epoch: 549: train_loss: 0.7118057623234662, train_acc: 0.5193452338377634, val_loss: 0.5542014241218567, val_acc: 1.0
epoch: 550: train_loss: 0.7117613833994844, train_acc: 0.5223214228947958, val_loss: 0.5529384911060333, val_acc: 1.0
epoch: 551: train_loss: 0.7116285785492779, train_acc: 0.569940467675527, val_loss: 0.5528058409690857, val_acc: 1.0
epoch: 552: train_loss: 0.7116235232159198, train_acc: 0.5059523781140646, val_loss: 0.5854179263114929, val_acc: 0.984375
epoch: 553: train_loss: 0.7116129601510995, train_acc: 0.5029761989911398, val_loss: 0.5641491711139679, val_acc: 0.984375
epoch: 554: train_loss: 0.7115570644537607, train_acc: 0.5089285671710968, val_loss: 0.5561014413833618, val_acc: 1.0
epoch: 555: train_loss: 0.7112873468539125, train_acc: 0.6279761989911398, val_loss: 0.5668579638004303, val_acc: 0.984375
epoch: 556: train_loss: 0.7112509441689628, train_acc: 0.5089285671710968, val_loss: 0.5520294904708862, val_acc: 1.0
epoch: 557: train_loss: 0.7113938684104592, train_acc: 0.4047619005044301, val_loss: 0.5517391264438629, val_acc: 1.0
epoch: 558: train_loss: 0.7113183595697725, train_acc: 0.5327380895614624, val_loss: 0.5518839061260223, val_acc: 1.0
epoch: 559: train_loss: 0.7112060282556784, train_acc: 0.5476190447807312, val_loss: 0.5517638325691223, val_acc: 1.0
epoch: 560: train_loss: 0.7111548601837294, train_acc: 0.5074404776096344, val_loss: 0.5519258379936218, val_acc: 1.0
epoch: 561: train_loss: 0.7110779926375279, train_acc: 0.5565476218859354, val_loss: 0.5521273016929626, val_acc: 1.0
epoch: 562: train_loss: 0.7110080609595443, train_acc: 0.5193452338377634, val_loss: 0.5638299584388733, val_acc: 0.984375
epoch: 563: train_loss: 0.7109213474583117, train_acc: 0.5193452338377634, val_loss: 0.6225716769695282, val_acc: 0.90625
epoch: 564: train_loss: 0.711029060435506, train_acc: 0.4851190447807312, val_loss: 0.6397736966609955, val_acc: 0.90625
epoch: 565: train_loss: 0.7109423558478922, train_acc: 0.517857144276301, val_loss: 0.5567030310630798, val_acc: 1.0
epoch: 566: train_loss: 0.7108797306596777, train_acc: 0.5327380895614624, val_loss: 0.5576978325843811, val_acc: 1.0
epoch: 567: train_loss: 0.7108660169960187, train_acc: 0.4821428557236989, val_loss: 0.5651376843452454, val_acc: 0.984375
epoch: 568: train_loss: 0.7108453095622697, train_acc: 0.5654761989911398, val_loss: 0.5532952845096588, val_acc: 1.0
epoch: 569: train_loss: 0.7108153085959585, train_acc: 0.4955357114473979, val_loss: 0.5849028825759888, val_acc: 0.9375
epoch: 570: train_loss: 0.7108183821529516, train_acc: 0.4702380895614624, val_loss: 0.6165823042392731, val_acc: 0.9375
epoch: 571: train_loss: 0.7107730028859941, train_acc: 0.5535714228947958, val_loss: 0.5833995342254639, val_acc: 0.96875
epoch: 572: train_loss: 0.7107017566951088, train_acc: 0.5104166666666666, val_loss: 0.5519373416900635, val_acc: 1.0
epoch: 573: train_loss: 0.7107609400696594, train_acc: 0.4449404776096344, val_loss: 0.551537424325943, val_acc: 1.0
epoch: 574: train_loss: 0.7106419095440186, train_acc: 0.5416666666666666, val_loss: 0.5666964054107666, val_acc: 0.984375
epoch: 575: train_loss: 0.7105782515955743, train_acc: 0.574404756228129, val_loss: 0.5623139142990112, val_acc: 1.0
epoch: 576: train_loss: 0.7105523799279739, train_acc: 0.5163690447807312, val_loss: 0.5519695580005646, val_acc: 1.0
epoch: 577: train_loss: 0.7106178246842398, train_acc: 0.4300595223903656, val_loss: 0.5645598471164703, val_acc: 0.984375
epoch: 578: train_loss: 0.7105211043536285, train_acc: 0.5386904776096344, val_loss: 0.579540342092514, val_acc: 0.96875
epoch: 579: train_loss: 0.710540365384913, train_acc: 0.5059523781140646, val_loss: 0.5559698343276978, val_acc: 1.0
epoch: 580: train_loss: 0.7105135580827228, train_acc: 0.5461309552192688, val_loss: 0.5671970248222351, val_acc: 0.984375
epoch: 581: train_loss: 0.7105790699265666, train_acc: 0.4866071442763011, val_loss: 0.5611923038959503, val_acc: 1.0
epoch: 582: train_loss: 0.7106210486830269, train_acc: 0.5074404776096344, val_loss: 0.5518644154071808, val_acc: 1.0
epoch: 583: train_loss: 0.7105215018334453, train_acc: 0.5297619005044302, val_loss: 0.5768784880638123, val_acc: 0.96875
epoch: 584: train_loss: 0.7104320927902503, train_acc: 0.5133928656578064, val_loss: 0.596322238445282, val_acc: 0.953125
epoch: 585: train_loss: 0.7104187127334128, train_acc: 0.543154756228129, val_loss: 0.5751962065696716, val_acc: 0.96875
epoch: 586: train_loss: 0.7105136133946185, train_acc: 0.4092261890570323, val_loss: 0.5515322387218475, val_acc: 1.0
epoch: 587: train_loss: 0.7103810335705878, train_acc: 0.5788690447807312, val_loss: 0.5539224445819855, val_acc: 1.0
epoch: 588: train_loss: 0.7104325575145812, train_acc: 0.4613095223903656, val_loss: 0.5561868250370026, val_acc: 1.0
epoch: 589: train_loss: 0.7103563703386121, train_acc: 0.5267857114473978, val_loss: 0.5514929294586182, val_acc: 1.0
epoch: 590: train_loss: 0.710377515202784, train_acc: 0.4583333333333333, val_loss: 0.5839006900787354, val_acc: 0.9375
epoch: 591: train_loss: 0.7103112228885962, train_acc: 0.5342261989911398, val_loss: 0.5649284422397614, val_acc: 0.984375
epoch: 592: train_loss: 0.710220573690917, train_acc: 0.5372023781140646, val_loss: 0.5532214343547821, val_acc: 1.0
epoch: 593: train_loss: 0.7101196982175544, train_acc: 0.5491071343421936, val_loss: 0.5745847523212433, val_acc: 0.96875
epoch: 594: train_loss: 0.7101689274571521, train_acc: 0.4389880895614624, val_loss: 0.5790797472000122, val_acc: 0.96875
epoch: 595: train_loss: 0.710131485323511, train_acc: 0.511904756228129, val_loss: 0.6703619360923767, val_acc: 0.859375
epoch: 596: train_loss: 0.7101438772338916, train_acc: 0.4598214228947957, val_loss: 0.5650248229503632, val_acc: 0.984375
epoch: 597: train_loss: 0.7100874290219117, train_acc: 0.5059523781140646, val_loss: 0.5520624220371246, val_acc: 1.0
epoch: 598: train_loss: 0.7100227942517151, train_acc: 0.5327380895614624, val_loss: 0.5545779466629028, val_acc: 1.0
epoch: 599: train_loss: 0.7100927265816264, train_acc: 0.4330357114473979, val_loss: 0.5514832735061646, val_acc: 1.0
epoch: 600: train_loss: 0.7100096144943321, train_acc: 0.5669642885526022, val_loss: 0.5519853234291077, val_acc: 1.0
epoch: 601: train_loss: 0.7099590084679498, train_acc: 0.4895833333333333, val_loss: 0.5661754608154297, val_acc: 0.984375
epoch: 602: train_loss: 0.7098766521830265, train_acc: 0.5223214228947958, val_loss: 0.568877100944519, val_acc: 0.984375
epoch: 603: train_loss: 0.7097825102392936, train_acc: 0.5729166666666666, val_loss: 0.5688114166259766, val_acc: 0.984375
epoch: 604: train_loss: 0.7097160743616171, train_acc: 0.569940467675527, val_loss: 0.5561320781707764, val_acc: 1.0
epoch: 605: train_loss: 0.7097803848017953, train_acc: 0.4285714328289032, val_loss: 0.5514934659004211, val_acc: 1.0
epoch: 606: train_loss: 0.7096859528296992, train_acc: 0.5505952338377634, val_loss: 0.5515617728233337, val_acc: 1.0
epoch: 607: train_loss: 0.7096381170773192, train_acc: 0.5104166666666666, val_loss: 0.5537897050380707, val_acc: 1.0
epoch: 608: train_loss: 0.7095316412935637, train_acc: 0.5505952338377634, val_loss: 0.5514715015888214, val_acc: 1.0
epoch: 609: train_loss: 0.7094176241608916, train_acc: 0.5654761989911398, val_loss: 0.5515359938144684, val_acc: 1.0
epoch: 610: train_loss: 0.7095396900788331, train_acc: 0.4017857114473979, val_loss: 0.5514674782752991, val_acc: 1.0
epoch: 611: train_loss: 0.7095368452581184, train_acc: 0.4985119005044301, val_loss: 0.5514760315418243, val_acc: 1.0
epoch: 612: train_loss: 0.7094083360642957, train_acc: 0.5446428656578064, val_loss: 0.5620938539505005, val_acc: 0.984375
epoch: 613: train_loss: 0.7092966635374759, train_acc: 0.5565476218859354, val_loss: 0.551490068435669, val_acc: 1.0
epoch: 614: train_loss: 0.7092479747808399, train_acc: 0.5297619005044302, val_loss: 0.5538134574890137, val_acc: 1.0
epoch: 615: train_loss: 0.7091829119435636, train_acc: 0.53125, val_loss: 0.5545971393585205, val_acc: 1.0
epoch: 616: train_loss: 0.7091709089755754, train_acc: 0.5029761989911398, val_loss: 0.5529458820819855, val_acc: 1.0
epoch: 617: train_loss: 0.709159320644319, train_acc: 0.5208333333333334, val_loss: 0.5515016615390778, val_acc: 1.0
epoch: 618: train_loss: 0.709011993102637, train_acc: 0.59375, val_loss: 0.5514723658561707, val_acc: 1.0
epoch: 619: train_loss: 0.708978521567519, train_acc: 0.4776785671710968, val_loss: 0.5618506968021393, val_acc: 0.984375
epoch: 620: train_loss: 0.7088670220927916, train_acc: 0.5565476218859354, val_loss: 0.5592121779918671, val_acc: 0.984375
epoch: 621: train_loss: 0.708899720560955, train_acc: 0.4568452338377635, val_loss: 0.560528427362442, val_acc: 1.0
epoch: 622: train_loss: 0.7088058725788735, train_acc: 0.5342261989911398, val_loss: 0.55170938372612, val_acc: 1.0
epoch: 623: train_loss: 0.7088325820290126, train_acc: 0.4553571442763011, val_loss: 0.5534740090370178, val_acc: 1.0
epoch: 624: train_loss: 0.7087221772193909, train_acc: 0.5788690447807312, val_loss: 0.5538318157196045, val_acc: 1.0
epoch: 625: train_loss: 0.708611881358428, train_acc: 0.5654761989911398, val_loss: 0.5520218312740326, val_acc: 1.0
epoch: 626: train_loss: 0.708612041705343, train_acc: 0.4851190447807312, val_loss: 0.5514985620975494, val_acc: 1.0
epoch: 627: train_loss: 0.7084495919398694, train_acc: 0.5848214228947958, val_loss: 0.5515212714672089, val_acc: 1.0
epoch: 628: train_loss: 0.7083788891285895, train_acc: 0.5163690447807312, val_loss: 0.5517098307609558, val_acc: 1.0
epoch: 629: train_loss: 0.7083005039780228, train_acc: 0.5342261989911398, val_loss: 0.5514642298221588, val_acc: 1.0
epoch: 630: train_loss: 0.7082850403465799, train_acc: 0.4761904776096344, val_loss: 0.5516825318336487, val_acc: 1.0
epoch: 631: train_loss: 0.7082859419820681, train_acc: 0.4642857114473979, val_loss: 0.5517343878746033, val_acc: 1.0
epoch: 632: train_loss: 0.7081240658511482, train_acc: 0.586309532324473, val_loss: 0.5673108696937561, val_acc: 0.984375
epoch: 633: train_loss: 0.7080834659930909, train_acc: 0.4985119005044301, val_loss: 0.5609560608863831, val_acc: 1.0
epoch: 634: train_loss: 0.7080773215907139, train_acc: 0.4791666666666667, val_loss: 0.5539758205413818, val_acc: 1.0
epoch: 635: train_loss: 0.7080067770723526, train_acc: 0.517857144276301, val_loss: 0.554513156414032, val_acc: 1.0
epoch: 636: train_loss: 0.7079023852016463, train_acc: 0.569940467675527, val_loss: 0.5699426829814911, val_acc: 0.984375
epoch: 637: train_loss: 0.7077703804493947, train_acc: 0.555059532324473, val_loss: 0.5565949082374573, val_acc: 1.0
epoch: 638: train_loss: 0.7076318149275622, train_acc: 0.5997023781140646, val_loss: 0.5524597465991974, val_acc: 1.0
epoch: 639: train_loss: 0.707639863093694, train_acc: 0.4806547562281291, val_loss: 0.5515238642692566, val_acc: 1.0
epoch: 640: train_loss: 0.7078057712896131, train_acc: 0.3764880994955699, val_loss: 0.5553576648235321, val_acc: 1.0
epoch: 641: train_loss: 0.7077893007272004, train_acc: 0.4880952338377635, val_loss: 0.5515371263027191, val_acc: 1.0
epoch: 642: train_loss: 0.7077453319985609, train_acc: 0.5401785671710968, val_loss: 0.5514836311340332, val_acc: 1.0
epoch: 643: train_loss: 0.7078769031457031, train_acc: 0.4300595323244731, val_loss: 0.5515992343425751, val_acc: 1.0
epoch: 644: train_loss: 0.7078049073847688, train_acc: 0.543154756228129, val_loss: 0.5851688086986542, val_acc: 0.9375
epoch: 645: train_loss: 0.7077280962676331, train_acc: 0.523809532324473, val_loss: 0.5516219437122345, val_acc: 1.0
epoch: 646: train_loss: 0.7077173621368309, train_acc: 0.5, val_loss: 0.5525099039077759, val_acc: 1.0
epoch: 647: train_loss: 0.7075717662894185, train_acc: 0.5877976218859354, val_loss: 0.5518939793109894, val_acc: 1.0
epoch: 648: train_loss: 0.7074773225590822, train_acc: 0.5491071343421936, val_loss: 0.5661005675792694, val_acc: 0.984375
epoch: 649: train_loss: 0.7075933237259203, train_acc: 0.4241071442763011, val_loss: 0.5515016913414001, val_acc: 1.0
epoch: 650: train_loss: 0.7075479101109248, train_acc: 0.5104166666666666, val_loss: 0.5517154932022095, val_acc: 1.0
epoch: 651: train_loss: 0.7074693332298645, train_acc: 0.5357142885526022, val_loss: 0.5520699620246887, val_acc: 1.0
epoch: 652: train_loss: 0.7074066662800561, train_acc: 0.5684523781140646, val_loss: 0.5551857352256775, val_acc: 1.0
epoch: 653: train_loss: 0.707301395900631, train_acc: 0.5357142885526022, val_loss: 0.5514562427997589, val_acc: 1.0
epoch: 654: train_loss: 0.7072182602554786, train_acc: 0.5505952338377634, val_loss: 0.5653226375579834, val_acc: 0.984375
epoch: 655: train_loss: 0.7071445992261898, train_acc: 0.5446428656578064, val_loss: 0.5522473156452179, val_acc: 1.0
epoch: 656: train_loss: 0.7069526138431466, train_acc: 0.613095243771871, val_loss: 0.5520032942295074, val_acc: 1.0
epoch: 657: train_loss: 0.7067718895071062, train_acc: 0.6086309552192688, val_loss: 0.5543043613433838, val_acc: 1.0
epoch: 658: train_loss: 0.7067639947419946, train_acc: 0.4747023781140645, val_loss: 0.5641362965106964, val_acc: 0.984375
epoch: 659: train_loss: 0.7066627098454369, train_acc: 0.555059532324473, val_loss: 0.5642853677272797, val_acc: 0.984375
epoch: 660: train_loss: 0.7066326137149328, train_acc: 0.4970238109429677, val_loss: 0.5669532418251038, val_acc: 0.984375
epoch: 661: train_loss: 0.7063925186732984, train_acc: 0.6517857114473978, val_loss: 0.5620632171630859, val_acc: 0.984375
epoch: 662: train_loss: 0.7062321798833786, train_acc: 0.601190467675527, val_loss: 0.5666994750499725, val_acc: 0.984375
epoch: 663: train_loss: 0.7061149070420897, train_acc: 0.5476190447807312, val_loss: 0.5575408935546875, val_acc: 1.0
epoch: 664: train_loss: 0.7060478754808429, train_acc: 0.5327380895614624, val_loss: 0.5789490044116974, val_acc: 0.96875
epoch: 665: train_loss: 0.7061123758464961, train_acc: 0.4553571442763011, val_loss: 0.558037519454956, val_acc: 1.0
epoch: 666: train_loss: 0.7060985954805113, train_acc: 0.5148809552192688, val_loss: 0.5516428351402283, val_acc: 1.0
epoch: 667: train_loss: 0.7060262714614886, train_acc: 0.5342261989911398, val_loss: 0.551648736000061, val_acc: 1.0
epoch: 668: train_loss: 0.7060667063445075, train_acc: 0.4672619005044301, val_loss: 0.552264541387558, val_acc: 1.0
epoch: 669: train_loss: 0.7061484838896128, train_acc: 0.4315476218859355, val_loss: 0.5517940819263458, val_acc: 1.0
epoch: 670: train_loss: 0.706196146022012, train_acc: 0.4404761890570323, val_loss: 0.553028404712677, val_acc: 1.0
epoch: 671: train_loss: 0.7060651668303068, train_acc: 0.580357144276301, val_loss: 0.557891845703125, val_acc: 1.0
epoch: 672: train_loss: 0.7060768387883296, train_acc: 0.5014880895614624, val_loss: 0.554501086473465, val_acc: 1.0
epoch: 673: train_loss: 0.7059364511044631, train_acc: 0.5997023781140646, val_loss: 0.559602290391922, val_acc: 0.984375
epoch: 674: train_loss: 0.7058380304442512, train_acc: 0.574404756228129, val_loss: 0.552863210439682, val_acc: 1.0
epoch: 675: train_loss: 0.7057364021944107, train_acc: 0.5610119005044302, val_loss: 0.5613299608230591, val_acc: 1.0
epoch: 676: train_loss: 0.7055476704646307, train_acc: 0.6309523781140646, val_loss: 0.5984068810939789, val_acc: 0.9375
epoch: 677: train_loss: 0.7054414684676374, train_acc: 0.5773809552192688, val_loss: 0.5789871215820312, val_acc: 0.9375
epoch: 678: train_loss: 0.7051935897455183, train_acc: 0.6651785771052042, val_loss: 0.5582100749015808, val_acc: 1.0
epoch: 679: train_loss: 0.705247713698476, train_acc: 0.4494047562281291, val_loss: 0.5514529943466187, val_acc: 1.0
epoch: 680: train_loss: 0.7051403414689614, train_acc: 0.5654761989911398, val_loss: 0.5515628457069397, val_acc: 1.0
epoch: 681: train_loss: 0.7050790376415002, train_acc: 0.5625, val_loss: 0.5516247153282166, val_acc: 1.0
epoch: 682: train_loss: 0.7048860629370528, train_acc: 0.6428571343421936, val_loss: 0.5536151230335236, val_acc: 1.0
epoch: 683: train_loss: 0.7047683385002915, train_acc: 0.5877976218859354, val_loss: 0.5518771111965179, val_acc: 1.0
epoch: 684: train_loss: 0.7048211302925491, train_acc: 0.4375, val_loss: 0.5514599978923798, val_acc: 1.0
epoch: 685: train_loss: 0.7046833426281593, train_acc: 0.5892857114473978, val_loss: 0.5517423748970032, val_acc: 1.0
epoch: 686: train_loss: 0.7046909443184568, train_acc: 0.4747023781140645, val_loss: 0.5514479577541351, val_acc: 1.0
epoch: 687: train_loss: 0.7046672114019478, train_acc: 0.4895833333333333, val_loss: 0.5896707475185394, val_acc: 0.9375
epoch: 688: train_loss: 0.704623753213052, train_acc: 0.5044642885526022, val_loss: 0.5669889450073242, val_acc: 0.984375
epoch: 689: train_loss: 0.7045522026825644, train_acc: 0.5520833333333334, val_loss: 0.5515681803226471, val_acc: 1.0
epoch: 690: train_loss: 0.7045176081391734, train_acc: 0.5089285671710968, val_loss: 0.566504955291748, val_acc: 0.984375
epoch: 691: train_loss: 0.7044479802021172, train_acc: 0.5252976218859354, val_loss: 0.5848357677459717, val_acc: 0.953125
epoch: 692: train_loss: 0.7045362894353872, train_acc: 0.4226190447807312, val_loss: 0.5739204585552216, val_acc: 1.0
epoch: 693: train_loss: 0.7046422804495558, train_acc: 0.3943452338377635, val_loss: 0.5515967905521393, val_acc: 1.0
epoch: 694: train_loss: 0.7046097889363909, train_acc: 0.5163690447807312, val_loss: 0.553157389163971, val_acc: 1.0
epoch: 695: train_loss: 0.7044469392305361, train_acc: 0.6116071343421936, val_loss: 0.5515052974224091, val_acc: 1.0
epoch: 696: train_loss: 0.7043963790595503, train_acc: 0.5446428656578064, val_loss: 0.5517188310623169, val_acc: 1.0
epoch: 697: train_loss: 0.7044473644399369, train_acc: 0.4494047562281291, val_loss: 0.5516628324985504, val_acc: 1.0
epoch: 698: train_loss: 0.7044611840260615, train_acc: 0.4672619005044301, val_loss: 0.5578413605690002, val_acc: 0.984375
epoch: 699: train_loss: 0.7044782539350646, train_acc: 0.5, val_loss: 0.5514476597309113, val_acc: 1.0
epoch: 700: train_loss: 0.7045299346623849, train_acc: 0.4375, val_loss: 0.551509439945221, val_acc: 1.0
epoch: 701: train_loss: 0.704455988260529, train_acc: 0.5535714228947958, val_loss: 0.5607027113437653, val_acc: 0.984375
epoch: 702: train_loss: 0.7044275135522546, train_acc: 0.5029761989911398, val_loss: 0.622322291135788, val_acc: 0.921875
epoch: 703: train_loss: 0.7044535829459853, train_acc: 0.46875, val_loss: 0.5790297091007233, val_acc: 1.0
epoch: 704: train_loss: 0.7044907752926468, train_acc: 0.4717261989911397, val_loss: 0.5713094770908356, val_acc: 0.984375
epoch: 705: train_loss: 0.7044950608969636, train_acc: 0.4657738109429677, val_loss: 0.5588797628879547, val_acc: 0.984375
epoch: 706: train_loss: 0.7045336058402837, train_acc: 0.4375, val_loss: 0.568568229675293, val_acc: 0.984375
epoch: 707: train_loss: 0.7045245738156323, train_acc: 0.4985119005044301, val_loss: 0.5570755898952484, val_acc: 1.0
epoch: 708: train_loss: 0.7044931350176918, train_acc: 0.517857144276301, val_loss: 0.6156246364116669, val_acc: 0.9375
epoch: 709: train_loss: 0.7042887352022207, train_acc: 0.6532738010088602, val_loss: 0.5610867142677307, val_acc: 1.0
epoch: 710: train_loss: 0.7042536009967578, train_acc: 0.5104166666666666, val_loss: 0.5515522658824921, val_acc: 1.0
epoch: 711: train_loss: 0.7043047595476166, train_acc: 0.4494047661622365, val_loss: 0.5514482855796814, val_acc: 1.0
epoch: 712: train_loss: 0.7041872398576874, train_acc: 0.5982142885526022, val_loss: 0.5784834027290344, val_acc: 0.9375
epoch: 713: train_loss: 0.7041275502225153, train_acc: 0.5446428656578064, val_loss: 0.5555340647697449, val_acc: 1.0
epoch: 714: train_loss: 0.7041411433047625, train_acc: 0.46875, val_loss: 0.5517743825912476, val_acc: 1.0
epoch: 715: train_loss: 0.70418157581122, train_acc: 0.4479166666666667, val_loss: 0.552025556564331, val_acc: 1.0
epoch: 716: train_loss: 0.7042040826691299, train_acc: 0.4836309552192688, val_loss: 0.5515058636665344, val_acc: 1.0
epoch: 717: train_loss: 0.7041908511295292, train_acc: 0.4702380895614624, val_loss: 0.5514480173587799, val_acc: 1.0
epoch: 718: train_loss: 0.7040698608287683, train_acc: 0.5758928656578064, val_loss: 0.5556670725345612, val_acc: 1.0
epoch: 719: train_loss: 0.7039717114496009, train_acc: 0.5520833333333334, val_loss: 0.5516871511936188, val_acc: 1.0
epoch: 720: train_loss: 0.7039995584921102, train_acc: 0.4568452338377635, val_loss: 0.5515697598457336, val_acc: 1.0
epoch: 721: train_loss: 0.703997315998883, train_acc: 0.5074404776096344, val_loss: 0.5704604089260101, val_acc: 0.984375
epoch: 722: train_loss: 0.703823524240856, train_acc: 0.6160714228947958, val_loss: 0.5815841257572174, val_acc: 0.96875
epoch: 723: train_loss: 0.7037775107482739, train_acc: 0.5282738109429678, val_loss: 0.5527411103248596, val_acc: 1.0
epoch: 724: train_loss: 0.7037667137315903, train_acc: 0.4895833333333333, val_loss: 0.5515795946121216, val_acc: 1.0
epoch: 725: train_loss: 0.7038715216985418, train_acc: 0.4047619005044301, val_loss: 0.5908956825733185, val_acc: 0.9375
epoch: 726: train_loss: 0.7038922485191303, train_acc: 0.4761904776096344, val_loss: 0.5982781052589417, val_acc: 0.921875
epoch: 727: train_loss: 0.7038428306497714, train_acc: 0.523809532324473, val_loss: 0.5554571449756622, val_acc: 1.0
epoch: 728: train_loss: 0.7038096814025848, train_acc: 0.5014880895614624, val_loss: 0.5517576932907104, val_acc: 1.0
epoch: 729: train_loss: 0.703864118511274, train_acc: 0.4375, val_loss: 0.5514737367630005, val_acc: 1.0
epoch: 730: train_loss: 0.7039211725487429, train_acc: 0.4270833333333333, val_loss: 0.5514608919620514, val_acc: 1.0
epoch: 731: train_loss: 0.703927347691007, train_acc: 0.4761904776096344, val_loss: 0.5514695346355438, val_acc: 1.0
epoch: 732: train_loss: 0.7039949497535805, train_acc: 0.4791666666666667, val_loss: 0.5661638677120209, val_acc: 0.984375
epoch: 733: train_loss: 0.7038354421750076, train_acc: 0.6086309552192688, val_loss: 0.5774552226066589, val_acc: 0.984375
epoch: 734: train_loss: 0.7038016353199542, train_acc: 0.523809532324473, val_loss: 0.5555116832256317, val_acc: 1.0
epoch: 735: train_loss: 0.7037527405562393, train_acc: 0.523809532324473, val_loss: 0.5515124797821045, val_acc: 1.0
epoch: 736: train_loss: 0.7036683991423349, train_acc: 0.5788690447807312, val_loss: 0.5518485605716705, val_acc: 1.0
epoch: 737: train_loss: 0.7036583965952159, train_acc: 0.46875, val_loss: 0.5557549297809601, val_acc: 1.0
epoch: 738: train_loss: 0.7035624999611778, train_acc: 0.5669642885526022, val_loss: 0.5522539913654327, val_acc: 1.0
epoch: 739: train_loss: 0.7035440977494997, train_acc: 0.4925595323244731, val_loss: 0.552015483379364, val_acc: 1.0
epoch: 740: train_loss: 0.7034527453896811, train_acc: 0.5818452338377634, val_loss: 0.5602172315120697, val_acc: 0.984375
epoch: 741: train_loss: 0.7034425872168023, train_acc: 0.5014880895614624, val_loss: 0.5917230248451233, val_acc: 0.96875
epoch: 742: train_loss: 0.7033703052152589, train_acc: 0.5416666666666666, val_loss: 0.6569889485836029, val_acc: 0.875
epoch: 743: train_loss: 0.7034221375795036, train_acc: 0.4598214228947957, val_loss: 0.5541821122169495, val_acc: 1.0
epoch: 744: train_loss: 0.7032820017412472, train_acc: 0.613095243771871, val_loss: 0.6201606094837189, val_acc: 0.9375
epoch: 745: train_loss: 0.7032089459379408, train_acc: 0.5565476218859354, val_loss: 0.6212792098522186, val_acc: 0.9375
epoch: 746: train_loss: 0.7031111198442954, train_acc: 0.5684523781140646, val_loss: 0.5554389953613281, val_acc: 1.0
epoch: 747: train_loss: 0.7029530206917655, train_acc: 0.6517857114473978, val_loss: 0.5521144866943359, val_acc: 1.0
epoch: 748: train_loss: 0.7028481121401708, train_acc: 0.5833333333333334, val_loss: 0.58321213722229, val_acc: 0.984375
epoch: 749: train_loss: 0.7028674559195837, train_acc: 0.511904756228129, val_loss: 0.5884808003902435, val_acc: 0.9375
epoch: 750: train_loss: 0.7029008083723409, train_acc: 0.4419642885526021, val_loss: 0.5549196898937225, val_acc: 1.0
epoch: 751: train_loss: 0.7029817219665075, train_acc: 0.4226190447807312, val_loss: 0.5930575132369995, val_acc: 0.9375
epoch: 752: train_loss: 0.702863699952392, train_acc: 0.5952380895614624, val_loss: 0.5688930153846741, val_acc: 0.984375
epoch: 753: train_loss: 0.702749292253394, train_acc: 0.5758928656578064, val_loss: 0.5595953166484833, val_acc: 1.0
epoch: 754: train_loss: 0.7027495223296927, train_acc: 0.5, val_loss: 0.5518803000450134, val_acc: 1.0
epoch: 755: train_loss: 0.7027659122249017, train_acc: 0.46875, val_loss: 0.5644839704036713, val_acc: 0.984375
epoch: 756: train_loss: 0.7026328088978954, train_acc: 0.5907738010088602, val_loss: 0.5544891655445099, val_acc: 1.0
epoch: 757: train_loss: 0.7025473495927316, train_acc: 0.5758928656578064, val_loss: 0.5551610589027405, val_acc: 1.0
epoch: 758: train_loss: 0.7024589707231546, train_acc: 0.5625, val_loss: 0.6643196940422058, val_acc: 0.875
epoch: 759: train_loss: 0.7023813012007039, train_acc: 0.574404756228129, val_loss: 0.6915079355239868, val_acc: 0.859375
epoch: 760: train_loss: 0.7024150180988966, train_acc: 0.5, val_loss: 0.5621117949485779, val_acc: 0.984375
epoch: 761: train_loss: 0.7024165297404573, train_acc: 0.4940476218859355, val_loss: 0.5978924036026001, val_acc: 0.9375
epoch: 762: train_loss: 0.7023623418136165, train_acc: 0.5625, val_loss: 0.5711296200752258, val_acc: 0.984375
epoch: 763: train_loss: 0.7022744579855063, train_acc: 0.5684523781140646, val_loss: 0.5829960703849792, val_acc: 0.96875
epoch: 764: train_loss: 0.7023099512705879, train_acc: 0.4806547562281291, val_loss: 0.5892826318740845, val_acc: 0.953125
epoch: 765: train_loss: 0.7023046860314125, train_acc: 0.517857144276301, val_loss: 0.5524622201919556, val_acc: 1.0
epoch: 766: train_loss: 0.7022661679027293, train_acc: 0.5193452338377634, val_loss: 0.5597585737705231, val_acc: 1.0
epoch: 767: train_loss: 0.70216212710107, train_acc: 0.586309532324473, val_loss: 0.5517696142196655, val_acc: 1.0
epoch: 768: train_loss: 0.7021447273240137, train_acc: 0.5133928656578064, val_loss: 0.5573228001594543, val_acc: 1.0
epoch: 769: train_loss: 0.7020322438571365, train_acc: 0.5788690447807312, val_loss: 0.5515550673007965, val_acc: 1.0
epoch: 770: train_loss: 0.7020088213246676, train_acc: 0.5014880895614624, val_loss: 0.55145063996315, val_acc: 1.0
epoch: 771: train_loss: 0.7019783404490083, train_acc: 0.5104166666666666, val_loss: 0.5515292286872864, val_acc: 1.0
epoch: 772: train_loss: 0.7019675661516173, train_acc: 0.4940476218859355, val_loss: 0.6208415329456329, val_acc: 0.921875
epoch: 773: train_loss: 0.7019246602936311, train_acc: 0.5342261890570322, val_loss: 0.5575739443302155, val_acc: 1.0
epoch: 774: train_loss: 0.701921953937059, train_acc: 0.4970238109429677, val_loss: 0.5790127515792847, val_acc: 0.9375
epoch: 775: train_loss: 0.7017214672441542, train_acc: 0.6875, val_loss: 0.5802120268344879, val_acc: 0.96875
epoch: 776: train_loss: 0.7015749738099802, train_acc: 0.6086309552192688, val_loss: 0.5581461787223816, val_acc: 1.0
epoch: 777: train_loss: 0.7015261736274688, train_acc: 0.5505952338377634, val_loss: 0.5563663840293884, val_acc: 1.0
epoch: 778: train_loss: 0.7014435186032105, train_acc: 0.5476190447807312, val_loss: 0.5519082546234131, val_acc: 1.0
epoch: 779: train_loss: 0.7013679135431595, train_acc: 0.5535714228947958, val_loss: 0.5529335737228394, val_acc: 1.0
epoch: 780: train_loss: 0.7012743097541176, train_acc: 0.5714285771052042, val_loss: 0.5531256198883057, val_acc: 1.0
epoch: 781: train_loss: 0.7012282481761645, train_acc: 0.549107144276301, val_loss: 0.5515139698982239, val_acc: 1.0
epoch: 782: train_loss: 0.7013194524338524, train_acc: 0.3913690447807312, val_loss: 0.5515982210636139, val_acc: 1.0
epoch: 783: train_loss: 0.7013273506368304, train_acc: 0.4672619005044301, val_loss: 0.5519873201847076, val_acc: 1.0
epoch: 784: train_loss: 0.701298802432994, train_acc: 0.5, val_loss: 0.5514482259750366, val_acc: 1.0
epoch: 785: train_loss: 0.7012269103218686, train_acc: 0.5833333333333334, val_loss: 0.5516685545444489, val_acc: 1.0
epoch: 786: train_loss: 0.7011616972860067, train_acc: 0.5446428656578064, val_loss: 0.5520187616348267, val_acc: 1.0
epoch: 787: train_loss: 0.7010804323602452, train_acc: 0.5580357114473978, val_loss: 0.5633622109889984, val_acc: 0.984375
epoch: 788: train_loss: 0.7010061818560219, train_acc: 0.53125, val_loss: 0.5677726566791534, val_acc: 0.984375
epoch: 789: train_loss: 0.7010143672237923, train_acc: 0.4866071442763011, val_loss: 0.5715044438838959, val_acc: 1.0
epoch: 790: train_loss: 0.700963396677929, train_acc: 0.5223214228947958, val_loss: 0.5532439351081848, val_acc: 1.0
epoch: 791: train_loss: 0.7009135777576004, train_acc: 0.5193452338377634, val_loss: 0.5517719089984894, val_acc: 1.0
epoch: 792: train_loss: 0.7009142958576715, train_acc: 0.5074404776096344, val_loss: 0.5516966581344604, val_acc: 1.0
epoch: 793: train_loss: 0.7009085008544548, train_acc: 0.4791666666666667, val_loss: 0.5514466464519501, val_acc: 1.0
epoch: 794: train_loss: 0.7008186336958186, train_acc: 0.5565476218859354, val_loss: 0.5517393052577972, val_acc: 1.0
epoch: 795: train_loss: 0.7008285820858565, train_acc: 0.4866071442763011, val_loss: 0.551540195941925, val_acc: 1.0
epoch: 796: train_loss: 0.700792003193943, train_acc: 0.517857144276301, val_loss: 0.5516538321971893, val_acc: 1.0
epoch: 797: train_loss: 0.7006120109617863, train_acc: 0.6413690447807312, val_loss: 0.5514675974845886, val_acc: 1.0
epoch: 798: train_loss: 0.7005460490074368, train_acc: 0.5803571343421936, val_loss: 0.5514806807041168, val_acc: 1.0
epoch: 799: train_loss: 0.700497283736865, train_acc: 0.5193452338377634, val_loss: 0.5521938502788544, val_acc: 1.0
epoch: 800: train_loss: 0.7005171115032299, train_acc: 0.4672619005044301, val_loss: 0.5672796666622162, val_acc: 0.984375
epoch: 801: train_loss: 0.7004476741067789, train_acc: 0.5520833333333334, val_loss: 0.5526627600193024, val_acc: 1.0
epoch: 802: train_loss: 0.7003991620428787, train_acc: 0.5297619005044302, val_loss: 0.5515660345554352, val_acc: 1.0
epoch: 803: train_loss: 0.700336362147213, train_acc: 0.543154756228129, val_loss: 0.55144864320755, val_acc: 1.0
epoch: 804: train_loss: 0.7003272607706599, train_acc: 0.4940476218859355, val_loss: 0.5514518320560455, val_acc: 1.0
epoch: 805: train_loss: 0.7003106982181447, train_acc: 0.4791666666666667, val_loss: 0.5544256269931793, val_acc: 1.0
epoch: 806: train_loss: 0.7002878404264363, train_acc: 0.4955357114473979, val_loss: 0.5517252385616302, val_acc: 1.0
epoch: 807: train_loss: 0.7002555456737877, train_acc: 0.5372023781140646, val_loss: 0.5606878995895386, val_acc: 1.0
epoch: 808: train_loss: 0.7001664702618228, train_acc: 0.5952380895614624, val_loss: 0.5617667734622955, val_acc: 0.984375
epoch: 809: train_loss: 0.700104457875829, train_acc: 0.5446428656578064, val_loss: 0.5514737665653229, val_acc: 1.0
epoch: 810: train_loss: 0.7000993204567595, train_acc: 0.4791666666666667, val_loss: 0.551593542098999, val_acc: 1.0
epoch: 811: train_loss: 0.7000897203270833, train_acc: 0.5014880895614624, val_loss: 0.5514536798000336, val_acc: 1.0
epoch: 812: train_loss: 0.7000751981191728, train_acc: 0.523809532324473, val_loss: 0.5539688766002655, val_acc: 1.0
epoch: 813: train_loss: 0.7000405752297607, train_acc: 0.5386904776096344, val_loss: 0.5525960326194763, val_acc: 1.0
epoch: 814: train_loss: 0.7000908945723305, train_acc: 0.4672619005044301, val_loss: 0.5794975757598877, val_acc: 0.984375
epoch: 815: train_loss: 0.7000203370268832, train_acc: 0.5535714228947958, val_loss: 0.5524108111858368, val_acc: 1.0
epoch: 816: train_loss: 0.6999428770047024, train_acc: 0.574404756228129, val_loss: 0.5671088993549347, val_acc: 0.984375
epoch: 817: train_loss: 0.6998223360224092, train_acc: 0.6205357114473978, val_loss: 0.5514529645442963, val_acc: 1.0
epoch: 818: train_loss: 0.6998693582290649, train_acc: 0.4434523781140645, val_loss: 0.6634302139282227, val_acc: 0.875
epoch: 819: train_loss: 0.6999458813328088, train_acc: 0.46875, val_loss: 0.5516412258148193, val_acc: 1.0
epoch: 820: train_loss: 0.699891008113005, train_acc: 0.5386904776096344, val_loss: 0.5702043771743774, val_acc: 0.984375
epoch: 821: train_loss: 0.6998797293272045, train_acc: 0.550595243771871, val_loss: 0.5514848232269287, val_acc: 1.0
epoch: 822: train_loss: 0.6998342196677758, train_acc: 0.5193452338377634, val_loss: 0.5514627695083618, val_acc: 1.0
epoch: 823: train_loss: 0.6998993018973612, train_acc: 0.4330357114473979, val_loss: 0.6135822236537933, val_acc: 0.9375
epoch: 824: train_loss: 0.6998916219942501, train_acc: 0.5252976218859354, val_loss: 0.5946054756641388, val_acc: 0.9375
epoch: 825: train_loss: 0.6998847857225898, train_acc: 0.4895833333333333, val_loss: 0.5591210424900055, val_acc: 1.0
epoch: 826: train_loss: 0.6998461810680519, train_acc: 0.5327380895614624, val_loss: 0.5515386760234833, val_acc: 1.0
epoch: 827: train_loss: 0.6998460822155322, train_acc: 0.5074404776096344, val_loss: 0.5586317479610443, val_acc: 0.984375
epoch: 828: train_loss: 0.6998014122930745, train_acc: 0.5610119005044302, val_loss: 0.551661491394043, val_acc: 1.0
epoch: 829: train_loss: 0.6996996948757329, train_acc: 0.6220238010088602, val_loss: 0.5673210620880127, val_acc: 0.984375
epoch: 830: train_loss: 0.6996690127632491, train_acc: 0.5163690447807312, val_loss: 0.5525248348712921, val_acc: 1.0
epoch: 831: train_loss: 0.699667897027655, train_acc: 0.4866071442763011, val_loss: 0.5603779256343842, val_acc: 0.984375
epoch: 832: train_loss: 0.699643797805759, train_acc: 0.5193452338377634, val_loss: 0.5704455375671387, val_acc: 0.984375
epoch: 833: train_loss: 0.6996456681967358, train_acc: 0.5148809552192688, val_loss: 0.591038852930069, val_acc: 0.9375
epoch: 834: train_loss: 0.6995836432108626, train_acc: 0.5461309552192688, val_loss: 0.5655153095722198, val_acc: 0.984375
epoch: 835: train_loss: 0.6994555115081862, train_acc: 0.5892857114473978, val_loss: 0.5679864883422852, val_acc: 0.984375
epoch: 836: train_loss: 0.6994891356823696, train_acc: 0.4866071442763011, val_loss: 0.5514629185199738, val_acc: 1.0
epoch: 837: train_loss: 0.6994364525272249, train_acc: 0.5267857114473978, val_loss: 0.5515028238296509, val_acc: 1.0
epoch: 838: train_loss: 0.6994551415475629, train_acc: 0.4895833333333333, val_loss: 0.5594289004802704, val_acc: 0.984375
epoch: 839: train_loss: 0.6995074898241062, train_acc: 0.4345238109429677, val_loss: 0.5515779554843903, val_acc: 1.0
epoch: 840: train_loss: 0.699487100292944, train_acc: 0.5297619005044302, val_loss: 0.551451563835144, val_acc: 1.0
epoch: 841: train_loss: 0.6994937928489348, train_acc: 0.46875, val_loss: 0.5515664517879486, val_acc: 1.0
epoch: 842: train_loss: 0.6995404377998032, train_acc: 0.4627976218859355, val_loss: 0.5542800724506378, val_acc: 1.0
epoch: 843: train_loss: 0.6994393712253936, train_acc: 0.5758928656578064, val_loss: 0.5523707270622253, val_acc: 1.0
epoch: 844: train_loss: 0.6993649499421057, train_acc: 0.5639880895614624, val_loss: 0.5798152387142181, val_acc: 0.96875
epoch: 845: train_loss: 0.6993049835012694, train_acc: 0.523809532324473, val_loss: 0.5515393614768982, val_acc: 1.0
epoch: 846: train_loss: 0.6992934739106477, train_acc: 0.4880952338377635, val_loss: 0.5597797930240631, val_acc: 1.0
epoch: 847: train_loss: 0.6992668930745728, train_acc: 0.5357142885526022, val_loss: 0.5514539480209351, val_acc: 1.0
epoch: 848: train_loss: 0.6992002042031638, train_acc: 0.5416666666666666, val_loss: 0.5514664351940155, val_acc: 1.0
epoch: 849: train_loss: 0.6991874250009952, train_acc: 0.5, val_loss: 0.5605258643627167, val_acc: 0.984375
epoch: 850: train_loss: 0.6991800693171288, train_acc: 0.4851190447807312, val_loss: 0.5514563024044037, val_acc: 1.0
epoch: 851: train_loss: 0.6991632970659204, train_acc: 0.5208333333333334, val_loss: 0.5514651536941528, val_acc: 1.0
epoch: 852: train_loss: 0.699143019259186, train_acc: 0.5074404776096344, val_loss: 0.552031934261322, val_acc: 1.0
epoch: 853: train_loss: 0.6990922746669497, train_acc: 0.569940467675527, val_loss: 0.5514979064464569, val_acc: 1.0
epoch: 854: train_loss: 0.6991020424556551, train_acc: 0.4776785671710968, val_loss: 0.6143129169940948, val_acc: 0.9375
epoch: 855: train_loss: 0.699144631833117, train_acc: 0.4538690447807312, val_loss: 0.551648736000061, val_acc: 1.0
epoch: 856: train_loss: 0.6990857234913664, train_acc: 0.5565476218859354, val_loss: 0.5514900386333466, val_acc: 1.0
epoch: 857: train_loss: 0.699071629990324, train_acc: 0.4910714228947957, val_loss: 0.5515048801898956, val_acc: 1.0
epoch: 858: train_loss: 0.6990594485724767, train_acc: 0.5282738109429678, val_loss: 0.5514529943466187, val_acc: 1.0
epoch: 859: train_loss: 0.6990835189126262, train_acc: 0.4672619005044301, val_loss: 0.5919667184352875, val_acc: 0.9375
epoch: 860: train_loss: 0.69906671117361, train_acc: 0.5029761989911398, val_loss: 0.5634235143661499, val_acc: 0.984375
epoch: 861: train_loss: 0.6990114261914115, train_acc: 0.6026785771052042, val_loss: 0.5752263069152832, val_acc: 0.984375
epoch: 862: train_loss: 0.6989944451285316, train_acc: 0.5297619005044302, val_loss: 0.627482146024704, val_acc: 0.921875
epoch: 863: train_loss: 0.6989283394068485, train_acc: 0.5892857114473978, val_loss: 0.5517275929450989, val_acc: 1.0
epoch: 864: train_loss: 0.6988718289860416, train_acc: 0.5565476218859354, val_loss: 0.5515507757663727, val_acc: 1.0
epoch: 865: train_loss: 0.6988064357186398, train_acc: 0.5639880895614624, val_loss: 0.5514969229698181, val_acc: 1.0
epoch: 866: train_loss: 0.6988109934334575, train_acc: 0.4732142885526021, val_loss: 0.5514538586139679, val_acc: 1.0
epoch: 867: train_loss: 0.6987760490673485, train_acc: 0.5133928656578064, val_loss: 0.5514453947544098, val_acc: 1.0
epoch: 868: train_loss: 0.6987212930413381, train_acc: 0.5327380895614624, val_loss: 0.5515095293521881, val_acc: 1.0
epoch: 869: train_loss: 0.6986622522845585, train_acc: 0.5357142885526022, val_loss: 0.5927253365516663, val_acc: 0.9375
epoch: 870: train_loss: 0.6986255414222244, train_acc: 0.5133928656578064, val_loss: 0.5554909408092499, val_acc: 1.0
epoch: 871: train_loss: 0.6985740865088026, train_acc: 0.549107144276301, val_loss: 0.5530890226364136, val_acc: 1.0
epoch: 872: train_loss: 0.6985432317621981, train_acc: 0.517857144276301, val_loss: 0.5520070493221283, val_acc: 1.0
epoch: 873: train_loss: 0.6984959326565136, train_acc: 0.549107144276301, val_loss: 0.5810084342956543, val_acc: 0.96875
epoch: 874: train_loss: 0.6985298164912639, train_acc: 0.5014880895614624, val_loss: 0.6282705664634705, val_acc: 0.921875
epoch: 875: train_loss: 0.6984432184306097, train_acc: 0.5967261989911398, val_loss: 0.5719299912452698, val_acc: 0.984375
epoch: 876: train_loss: 0.6983848674493982, train_acc: 0.5788690447807312, val_loss: 0.5514610409736633, val_acc: 1.0
epoch: 877: train_loss: 0.698347641739234, train_acc: 0.5193452338377634, val_loss: 0.5641144812107086, val_acc: 1.0
epoch: 878: train_loss: 0.6983250636108731, train_acc: 0.5252976218859354, val_loss: 0.6377612352371216, val_acc: 0.875
epoch: 879: train_loss: 0.698286326647257, train_acc: 0.5476190447807312, val_loss: 0.551799088716507, val_acc: 1.0
epoch: 880: train_loss: 0.6983071120566512, train_acc: 0.4791666666666667, val_loss: 0.5515308082103729, val_acc: 1.0
epoch: 881: train_loss: 0.6983197734762306, train_acc: 0.523809532324473, val_loss: 0.5555495619773865, val_acc: 1.0
epoch: 882: train_loss: 0.698250086499233, train_acc: 0.549107144276301, val_loss: 0.5516058504581451, val_acc: 1.0
epoch: 883: train_loss: 0.6982609126632575, train_acc: 0.5014880895614624, val_loss: 0.5541545748710632, val_acc: 1.0
epoch: 884: train_loss: 0.6982062717615553, train_acc: 0.5669642885526022, val_loss: 0.5515438318252563, val_acc: 1.0
epoch: 885: train_loss: 0.6981015455193593, train_acc: 0.574404756228129, val_loss: 0.5514788627624512, val_acc: 1.0
epoch: 886: train_loss: 0.6981426063716439, train_acc: 0.4241071442763011, val_loss: 0.5518584251403809, val_acc: 1.0
epoch: 887: train_loss: 0.6981315123023577, train_acc: 0.5223214228947958, val_loss: 0.557248055934906, val_acc: 1.0
epoch: 888: train_loss: 0.6980816927526527, train_acc: 0.5446428656578064, val_loss: 0.5516768097877502, val_acc: 1.0
epoch: 889: train_loss: 0.698049199826709, train_acc: 0.543154756228129, val_loss: 0.5661582350730896, val_acc: 0.984375
epoch: 890: train_loss: 0.6980368227848083, train_acc: 0.5, val_loss: 0.5675555765628815, val_acc: 0.984375
epoch: 891: train_loss: 0.698187803005959, train_acc: 0.3095238109429677, val_loss: 0.5753980875015259, val_acc: 0.96875
epoch: 892: train_loss: 0.6980779244411172, train_acc: 0.6026785771052042, val_loss: 0.5760404169559479, val_acc: 0.96875
epoch: 893: train_loss: 0.6980149536813761, train_acc: 0.5684523781140646, val_loss: 0.5514537692070007, val_acc: 1.0
epoch: 894: train_loss: 0.6979611972634804, train_acc: 0.555059532324473, val_loss: 0.5514489114284515, val_acc: 1.0
epoch: 895: train_loss: 0.6979108513111167, train_acc: 0.5535714228947958, val_loss: 0.5514789819717407, val_acc: 1.0
epoch: 896: train_loss: 0.6978573937568473, train_acc: 0.5654761989911398, val_loss: 0.5514528751373291, val_acc: 1.0
epoch: 897: train_loss: 0.6979085265837877, train_acc: 0.4285714228947957, val_loss: 0.5515238046646118, val_acc: 1.0
epoch: 898: train_loss: 0.6978238887551724, train_acc: 0.586309532324473, val_loss: 0.5530204176902771, val_acc: 1.0
epoch: 899: train_loss: 0.6977712265650438, train_acc: 0.5357142885526022, val_loss: 0.5514679849147797, val_acc: 1.0
epoch: 900: train_loss: 0.6977509289285325, train_acc: 0.5044642885526022, val_loss: 0.5514543950557709, val_acc: 1.0
epoch: 901: train_loss: 0.6976868575445746, train_acc: 0.5654761989911398, val_loss: 0.5775000154972076, val_acc: 0.96875
epoch: 902: train_loss: 0.6975931274842501, train_acc: 0.5877976218859354, val_loss: 0.5869631469249725, val_acc: 0.96875
epoch: 903: train_loss: 0.6975731183953346, train_acc: 0.5163690447807312, val_loss: 0.5670666396617889, val_acc: 0.984375
epoch: 904: train_loss: 0.6975889467402722, train_acc: 0.4523809552192688, val_loss: 0.5562926530838013, val_acc: 1.0
epoch: 905: train_loss: 0.6975588961240565, train_acc: 0.5401785671710968, val_loss: 0.5602999925613403, val_acc: 0.984375
epoch: 906: train_loss: 0.6975387741826297, train_acc: 0.5446428656578064, val_loss: 0.5514454543590546, val_acc: 1.0
epoch: 907: train_loss: 0.6974695404983307, train_acc: 0.605654756228129, val_loss: 0.5615380704402924, val_acc: 0.984375
epoch: 908: train_loss: 0.6973708632187312, train_acc: 0.605654756228129, val_loss: 0.5516279935836792, val_acc: 1.0
epoch: 909: train_loss: 0.6973271317752732, train_acc: 0.5461309552192688, val_loss: 0.5526870489120483, val_acc: 1.0
epoch: 910: train_loss: 0.697352254735303, train_acc: 0.4821428557236989, val_loss: 0.5651662051677704, val_acc: 0.984375
epoch: 911: train_loss: 0.6973287703916, train_acc: 0.5044642885526022, val_loss: 0.5560044348239899, val_acc: 1.0
epoch: 912: train_loss: 0.6973216408532487, train_acc: 0.5059523781140646, val_loss: 0.560857504606247, val_acc: 0.984375
epoch: 913: train_loss: 0.6972879134200078, train_acc: 0.5372023781140646, val_loss: 0.5525447726249695, val_acc: 1.0
epoch: 914: train_loss: 0.6972375987657438, train_acc: 0.53125, val_loss: 0.5540957748889923, val_acc: 1.0
epoch: 915: train_loss: 0.6971858135130385, train_acc: 0.543154756228129, val_loss: 0.5672995746135712, val_acc: 0.984375
epoch: 916: train_loss: 0.6971710370870475, train_acc: 0.5446428656578064, val_loss: 0.5799687802791595, val_acc: 0.984375
epoch: 917: train_loss: 0.6971891763293411, train_acc: 0.46875, val_loss: 0.5517111420631409, val_acc: 1.0
epoch: 918: train_loss: 0.6971809545158264, train_acc: 0.4761904776096344, val_loss: 0.551462709903717, val_acc: 1.0
epoch: 919: train_loss: 0.6971910996929463, train_acc: 0.46875, val_loss: 0.5514477789402008, val_acc: 1.0
epoch: 920: train_loss: 0.6971480835834539, train_acc: 0.5357142885526022, val_loss: 0.5514476299285889, val_acc: 1.0
epoch: 921: train_loss: 0.6971509598211719, train_acc: 0.4642857114473979, val_loss: 0.563047468662262, val_acc: 0.984375
epoch: 922: train_loss: 0.6971288557601952, train_acc: 0.5223214228947958, val_loss: 0.5514460206031799, val_acc: 1.0
epoch: 923: train_loss: 0.6971690661850435, train_acc: 0.4464285671710968, val_loss: 0.5527949631214142, val_acc: 1.0
epoch: 924: train_loss: 0.6971718737885764, train_acc: 0.4866071442763011, val_loss: 0.5514622032642365, val_acc: 1.0
epoch: 925: train_loss: 0.6971662061689915, train_acc: 0.5104166666666666, val_loss: 0.6043185889720917, val_acc: 0.9375
epoch: 926: train_loss: 0.6971772786140104, train_acc: 0.4895833333333333, val_loss: 0.6139585971832275, val_acc: 0.9375
epoch: 927: train_loss: 0.6972118898498263, train_acc: 0.4895833333333333, val_loss: 0.5658279359340668, val_acc: 0.984375
epoch: 928: train_loss: 0.6971500127769779, train_acc: 0.5877976218859354, val_loss: 0.5514996945858002, val_acc: 1.0
epoch: 929: train_loss: 0.6971514684325055, train_acc: 0.4985119005044301, val_loss: 0.5516404211521149, val_acc: 1.0
epoch: 930: train_loss: 0.6971488470805836, train_acc: 0.4940476218859355, val_loss: 0.5526898503303528, val_acc: 1.0
epoch: 931: train_loss: 0.6971160076846386, train_acc: 0.5476190447807312, val_loss: 0.5666908025741577, val_acc: 0.984375
epoch: 932: train_loss: 0.6970136024859093, train_acc: 0.5892857114473978, val_loss: 0.5879866778850555, val_acc: 0.96875
epoch: 933: train_loss: 0.6969851137356283, train_acc: 0.555059532324473, val_loss: 0.55185467004776, val_acc: 1.0
epoch: 934: train_loss: 0.6969389894216885, train_acc: 0.549107144276301, val_loss: 0.5629992187023163, val_acc: 0.984375
epoch: 935: train_loss: 0.6969221858811858, train_acc: 0.5193452338377634, val_loss: 0.5660178661346436, val_acc: 0.984375
epoch: 936: train_loss: 0.6968706413250049, train_acc: 0.5342261989911398, val_loss: 0.552062064409256, val_acc: 1.0
epoch: 937: train_loss: 0.696867803753736, train_acc: 0.5104166666666666, val_loss: 0.5514450371265411, val_acc: 1.0
epoch: 938: train_loss: 0.696897143309617, train_acc: 0.4583333333333333, val_loss: 0.551458090543747, val_acc: 1.0
epoch: 939: train_loss: 0.6969005741548882, train_acc: 0.5416666666666666, val_loss: 0.5620498061180115, val_acc: 1.0
epoch: 940: train_loss: 0.6968077534147924, train_acc: 0.605654756228129, val_loss: 0.5669665038585663, val_acc: 0.984375
epoch: 941: train_loss: 0.6966927097523716, train_acc: 0.6309523781140646, val_loss: 0.573598325252533, val_acc: 0.984375
epoch: 942: train_loss: 0.6966972299665481, train_acc: 0.5133928656578064, val_loss: 0.5946772694587708, val_acc: 0.96875
epoch: 943: train_loss: 0.6966515655739839, train_acc: 0.5193452338377634, val_loss: 0.6437261700630188, val_acc: 0.90625
epoch: 944: train_loss: 0.6966024930304746, train_acc: 0.5520833333333334, val_loss: 0.5514539182186127, val_acc: 1.0
epoch: 945: train_loss: 0.6965263619483405, train_acc: 0.574404756228129, val_loss: 0.5620432794094086, val_acc: 0.984375
epoch: 946: train_loss: 0.6964672185383568, train_acc: 0.5877976218859354, val_loss: 0.5516960620880127, val_acc: 1.0
epoch: 947: train_loss: 0.6965133328352302, train_acc: 0.4583333333333333, val_loss: 0.5525453388690948, val_acc: 1.0
epoch: 948: train_loss: 0.6964589068669378, train_acc: 0.5848214228947958, val_loss: 0.5515307188034058, val_acc: 1.0
epoch: 949: train_loss: 0.6964557533724273, train_acc: 0.4821428656578064, val_loss: 0.628868967294693, val_acc: 0.921875
epoch: 950: train_loss: 0.6963947742519324, train_acc: 0.5565476218859354, val_loss: 0.682749330997467, val_acc: 0.859375
epoch: 951: train_loss: 0.6963628761115535, train_acc: 0.5282738109429678, val_loss: 0.5519947707653046, val_acc: 1.0
epoch: 952: train_loss: 0.6962575072923636, train_acc: 0.6071428656578064, val_loss: 0.5514614880084991, val_acc: 1.0
epoch: 953: train_loss: 0.6962826188255413, train_acc: 0.4523809552192688, val_loss: 0.5514574348926544, val_acc: 1.0
epoch: 954: train_loss: 0.6962838982204291, train_acc: 0.4672619005044301, val_loss: 0.5516227781772614, val_acc: 1.0
epoch: 955: train_loss: 0.6963160101381479, train_acc: 0.4866071442763011, val_loss: 0.5514600574970245, val_acc: 1.0
epoch: 956: train_loss: 0.6963051321073849, train_acc: 0.5625, val_loss: 0.5514451861381531, val_acc: 1.0
epoch: 957: train_loss: 0.6963130344967925, train_acc: 0.4821428656578064, val_loss: 0.5745377540588379, val_acc: 0.984375
epoch: 958: train_loss: 0.6963334980009335, train_acc: 0.4955357114473979, val_loss: 0.5839695334434509, val_acc: 0.96875
epoch: 959: train_loss: 0.6962359826597909, train_acc: 0.613095243771871, val_loss: 0.5776673555374146, val_acc: 0.96875
epoch: 960: train_loss: 0.6962991308471297, train_acc: 0.4389880994955699, val_loss: 0.5862403810024261, val_acc: 0.9375
epoch: 961: train_loss: 0.6962963825741004, train_acc: 0.5059523781140646, val_loss: 0.5682167112827301, val_acc: 0.984375
epoch: 962: train_loss: 0.696321443997275, train_acc: 0.4895833333333333, val_loss: 0.5547386109828949, val_acc: 1.0
epoch: 963: train_loss: 0.6964015480682239, train_acc: 0.4092261890570323, val_loss: 0.565407395362854, val_acc: 0.96875
epoch: 964: train_loss: 0.6964047199909762, train_acc: 0.5252976218859354, val_loss: 0.5601735413074493, val_acc: 0.984375
epoch: 965: train_loss: 0.6963658706947724, train_acc: 0.5342261989911398, val_loss: 0.5994636416435242, val_acc: 0.953125
epoch: 966: train_loss: 0.6963550739763364, train_acc: 0.5223214228947958, val_loss: 0.6176598370075226, val_acc: 0.921875
epoch: 967: train_loss: 0.6964063748060513, train_acc: 0.5252976218859354, val_loss: 0.572500467300415, val_acc: 0.984375
epoch: 968: train_loss: 0.696299649736584, train_acc: 0.5997023781140646, val_loss: 0.5646764636039734, val_acc: 0.984375
epoch: 969: train_loss: 0.696245710382757, train_acc: 0.5669642885526022, val_loss: 0.5517771244049072, val_acc: 1.0
epoch: 970: train_loss: 0.6963292528090922, train_acc: 0.4136904776096344, val_loss: 0.6480973064899445, val_acc: 0.90625
epoch: 971: train_loss: 0.6965168800578038, train_acc: 0.4017857114473979, val_loss: 0.5519586801528931, val_acc: 1.0
epoch: 972: train_loss: 0.6964902126572652, train_acc: 0.53125, val_loss: 0.5710763335227966, val_acc: 0.984375
epoch: 973: train_loss: 0.6964847240686258, train_acc: 0.5282738109429678, val_loss: 0.6375525891780853, val_acc: 0.90625
epoch: 974: train_loss: 0.6964292354257703, train_acc: 0.574404756228129, val_loss: 0.6884879767894745, val_acc: 0.859375
epoch: 975: train_loss: 0.6963908659711566, train_acc: 0.5416666666666666, val_loss: 0.5930228233337402, val_acc: 0.984375
epoch: 976: train_loss: 0.6963782159999877, train_acc: 0.5520833333333334, val_loss: 0.5678439736366272, val_acc: 0.984375
epoch: 977: train_loss: 0.6963703079173257, train_acc: 0.5104166666666666, val_loss: 0.5533314347267151, val_acc: 1.0
epoch: 978: train_loss: 0.6963660138011348, train_acc: 0.5297619005044302, val_loss: 0.6154075562953949, val_acc: 0.953125
epoch: 979: train_loss: 0.6963959352297042, train_acc: 0.4553571442763011, val_loss: 0.5514691770076752, val_acc: 1.0
epoch: 980: train_loss: 0.6963801241390896, train_acc: 0.5074404776096344, val_loss: 0.5523283779621124, val_acc: 1.0
epoch: 981: train_loss: 0.6963647282698171, train_acc: 0.5089285671710968, val_loss: 0.5520503222942352, val_acc: 1.0
epoch: 982: train_loss: 0.6962996773414191, train_acc: 0.5446428656578064, val_loss: 0.551464170217514, val_acc: 1.0
epoch: 983: train_loss: 0.6963463624924184, train_acc: 0.4613095323244731, val_loss: 0.6047200858592987, val_acc: 0.9375
epoch: 984: train_loss: 0.6963369438490897, train_acc: 0.5, val_loss: 0.5668087601661682, val_acc: 0.984375
epoch: 985: train_loss: 0.6962548461090968, train_acc: 0.5848214228947958, val_loss: 0.5920555591583252, val_acc: 0.9375
epoch: 986: train_loss: 0.6962221327845777, train_acc: 0.5267857114473978, val_loss: 0.5531464517116547, val_acc: 1.0
epoch: 987: train_loss: 0.696222235497676, train_acc: 0.4806547562281291, val_loss: 0.5630008280277252, val_acc: 1.0
epoch: 988: train_loss: 0.696153436781463, train_acc: 0.5580357114473978, val_loss: 0.5515986680984497, val_acc: 1.0
epoch: 989: train_loss: 0.696184196957836, train_acc: 0.4880952338377635, val_loss: 0.5549927055835724, val_acc: 1.0
epoch: 990: train_loss: 0.696174266500342, train_acc: 0.5029761989911398, val_loss: 0.5536907315254211, val_acc: 1.0
epoch: 991: train_loss: 0.696127622398318, train_acc: 0.5535714228947958, val_loss: 0.551690012216568, val_acc: 1.0
epoch: 992: train_loss: 0.6960747673552328, train_acc: 0.5401785671710968, val_loss: 0.5639951527118683, val_acc: 0.984375
epoch: 993: train_loss: 0.6960373203039655, train_acc: 0.5223214228947958, val_loss: 0.5514744222164154, val_acc: 1.0
epoch: 994: train_loss: 0.6960599655082683, train_acc: 0.4538690447807312, val_loss: 0.5544222891330719, val_acc: 1.0
epoch: 995: train_loss: 0.6960572777303513, train_acc: 0.4925595323244731, val_loss: 0.5514799356460571, val_acc: 1.0
epoch: 996: train_loss: 0.6961033606521271, train_acc: 0.4375, val_loss: 0.5516543388366699, val_acc: 1.0
epoch: 997: train_loss: 0.6960583494835568, train_acc: 0.5625, val_loss: 0.5545304417610168, val_acc: 1.0
epoch: 998: train_loss: 0.6960096058345937, train_acc: 0.5669642885526022, val_loss: 0.5514552295207977, val_acc: 1.0
epoch: 999: train_loss: 0.6960236104528116, train_acc: 0.4523809552192688, val_loss: 0.5514908730983734, val_acc: 1.0
1.2546268701553345 0.296875
Accuracy of the network on the test images: 33%
F1 Score: 0.22602319616371136
precision_score: 0.17053571428571426
recall_total: 0.3392857142857143
Accuracy for class: Alligator Cracks is 40.0 %
F1 score: 0.027777777777777776
precision_score: 0.015625
recall_score: 0.125
Accuracy for class: Longitudinal Cracks is 81.0 %
F1 score: 0.2535087719298245
precision_score: 0.18062499999999998
recall_score: 0.425
Accuracy for class: Transverse Cracks is 0.0 %
F1 score: 0.0
precision_score: 0.0
recall_score: 0.0
