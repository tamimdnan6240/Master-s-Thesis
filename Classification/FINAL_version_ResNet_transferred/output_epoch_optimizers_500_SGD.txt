cpu
Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Alligator Cracks Alligator Cracks Alligator Cracks Alligator Cracks Longitudinal Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.1969212293624878, train_acc: 0.2589285721381505, val_loss: 1.1093120574951172, val_acc: 0.359375
epoch: 1: train_loss: 1.158078153928121, train_acc: 0.2633928557236989, val_loss: 1.0988778471946716, val_acc: 0.40625
epoch: 2: train_loss: 1.1549698114395142, train_acc: 0.2916666666666667, val_loss: 1.0987258553504944, val_acc: 0.3125
epoch: 3: train_loss: 1.146575133005778, train_acc: 0.2827380994955699, val_loss: 1.1059226393699646, val_acc: 0.265625
epoch: 4: train_loss: 1.1534622192382813, train_acc: 0.21726190547148386, val_loss: 1.0979891419410706, val_acc: 0.40625
epoch: 5: train_loss: 1.153206679556105, train_acc: 0.2782738109429677, val_loss: 1.1031534075737, val_acc: 0.359375
epoch: 6: train_loss: 1.1565920738946822, train_acc: 0.2931547661622365, val_loss: 1.1041699051856995, val_acc: 0.359375
epoch: 7: train_loss: 1.1508398403724034, train_acc: 0.2752976218859355, val_loss: 1.0983991026878357, val_acc: 0.453125
epoch: 8: train_loss: 1.1459178218135126, train_acc: 0.4270833333333333, val_loss: 1.102035403251648, val_acc: 0.40625
epoch: 9: train_loss: 1.149182681242625, train_acc: 0.2425595223903656, val_loss: 1.108267068862915, val_acc: 0.203125
epoch: 10: train_loss: 1.1531861442508118, train_acc: 0.2842261890570323, val_loss: 1.1054081320762634, val_acc: 0.265625
epoch: 11: train_loss: 1.156990905602773, train_acc: 0.2544642885526021, val_loss: 1.1019471287727356, val_acc: 0.328125
epoch: 12: train_loss: 1.1535853025240774, train_acc: 0.3735119054714839, val_loss: 1.1007599234580994, val_acc: 0.28125
epoch: 13: train_loss: 1.1542818461145674, train_acc: 0.3288690447807312, val_loss: 1.1036061644554138, val_acc: 0.265625
epoch: 14: train_loss: 1.1523904297086927, train_acc: 0.3898809552192688, val_loss: 1.1055676937103271, val_acc: 0.328125
epoch: 15: train_loss: 1.1529849270979562, train_acc: 0.3452380994955699, val_loss: 1.1005527973175049, val_acc: 0.296875
epoch: 16: train_loss: 1.150622231118819, train_acc: 0.3526785671710968, val_loss: 1.0998803973197937, val_acc: 0.3125
epoch: 17: train_loss: 1.1483018806687104, train_acc: 0.4345238109429677, val_loss: 1.0933840870857239, val_acc: 0.40625
epoch: 18: train_loss: 1.1455121364509848, train_acc: 0.4375, val_loss: 1.0969961881637573, val_acc: 0.328125
epoch: 19: train_loss: 1.142837944626808, train_acc: 0.4241071442763011, val_loss: 1.097994327545166, val_acc: 0.390625
epoch: 20: train_loss: 1.1411022979115681, train_acc: 0.3958333333333333, val_loss: 1.0978165864944458, val_acc: 0.328125
epoch: 21: train_loss: 1.141203314065933, train_acc: 0.3363095223903656, val_loss: 1.0936710238456726, val_acc: 0.375
epoch: 22: train_loss: 1.1392731917077215, train_acc: 0.3824404776096344, val_loss: 1.095250129699707, val_acc: 0.390625
epoch: 23: train_loss: 1.1398485195305612, train_acc: 0.3571428557236989, val_loss: 1.0887060165405273, val_acc: 0.4375
epoch: 24: train_loss: 1.138739910920461, train_acc: 0.4360119005044301, val_loss: 1.1006407141685486, val_acc: 0.34375
epoch: 25: train_loss: 1.1377341067179656, train_acc: 0.3690476218859355, val_loss: 1.0905115604400635, val_acc: 0.375
epoch: 26: train_loss: 1.1376994708437977, train_acc: 0.3273809552192688, val_loss: 1.0885764956474304, val_acc: 0.546875
epoch: 27: train_loss: 1.1369294517097017, train_acc: 0.3511904776096344, val_loss: 1.090656816959381, val_acc: 0.484375
epoch: 28: train_loss: 1.135608302450728, train_acc: 0.3497023830811183, val_loss: 1.0918256640434265, val_acc: 0.375
epoch: 29: train_loss: 1.1345542702409956, train_acc: 0.3854166666666667, val_loss: 1.077855408191681, val_acc: 0.46875
epoch: 30: train_loss: 1.1333760426890465, train_acc: 0.3958333333333333, val_loss: 1.0780450105667114, val_acc: 0.546875
epoch: 31: train_loss: 1.1337150329103072, train_acc: 0.3377976218859355, val_loss: 1.0766476392745972, val_acc: 0.453125
epoch: 32: train_loss: 1.134621424506409, train_acc: 0.2872023781140645, val_loss: 1.085332214832306, val_acc: 0.453125
epoch: 33: train_loss: 1.1340589049984426, train_acc: 0.3720238109429677, val_loss: 1.0856584310531616, val_acc: 0.359375
epoch: 34: train_loss: 1.1328809936841329, train_acc: 0.4107142885526021, val_loss: 1.091671645641327, val_acc: 0.359375
epoch: 35: train_loss: 1.133183799959995, train_acc: 0.4077380895614624, val_loss: 1.0859624743461609, val_acc: 0.40625
epoch: 36: train_loss: 1.1332724325291743, train_acc: 0.3407738109429677, val_loss: 1.0601397156715393, val_acc: 0.546875
epoch: 37: train_loss: 1.1316851194490465, train_acc: 0.4553571442763011, val_loss: 1.082338809967041, val_acc: 0.40625
epoch: 38: train_loss: 1.1316990918583338, train_acc: 0.375, val_loss: 1.0933935046195984, val_acc: 0.359375
epoch: 39: train_loss: 1.1313934748371441, train_acc: 0.4211309552192688, val_loss: 1.0800607204437256, val_acc: 0.40625
epoch: 40: train_loss: 1.1302152363265432, train_acc: 0.3630952388048172, val_loss: 1.086643397808075, val_acc: 0.359375
epoch: 41: train_loss: 1.1304498991322893, train_acc: 0.3705357114473979, val_loss: 1.0829198360443115, val_acc: 0.40625
epoch: 42: train_loss: 1.1300785093344456, train_acc: 0.4241071442763011, val_loss: 1.0750736594200134, val_acc: 0.453125
epoch: 43: train_loss: 1.1296102950970328, train_acc: 0.4330357114473979, val_loss: 1.085934579372406, val_acc: 0.359375
epoch: 44: train_loss: 1.1279128564728629, train_acc: 0.4226190447807312, val_loss: 1.0666345953941345, val_acc: 0.453125
epoch: 45: train_loss: 1.1275034913982167, train_acc: 0.3690476218859355, val_loss: 1.0535894632339478, val_acc: 0.546875
epoch: 46: train_loss: 1.1272068416818657, train_acc: 0.3943452388048172, val_loss: 1.069929838180542, val_acc: 0.453125
epoch: 47: train_loss: 1.1269118309848836, train_acc: 0.3363095223903656, val_loss: 1.0774677395820618, val_acc: 0.453125
epoch: 48: train_loss: 1.1265836009362924, train_acc: 0.3928571442763011, val_loss: 1.0726698637008667, val_acc: 0.453125
epoch: 49: train_loss: 1.125802812973658, train_acc: 0.3616071442763011, val_loss: 1.0805659294128418, val_acc: 0.40625
epoch: 50: train_loss: 1.125772094025331, train_acc: 0.3616071442763011, val_loss: 1.0713773369789124, val_acc: 0.453125
epoch: 51: train_loss: 1.1253040772982128, train_acc: 0.3883928557236989, val_loss: 1.0705618858337402, val_acc: 0.40625
epoch: 52: train_loss: 1.1253903590658172, train_acc: 0.3452380994955699, val_loss: 1.090293049812317, val_acc: 0.3125
epoch: 53: train_loss: 1.1246064235398794, train_acc: 0.3898809552192688, val_loss: 1.050081729888916, val_acc: 0.5
epoch: 54: train_loss: 1.125259826761303, train_acc: 0.3154761890570323, val_loss: 1.0709800720214844, val_acc: 0.40625
epoch: 55: train_loss: 1.124731155023688, train_acc: 0.4375, val_loss: 1.078326165676117, val_acc: 0.359375
epoch: 56: train_loss: 1.1245438951497884, train_acc: 0.3705357114473979, val_loss: 1.0642419457435608, val_acc: 0.453125
epoch: 57: train_loss: 1.1242305722044799, train_acc: 0.3244047661622365, val_loss: 1.0780094265937805, val_acc: 0.40625
epoch: 58: train_loss: 1.1247059204484107, train_acc: 0.2648809552192688, val_loss: 1.0652872323989868, val_acc: 0.40625
epoch: 59: train_loss: 1.1244365079535374, train_acc: 0.3869047661622365, val_loss: 1.0628020763397217, val_acc: 0.453125
epoch: 60: train_loss: 1.1250800646719383, train_acc: 0.3005952388048172, val_loss: 1.0654502511024475, val_acc: 0.453125
epoch: 61: train_loss: 1.1240721787175822, train_acc: 0.4211309552192688, val_loss: 1.048781156539917, val_acc: 0.5
epoch: 62: train_loss: 1.1237344748128657, train_acc: 0.3809523781140645, val_loss: 1.084189534187317, val_acc: 0.359375
epoch: 63: train_loss: 1.1239316351711748, train_acc: 0.3154761890570323, val_loss: 1.0395736992359161, val_acc: 0.546875
epoch: 64: train_loss: 1.123099250059861, train_acc: 0.3854166666666667, val_loss: 1.0616695880889893, val_acc: 0.453125
epoch: 65: train_loss: 1.122623405673287, train_acc: 0.4226190447807312, val_loss: 1.0463725328445435, val_acc: 0.5
epoch: 66: train_loss: 1.1226967804467498, train_acc: 0.3482142885526021, val_loss: 1.0882933735847473, val_acc: 0.3125
epoch: 67: train_loss: 1.1224255929974947, train_acc: 0.34375, val_loss: 1.0713865160942078, val_acc: 0.40625
epoch: 68: train_loss: 1.1224160505377725, train_acc: 0.2827380994955699, val_loss: 1.0521103143692017, val_acc: 0.453125
epoch: 69: train_loss: 1.1219660174278983, train_acc: 0.3764880994955699, val_loss: 1.0944654941558838, val_acc: 0.3125
epoch: 70: train_loss: 1.1219198474301975, train_acc: 0.3497023781140645, val_loss: 1.0754989385604858, val_acc: 0.40625
epoch: 71: train_loss: 1.1218912882937322, train_acc: 0.3482142885526021, val_loss: 1.081645131111145, val_acc: 0.359375
epoch: 72: train_loss: 1.121744307208823, train_acc: 0.3467261890570323, val_loss: 1.0925869941711426, val_acc: 0.3125
epoch: 73: train_loss: 1.1211013606002733, train_acc: 0.3392857114473979, val_loss: 1.0750736594200134, val_acc: 0.40625
epoch: 74: train_loss: 1.1208896928363372, train_acc: 0.3288690447807312, val_loss: 1.0603479146957397, val_acc: 0.453125
epoch: 75: train_loss: 1.120449887033094, train_acc: 0.3497023781140645, val_loss: 1.0585222244262695, val_acc: 0.453125
epoch: 76: train_loss: 1.1196348863246632, train_acc: 0.4538690447807312, val_loss: 1.0648205280303955, val_acc: 0.40625
epoch: 77: train_loss: 1.1192509582918928, train_acc: 0.3645833333333333, val_loss: 1.0640864372253418, val_acc: 0.40625
epoch: 78: train_loss: 1.1192332868334607, train_acc: 0.3616071442763011, val_loss: 1.0656334161758423, val_acc: 0.40625
epoch: 79: train_loss: 1.1191367099682483, train_acc: 0.3154761890570323, val_loss: 1.0619794130325317, val_acc: 0.40625
epoch: 80: train_loss: 1.1187229941411267, train_acc: 0.3794642885526021, val_loss: 1.0513731837272644, val_acc: 0.5
epoch: 81: train_loss: 1.118212667907156, train_acc: 0.3898809552192688, val_loss: 1.060301423072815, val_acc: 0.453125
epoch: 82: train_loss: 1.117561533747906, train_acc: 0.3988095223903656, val_loss: 1.0691655278205872, val_acc: 0.40625
epoch: 83: train_loss: 1.1175451430063392, train_acc: 0.3348214328289032, val_loss: 1.0619422793388367, val_acc: 0.40625
epoch: 84: train_loss: 1.117202729337355, train_acc: 0.4107142885526021, val_loss: 1.0356762111186981, val_acc: 0.546875
epoch: 85: train_loss: 1.1173394682795497, train_acc: 0.3452380994955699, val_loss: 1.078533113002777, val_acc: 0.359375
epoch: 86: train_loss: 1.1170278189282759, train_acc: 0.4077380895614624, val_loss: 1.0428211688995361, val_acc: 0.5
epoch: 87: train_loss: 1.1164061984781057, train_acc: 0.4583333333333333, val_loss: 1.0633673071861267, val_acc: 0.40625
epoch: 88: train_loss: 1.1161885906694529, train_acc: 0.3779761890570323, val_loss: 1.0456681847572327, val_acc: 0.5
epoch: 89: train_loss: 1.1165929430060912, train_acc: 0.2738095223903656, val_loss: 1.055918037891388, val_acc: 0.453125
epoch: 90: train_loss: 1.1164854531323074, train_acc: 0.34375, val_loss: 1.0722774863243103, val_acc: 0.359375
epoch: 91: train_loss: 1.1161101592191747, train_acc: 0.3869047562281291, val_loss: 1.0922019481658936, val_acc: 0.3125
epoch: 92: train_loss: 1.1157556137730993, train_acc: 0.3705357114473979, val_loss: 1.0760965943336487, val_acc: 0.359375
epoch: 93: train_loss: 1.115654421402207, train_acc: 0.3660714328289032, val_loss: 1.0757033228874207, val_acc: 0.359375
epoch: 94: train_loss: 1.1154816888926318, train_acc: 0.3288690447807312, val_loss: 1.0634686350822449, val_acc: 0.40625
epoch: 95: train_loss: 1.1151156808353129, train_acc: 0.3482142885526021, val_loss: 1.0630977749824524, val_acc: 0.40625
epoch: 96: train_loss: 1.1150121518836396, train_acc: 0.3318452338377635, val_loss: 1.0806421637535095, val_acc: 0.3125
epoch: 97: train_loss: 1.1137597978520553, train_acc: 0.5104166666666666, val_loss: 1.0841692090034485, val_acc: 0.3125
epoch: 98: train_loss: 1.1138768360670965, train_acc: 0.3333333333333333, val_loss: 1.0441021919250488, val_acc: 0.453125
epoch: 99: train_loss: 1.1128077288468676, train_acc: 0.4910714228947957, val_loss: 1.0459766387939453, val_acc: 0.453125
epoch: 100: train_loss: 1.1123043538713608, train_acc: 0.4241071442763011, val_loss: 1.048060953617096, val_acc: 0.453125
epoch: 101: train_loss: 1.111905108869465, train_acc: 0.3273809552192688, val_loss: 1.0192395746707916, val_acc: 0.546875
epoch: 102: train_loss: 1.1113167881579844, train_acc: 0.4047619005044301, val_loss: 1.0555260181427002, val_acc: 0.40625
epoch: 103: train_loss: 1.1109470736521938, train_acc: 0.3913690447807312, val_loss: 1.0893634557724, val_acc: 0.3125
epoch: 104: train_loss: 1.1111063574987743, train_acc: 0.2931547661622365, val_loss: 1.0544193387031555, val_acc: 0.40625
epoch: 105: train_loss: 1.111254126770691, train_acc: 0.3288690447807312, val_loss: 1.0710955262184143, val_acc: 0.359375
epoch: 106: train_loss: 1.1108488561208372, train_acc: 0.3824404776096344, val_loss: 1.037356674671173, val_acc: 0.5
epoch: 107: train_loss: 1.110388476355576, train_acc: 0.4122023781140645, val_loss: 1.0432382822036743, val_acc: 0.453125
epoch: 108: train_loss: 1.1107557718542371, train_acc: 0.3020833333333333, val_loss: 1.0766906142234802, val_acc: 0.359375
epoch: 109: train_loss: 1.1102504685069574, train_acc: 0.4002976218859355, val_loss: 1.041714608669281, val_acc: 0.453125
epoch: 110: train_loss: 1.1100387347711096, train_acc: 0.3675595223903656, val_loss: 1.064604938030243, val_acc: 0.359375
epoch: 111: train_loss: 1.1097927182203244, train_acc: 0.3377976218859355, val_loss: 1.0678221583366394, val_acc: 0.359375
epoch: 112: train_loss: 1.1098800401772015, train_acc: 0.3244047661622365, val_loss: 1.0102885365486145, val_acc: 0.546875
epoch: 113: train_loss: 1.109360628483588, train_acc: 0.4136904776096344, val_loss: 1.0314111113548279, val_acc: 0.5
epoch: 114: train_loss: 1.1091081617535021, train_acc: 0.3943452338377635, val_loss: 1.0683913826942444, val_acc: 0.359375
epoch: 115: train_loss: 1.1088996435376417, train_acc: 0.3422619054714839, val_loss: 1.0558961033821106, val_acc: 0.40625
epoch: 116: train_loss: 1.1086924407217236, train_acc: 0.3973214328289032, val_loss: 1.095356822013855, val_acc: 0.265625
epoch: 117: train_loss: 1.1082004559578866, train_acc: 0.4122023781140645, val_loss: 1.0264906287193298, val_acc: 0.5
epoch: 118: train_loss: 1.1077844206692453, train_acc: 0.4449404776096344, val_loss: 1.0458702445030212, val_acc: 0.40625
epoch: 119: train_loss: 1.1078502120243174, train_acc: 0.3601190447807312, val_loss: 1.0631065964698792, val_acc: 0.40625
epoch: 120: train_loss: 1.1080794779393952, train_acc: 0.28125, val_loss: 1.0433757305145264, val_acc: 0.453125
epoch: 121: train_loss: 1.1081269515342396, train_acc: 0.3675595223903656, val_loss: 1.0347435474395752, val_acc: 0.453125
epoch: 122: train_loss: 1.1078440269157488, train_acc: 0.3779761890570323, val_loss: 1.068330466747284, val_acc: 0.359375
epoch: 123: train_loss: 1.1074091623867708, train_acc: 0.4211309552192688, val_loss: 1.029726266860962, val_acc: 0.5
epoch: 124: train_loss: 1.1071246056556696, train_acc: 0.3526785671710968, val_loss: 1.0504247546195984, val_acc: 0.40625
epoch: 125: train_loss: 1.1069098116544185, train_acc: 0.3809523781140645, val_loss: 1.0420174598693848, val_acc: 0.453125
epoch: 126: train_loss: 1.1062691244553389, train_acc: 0.4241071442763011, val_loss: 1.0387938618659973, val_acc: 0.453125
epoch: 127: train_loss: 1.1059972139385834, train_acc: 0.3630952338377635, val_loss: 1.0386568903923035, val_acc: 0.453125
epoch: 128: train_loss: 1.105191512163295, train_acc: 0.4375, val_loss: 1.0612669587135315, val_acc: 0.359375
epoch: 129: train_loss: 1.1048892473563166, train_acc: 0.3958333333333333, val_loss: 1.0309259295463562, val_acc: 0.5
epoch: 130: train_loss: 1.1047225576320674, train_acc: 0.3526785671710968, val_loss: 1.028882384300232, val_acc: 0.5
epoch: 131: train_loss: 1.1039958591714045, train_acc: 0.4627976218859355, val_loss: 1.0356634259223938, val_acc: 0.453125
epoch: 132: train_loss: 1.1038429645966163, train_acc: 0.3511904776096344, val_loss: 1.0423368215560913, val_acc: 0.453125
epoch: 133: train_loss: 1.1036680701063635, train_acc: 0.3482142885526021, val_loss: 1.0453843474388123, val_acc: 0.453125
epoch: 134: train_loss: 1.1036610136797396, train_acc: 0.2961309552192688, val_loss: 1.0466687679290771, val_acc: 0.40625
epoch: 135: train_loss: 1.1039519104010915, train_acc: 0.3318452338377635, val_loss: 1.0527244806289673, val_acc: 0.40625
epoch: 136: train_loss: 1.104095672313894, train_acc: 0.2797619054714839, val_loss: 1.0637854933738708, val_acc: 0.359375
epoch: 137: train_loss: 1.1040965058089451, train_acc: 0.3184523781140645, val_loss: 1.0492367148399353, val_acc: 0.40625
epoch: 138: train_loss: 1.103969444616807, train_acc: 0.3363095223903656, val_loss: 1.0623725652694702, val_acc: 0.359375
epoch: 139: train_loss: 1.1034996877113974, train_acc: 0.4375, val_loss: 1.0528187155723572, val_acc: 0.40625
epoch: 140: train_loss: 1.103021210530689, train_acc: 0.4925595323244731, val_loss: 1.067559540271759, val_acc: 0.359375
epoch: 141: train_loss: 1.102951559662259, train_acc: 0.34375, val_loss: 1.0499533414840698, val_acc: 0.40625
epoch: 142: train_loss: 1.1028422495146173, train_acc: 0.3571428557236989, val_loss: 1.0259414315223694, val_acc: 0.453125
epoch: 143: train_loss: 1.102596634240062, train_acc: 0.3824404776096344, val_loss: 1.0354912281036377, val_acc: 0.453125
epoch: 144: train_loss: 1.102755045068675, train_acc: 0.3050595223903656, val_loss: 1.0131148099899292, val_acc: 0.546875
epoch: 145: train_loss: 1.1026164089163686, train_acc: 0.3586309552192688, val_loss: 1.0591676235198975, val_acc: 0.359375
epoch: 146: train_loss: 1.1025062075007257, train_acc: 0.3482142885526021, val_loss: 1.0609488487243652, val_acc: 0.359375
epoch: 147: train_loss: 1.102384411268406, train_acc: 0.3363095223903656, val_loss: 1.0562529563903809, val_acc: 0.40625
epoch: 148: train_loss: 1.1024739094228553, train_acc: 0.3139880994955699, val_loss: 1.059619128704071, val_acc: 0.359375
epoch: 149: train_loss: 1.1020073499944474, train_acc: 0.4002976218859355, val_loss: 1.0451716780662537, val_acc: 0.40625
epoch: 150: train_loss: 1.1018633567471114, train_acc: 0.4047619005044301, val_loss: 1.0337935090065002, val_acc: 0.453125
epoch: 151: train_loss: 1.1012718974236855, train_acc: 0.3988095223903656, val_loss: 1.0319219827651978, val_acc: 0.453125
epoch: 152: train_loss: 1.1007143668099946, train_acc: 0.4657738109429677, val_loss: 1.0342684388160706, val_acc: 0.453125
epoch: 153: train_loss: 1.1006091351890976, train_acc: 0.3363095223903656, val_loss: 1.0266730785369873, val_acc: 0.453125
epoch: 154: train_loss: 1.1002333711552363, train_acc: 0.3869047661622365, val_loss: 1.0165871679782867, val_acc: 0.5
epoch: 155: train_loss: 1.099885955198198, train_acc: 0.4464285671710968, val_loss: 1.0544837713241577, val_acc: 0.40625
epoch: 156: train_loss: 1.0997296759024293, train_acc: 0.3586309552192688, val_loss: 1.0490634441375732, val_acc: 0.40625
epoch: 157: train_loss: 1.0995927986213425, train_acc: 0.3898809552192688, val_loss: 1.0258302688598633, val_acc: 0.453125
epoch: 158: train_loss: 1.0996272801353246, train_acc: 0.3244047661622365, val_loss: 1.0463645458221436, val_acc: 0.359375
epoch: 159: train_loss: 1.0995616873105365, train_acc: 0.3303571442763011, val_loss: 1.0476003289222717, val_acc: 0.40625
epoch: 160: train_loss: 1.0993959318777047, train_acc: 0.3497023781140645, val_loss: 1.0489761233329773, val_acc: 0.359375
epoch: 161: train_loss: 1.0988926189671817, train_acc: 0.4166666666666667, val_loss: 1.0302106738090515, val_acc: 0.453125
epoch: 162: train_loss: 1.0988306839529478, train_acc: 0.3303571442763011, val_loss: 1.041919767856598, val_acc: 0.40625
epoch: 163: train_loss: 1.0982691822497825, train_acc: 0.4255952338377635, val_loss: 1.0565433502197266, val_acc: 0.359375
epoch: 164: train_loss: 1.0979007291071343, train_acc: 0.4032738109429677, val_loss: 1.0080679655075073, val_acc: 0.5
epoch: 165: train_loss: 1.097671484253014, train_acc: 0.3497023781140645, val_loss: 1.0335704684257507, val_acc: 0.453125
epoch: 166: train_loss: 1.0978278601003024, train_acc: 0.28125, val_loss: 1.0146030187606812, val_acc: 0.5
epoch: 167: train_loss: 1.0976671538655718, train_acc: 0.3482142860690753, val_loss: 1.018226981163025, val_acc: 0.5
epoch: 168: train_loss: 1.0974776159376787, train_acc: 0.3735119054714839, val_loss: 1.0622908473014832, val_acc: 0.3125
epoch: 169: train_loss: 1.0975377033738527, train_acc: 0.2797619054714839, val_loss: 1.0384171605110168, val_acc: 0.40625
epoch: 170: train_loss: 1.0976115462840417, train_acc: 0.3273809552192688, val_loss: 1.038604736328125, val_acc: 0.40625
epoch: 171: train_loss: 1.0972614468530166, train_acc: 0.4107142885526021, val_loss: 0.9856007695198059, val_acc: 0.59375
epoch: 172: train_loss: 1.0969440287018557, train_acc: 0.3675595223903656, val_loss: 1.0574480891227722, val_acc: 0.359375
epoch: 173: train_loss: 1.0966185548296377, train_acc: 0.4002976218859355, val_loss: 1.0339335203170776, val_acc: 0.40625
epoch: 174: train_loss: 1.0963804215476625, train_acc: 0.3928571442763011, val_loss: 1.0775887966156006, val_acc: 0.3125
epoch: 175: train_loss: 1.09611338839838, train_acc: 0.3720238109429677, val_loss: 1.05352121591568, val_acc: 0.3125
epoch: 176: train_loss: 1.0959112548783225, train_acc: 0.3616071442763011, val_loss: 1.0277959108352661, val_acc: 0.453125
epoch: 177: train_loss: 1.0957429414608058, train_acc: 0.3705357114473979, val_loss: 1.0410270690917969, val_acc: 0.40625
epoch: 178: train_loss: 1.0956754594541795, train_acc: 0.3467261890570323, val_loss: 1.039706289768219, val_acc: 0.40625
epoch: 179: train_loss: 1.0956210402426896, train_acc: 0.3586309552192688, val_loss: 1.0387507677078247, val_acc: 0.40625
epoch: 180: train_loss: 1.0955480162610007, train_acc: 0.3273809552192688, val_loss: 1.0296059250831604, val_acc: 0.453125
epoch: 181: train_loss: 1.0953379311622717, train_acc: 0.3452380994955699, val_loss: 1.0099307894706726, val_acc: 0.5
epoch: 182: train_loss: 1.0950212119272715, train_acc: 0.3705357114473979, val_loss: 1.0248460173606873, val_acc: 0.40625
epoch: 183: train_loss: 1.0949240127119464, train_acc: 0.3586309552192688, val_loss: 1.0100554823875427, val_acc: 0.5
epoch: 184: train_loss: 1.0946032761453508, train_acc: 0.3988095323244731, val_loss: 1.0345433354377747, val_acc: 0.40625
epoch: 185: train_loss: 1.094247349274201, train_acc: 0.3973214228947957, val_loss: 1.0312268733978271, val_acc: 0.453125
epoch: 186: train_loss: 1.0938685845563754, train_acc: 0.4255952338377635, val_loss: 1.0063421130180359, val_acc: 0.5
epoch: 187: train_loss: 1.0938318978387411, train_acc: 0.2767857164144516, val_loss: 1.0520055294036865, val_acc: 0.359375
epoch: 188: train_loss: 1.0933918430481426, train_acc: 0.4494047562281291, val_loss: 1.0264707207679749, val_acc: 0.40625
epoch: 189: train_loss: 1.0931659314716071, train_acc: 0.3616071442763011, val_loss: 1.0507012009620667, val_acc: 0.359375
epoch: 190: train_loss: 1.0930920925648009, train_acc: 0.3720238109429677, val_loss: 1.0333957076072693, val_acc: 0.40625
epoch: 191: train_loss: 1.093193000079029, train_acc: 0.2976190447807312, val_loss: 1.0101412534713745, val_acc: 0.5
epoch: 192: train_loss: 1.0931087854818564, train_acc: 0.2931547661622365, val_loss: 1.0178051590919495, val_acc: 0.453125
epoch: 193: train_loss: 1.0931504693432772, train_acc: 0.3080357164144516, val_loss: 1.0432706475257874, val_acc: 0.359375
epoch: 194: train_loss: 1.0932019662653278, train_acc: 0.3065476218859355, val_loss: 1.0357494950294495, val_acc: 0.40625
epoch: 195: train_loss: 1.092817055834394, train_acc: 0.4672619005044301, val_loss: 1.0278992652893066, val_acc: 0.40625
epoch: 196: train_loss: 1.092564491551943, train_acc: 0.3958333333333333, val_loss: 1.0101767778396606, val_acc: 0.453125
epoch: 197: train_loss: 1.0925432388429288, train_acc: 0.3154761890570323, val_loss: 1.0264390707015991, val_acc: 0.40625
epoch: 198: train_loss: 1.0925575156307699, train_acc: 0.3154761890570323, val_loss: 1.0337069630622864, val_acc: 0.40625
epoch: 199: train_loss: 1.0922685747345289, train_acc: 0.3601190447807312, val_loss: 1.0203442573547363, val_acc: 0.453125
epoch: 200: train_loss: 1.0920305807594437, train_acc: 0.3839285671710968, val_loss: 1.0279977917671204, val_acc: 0.40625
epoch: 201: train_loss: 1.0920522984104974, train_acc: 0.3229166666666667, val_loss: 1.0352699756622314, val_acc: 0.40625
epoch: 202: train_loss: 1.0918457949494296, train_acc: 0.3839285671710968, val_loss: 1.0332956314086914, val_acc: 0.40625
epoch: 203: train_loss: 1.0915330166325847, train_acc: 0.3779761890570323, val_loss: 1.0255653262138367, val_acc: 0.40625
epoch: 204: train_loss: 1.0912332532851676, train_acc: 0.3869047661622365, val_loss: 1.0373362302780151, val_acc: 0.40625
epoch: 205: train_loss: 1.090929949842996, train_acc: 0.4017857114473979, val_loss: 1.0289968252182007, val_acc: 0.40625
epoch: 206: train_loss: 1.0905150228844365, train_acc: 0.4300595223903656, val_loss: 1.0264142751693726, val_acc: 0.359375
epoch: 207: train_loss: 1.0902789794863799, train_acc: 0.3720238109429677, val_loss: 1.0028662085533142, val_acc: 0.515625
epoch: 208: train_loss: 1.0899166263271556, train_acc: 0.3869047661622365, val_loss: 1.0604379773139954, val_acc: 0.3125
epoch: 209: train_loss: 1.0897797647922758, train_acc: 0.3601190447807312, val_loss: 1.01922208070755, val_acc: 0.46875
epoch: 210: train_loss: 1.0897580841894585, train_acc: 0.3333333333333333, val_loss: 1.0536954700946808, val_acc: 0.3125
epoch: 211: train_loss: 1.0900297460128676, train_acc: 0.23958333333333334, val_loss: 1.0168140530586243, val_acc: 0.453125
epoch: 212: train_loss: 1.0898248536486022, train_acc: 0.3690476218859355, val_loss: 0.9740044176578522, val_acc: 0.59375
epoch: 213: train_loss: 1.089730924999231, train_acc: 0.3809523781140645, val_loss: 1.0495063066482544, val_acc: 0.3125
epoch: 214: train_loss: 1.089488357998604, train_acc: 0.4330357114473979, val_loss: 1.0090239346027374, val_acc: 0.46875
epoch: 215: train_loss: 1.0892919586212546, train_acc: 0.3675595223903656, val_loss: 1.016858458518982, val_acc: 0.40625
epoch: 216: train_loss: 1.0893415468079704, train_acc: 0.3244047661622365, val_loss: 1.0411318242549896, val_acc: 0.359375
epoch: 217: train_loss: 1.0889659846594575, train_acc: 0.4672619005044301, val_loss: 1.034030556678772, val_acc: 0.421875
epoch: 218: train_loss: 1.0888445823704271, train_acc: 0.3452380994955699, val_loss: 0.9794573783874512, val_acc: 0.5
epoch: 219: train_loss: 1.0887136462059888, train_acc: 0.3482142885526021, val_loss: 1.0325265526771545, val_acc: 0.359375
epoch: 220: train_loss: 1.0886210678749315, train_acc: 0.3794642885526021, val_loss: 1.038789063692093, val_acc: 0.375
epoch: 221: train_loss: 1.0882676258101478, train_acc: 0.4449404776096344, val_loss: 1.0278818011283875, val_acc: 0.40625
epoch: 222: train_loss: 1.088474220759131, train_acc: 0.2663690497477849, val_loss: 1.0318207740783691, val_acc: 0.40625
epoch: 223: train_loss: 1.0884134631958746, train_acc: 0.34375, val_loss: 1.007899820804596, val_acc: 0.515625
epoch: 224: train_loss: 1.088277843616627, train_acc: 0.3556547661622365, val_loss: 1.0123087763786316, val_acc: 0.40625
epoch: 225: train_loss: 1.0882723408814376, train_acc: 0.3169642885526021, val_loss: 1.0124874711036682, val_acc: 0.53125
epoch: 226: train_loss: 1.0883046945286217, train_acc: 0.3392857114473979, val_loss: 1.0298986434936523, val_acc: 0.40625
epoch: 227: train_loss: 1.0879680591891383, train_acc: 0.4226190447807312, val_loss: 1.0182751417160034, val_acc: 0.421875
epoch: 228: train_loss: 1.0878828449887825, train_acc: 0.3779761890570323, val_loss: 0.9830692112445831, val_acc: 0.53125
epoch: 229: train_loss: 1.0877401657726453, train_acc: 0.3482142885526021, val_loss: 1.0022279620170593, val_acc: 0.46875
epoch: 230: train_loss: 1.0876125896303854, train_acc: 0.3809523781140645, val_loss: 1.0279730558395386, val_acc: 0.375
epoch: 231: train_loss: 1.0876312033198343, train_acc: 0.3467261890570323, val_loss: 1.0288159847259521, val_acc: 0.421875
epoch: 232: train_loss: 1.0874867314773906, train_acc: 0.3794642885526021, val_loss: 0.9859533309936523, val_acc: 0.53125
epoch: 233: train_loss: 1.0871967777737184, train_acc: 0.4047619005044301, val_loss: 1.0381098985671997, val_acc: 0.375
epoch: 234: train_loss: 1.0868946388258154, train_acc: 0.4434523781140645, val_loss: 0.9954012632369995, val_acc: 0.5625
epoch: 235: train_loss: 1.086706444368524, train_acc: 0.4196428557236989, val_loss: 1.0262374877929688, val_acc: 0.375
epoch: 236: train_loss: 1.0865588535236403, train_acc: 0.3244047661622365, val_loss: 1.037129282951355, val_acc: 0.375
epoch: 237: train_loss: 1.0861390862478257, train_acc: 0.4568452338377635, val_loss: 1.0140329003334045, val_acc: 0.46875
epoch: 238: train_loss: 1.0858711482258185, train_acc: 0.4583333333333333, val_loss: 0.9811477363109589, val_acc: 0.53125
epoch: 239: train_loss: 1.0855325557291502, train_acc: 0.4464285671710968, val_loss: 0.9932467341423035, val_acc: 0.546875
epoch: 240: train_loss: 1.0851827161605587, train_acc: 0.4136904776096344, val_loss: 1.019867479801178, val_acc: 0.359375
epoch: 241: train_loss: 1.085054474100265, train_acc: 0.4002976218859355, val_loss: 1.0044312179088593, val_acc: 0.5
epoch: 242: train_loss: 1.084684792778292, train_acc: 0.4389880895614624, val_loss: 0.9875326156616211, val_acc: 0.515625
epoch: 243: train_loss: 1.0843028183517556, train_acc: 0.4449404776096344, val_loss: 0.9807926118373871, val_acc: 0.609375
epoch: 244: train_loss: 1.084151255678968, train_acc: 0.4032738109429677, val_loss: 1.0243617296218872, val_acc: 0.453125
epoch: 245: train_loss: 1.0839114419328486, train_acc: 0.4434523781140645, val_loss: 1.0199424624443054, val_acc: 0.421875
epoch: 246: train_loss: 1.083535623453889, train_acc: 0.4523809552192688, val_loss: 0.9977661073207855, val_acc: 0.53125
epoch: 247: train_loss: 1.0834213282151886, train_acc: 0.3258928557236989, val_loss: 1.0266211330890656, val_acc: 0.4375
epoch: 248: train_loss: 1.0831314520025186, train_acc: 0.4122023781140645, val_loss: 0.9907738268375397, val_acc: 0.53125
epoch: 249: train_loss: 1.0829779019355772, train_acc: 0.3794642885526021, val_loss: 1.0210310816764832, val_acc: 0.453125
epoch: 250: train_loss: 1.08266590403054, train_acc: 0.4434523781140645, val_loss: 1.0078263878822327, val_acc: 0.421875
epoch: 251: train_loss: 1.0822862368569803, train_acc: 0.4747023781140645, val_loss: 1.0302684903144836, val_acc: 0.390625
epoch: 252: train_loss: 1.081905065160809, train_acc: 0.4196428557236989, val_loss: 1.0369419753551483, val_acc: 0.375
epoch: 253: train_loss: 1.0814039063422385, train_acc: 0.4821428656578064, val_loss: 1.0325860977172852, val_acc: 0.375
epoch: 254: train_loss: 1.0811838555180169, train_acc: 0.4002976218859355, val_loss: 0.9843843579292297, val_acc: 0.453125
epoch: 255: train_loss: 1.0811093291267753, train_acc: 0.3705357114473979, val_loss: 1.0167675018310547, val_acc: 0.421875
epoch: 256: train_loss: 1.081042808008256, train_acc: 0.3095238109429677, val_loss: 0.9788306653499603, val_acc: 0.5
epoch: 257: train_loss: 1.081014057000478, train_acc: 0.3377976218859355, val_loss: 0.9984525144100189, val_acc: 0.515625
epoch: 258: train_loss: 1.0809326899864804, train_acc: 0.3541666666666667, val_loss: 1.0209619998931885, val_acc: 0.40625
epoch: 259: train_loss: 1.080620827201085, train_acc: 0.4642857114473979, val_loss: 1.0265631973743439, val_acc: 0.390625
epoch: 260: train_loss: 1.080301576114401, train_acc: 0.4494047562281291, val_loss: 1.0040643215179443, val_acc: 0.484375
epoch: 261: train_loss: 1.0802429674084253, train_acc: 0.2931547661622365, val_loss: 0.9789223670959473, val_acc: 0.609375
epoch: 262: train_loss: 1.0801076968359857, train_acc: 0.3690476218859355, val_loss: 1.0179314315319061, val_acc: 0.375
epoch: 263: train_loss: 1.0799487822435119, train_acc: 0.3898809552192688, val_loss: 0.986806720495224, val_acc: 0.46875
epoch: 264: train_loss: 1.079663825784839, train_acc: 0.4479166666666667, val_loss: 1.0201314091682434, val_acc: 0.4375
epoch: 265: train_loss: 1.0794002327853276, train_acc: 0.3988095223903656, val_loss: 1.0137297213077545, val_acc: 0.421875
epoch: 266: train_loss: 1.0793106774712322, train_acc: 0.3571428557236989, val_loss: 0.9806079268455505, val_acc: 0.546875
epoch: 267: train_loss: 1.0791199758871275, train_acc: 0.3720238109429677, val_loss: 1.0263259410858154, val_acc: 0.359375
epoch: 268: train_loss: 1.0787898294692293, train_acc: 0.4479166666666667, val_loss: 0.9686689078807831, val_acc: 0.578125
epoch: 269: train_loss: 1.0787092551772977, train_acc: 0.3467261890570323, val_loss: 0.9741582870483398, val_acc: 0.515625
epoch: 270: train_loss: 1.078598159705581, train_acc: 0.3452380994955699, val_loss: 1.0208090245723724, val_acc: 0.4375
epoch: 271: train_loss: 1.078465283428337, train_acc: 0.3869047661622365, val_loss: 1.0278415977954865, val_acc: 0.4375
epoch: 272: train_loss: 1.0783793789417606, train_acc: 0.4122023781140645, val_loss: 1.0089190006256104, val_acc: 0.46875
epoch: 273: train_loss: 1.0782516067915588, train_acc: 0.3779761890570323, val_loss: 1.0443741381168365, val_acc: 0.296875
epoch: 274: train_loss: 1.0781617083693997, train_acc: 0.3705357114473979, val_loss: 1.0072042644023895, val_acc: 0.484375
epoch: 275: train_loss: 1.0780257648891873, train_acc: 0.4122023781140645, val_loss: 0.9881646037101746, val_acc: 0.46875
epoch: 276: train_loss: 1.0779601790270938, train_acc: 0.3675595223903656, val_loss: 0.9597884118556976, val_acc: 0.625
epoch: 277: train_loss: 1.077893422376052, train_acc: 0.3377976218859355, val_loss: 1.0127279162406921, val_acc: 0.453125
epoch: 278: train_loss: 1.0779567090983602, train_acc: 0.2857142885526021, val_loss: 1.0273134112358093, val_acc: 0.40625
epoch: 279: train_loss: 1.0779203684557053, train_acc: 0.3913690447807312, val_loss: 0.9605797827243805, val_acc: 0.5625
epoch: 280: train_loss: 1.0776082186795075, train_acc: 0.4360119005044301, val_loss: 0.9992525279521942, val_acc: 0.5
epoch: 281: train_loss: 1.0773868459336302, train_acc: 0.4300595223903656, val_loss: 0.9981950223445892, val_acc: 0.515625
epoch: 282: train_loss: 1.0770747158215663, train_acc: 0.4464285671710968, val_loss: 0.9582414031028748, val_acc: 0.546875
epoch: 283: train_loss: 1.0770551511239561, train_acc: 0.3690476218859355, val_loss: 0.993208646774292, val_acc: 0.515625
epoch: 284: train_loss: 1.0769090257192915, train_acc: 0.4122023781140645, val_loss: 0.9918516576290131, val_acc: 0.484375
epoch: 285: train_loss: 1.0767225678706227, train_acc: 0.3943452338377635, val_loss: 0.9552861452102661, val_acc: 0.59375
epoch: 286: train_loss: 1.07650283730127, train_acc: 0.4270833333333333, val_loss: 1.0199232399463654, val_acc: 0.4375
epoch: 287: train_loss: 1.0765006160708492, train_acc: 0.3035714328289032, val_loss: 1.0037400722503662, val_acc: 0.421875
epoch: 288: train_loss: 1.0763823640250283, train_acc: 0.4285714228947957, val_loss: 1.01614111661911, val_acc: 0.453125
epoch: 289: train_loss: 1.0761414945125585, train_acc: 0.4196428557236989, val_loss: 0.9565974771976471, val_acc: 0.578125
epoch: 290: train_loss: 1.0758113465085204, train_acc: 0.4241071442763011, val_loss: 0.982622504234314, val_acc: 0.46875
epoch: 291: train_loss: 1.0757650706457769, train_acc: 0.2872023830811183, val_loss: 0.9720191061496735, val_acc: 0.53125
epoch: 292: train_loss: 1.0756113567856802, train_acc: 0.3839285721381505, val_loss: 0.9830031991004944, val_acc: 0.46875
epoch: 293: train_loss: 1.0753365589512722, train_acc: 0.4464285671710968, val_loss: 1.01751509308815, val_acc: 0.359375
epoch: 294: train_loss: 1.075135805849302, train_acc: 0.4032738109429677, val_loss: 1.035665363073349, val_acc: 0.46875
epoch: 295: train_loss: 1.07521988082308, train_acc: 0.3065476218859355, val_loss: 1.030870795249939, val_acc: 0.453125
epoch: 296: train_loss: 1.0751955616888913, train_acc: 0.3377976218859355, val_loss: 0.9965039491653442, val_acc: 0.5
epoch: 297: train_loss: 1.0746802474295005, train_acc: 0.5476190447807312, val_loss: 0.953124612569809, val_acc: 0.671875
epoch: 298: train_loss: 1.0745057774626696, train_acc: 0.3809523781140645, val_loss: 1.0065343976020813, val_acc: 0.4375
epoch: 299: train_loss: 1.0743913692235951, train_acc: 0.3705357114473979, val_loss: 0.990381121635437, val_acc: 0.5
epoch: 300: train_loss: 1.0741666624052322, train_acc: 0.4508928656578064, val_loss: 1.0271848738193512, val_acc: 0.390625
epoch: 301: train_loss: 1.0740555024831244, train_acc: 0.3824404776096344, val_loss: 0.9783458113670349, val_acc: 0.5625
epoch: 302: train_loss: 1.0738144699234125, train_acc: 0.4404761989911397, val_loss: 0.9943520724773407, val_acc: 0.484375
epoch: 303: train_loss: 1.0736895518606175, train_acc: 0.4300595323244731, val_loss: 0.9708467125892639, val_acc: 0.625
epoch: 304: train_loss: 1.0733294657670742, train_acc: 0.4583333333333333, val_loss: 0.9766057431697845, val_acc: 0.5625
epoch: 305: train_loss: 1.0730631876745003, train_acc: 0.4494047562281291, val_loss: 0.9864903688430786, val_acc: 0.515625
epoch: 306: train_loss: 1.072817676942331, train_acc: 0.4196428557236989, val_loss: 0.9813647270202637, val_acc: 0.5
epoch: 307: train_loss: 1.0726430877333601, train_acc: 0.4464285671710968, val_loss: 0.9817186892032623, val_acc: 0.625
epoch: 308: train_loss: 1.0723627467500103, train_acc: 0.4166666666666667, val_loss: 0.9607086479663849, val_acc: 0.609375
epoch: 309: train_loss: 1.0720533648485786, train_acc: 0.4508928656578064, val_loss: 0.9744678139686584, val_acc: 0.5625
epoch: 310: train_loss: 1.0719218300725366, train_acc: 0.3839285671710968, val_loss: 0.9832497835159302, val_acc: 0.53125
epoch: 311: train_loss: 1.0717494361038906, train_acc: 0.4151785671710968, val_loss: 1.001428633928299, val_acc: 0.484375
epoch: 312: train_loss: 1.0714410101643772, train_acc: 0.4895833333333333, val_loss: 0.9953849613666534, val_acc: 0.5625
epoch: 313: train_loss: 1.0712120930353806, train_acc: 0.3690476218859355, val_loss: 1.0311375260353088, val_acc: 0.34375
epoch: 314: train_loss: 1.071009186525194, train_acc: 0.4345238109429677, val_loss: 0.9782626330852509, val_acc: 0.59375
epoch: 315: train_loss: 1.0707611440354767, train_acc: 0.4330357114473979, val_loss: 0.9812383949756622, val_acc: 0.578125
epoch: 316: train_loss: 1.070415481972018, train_acc: 0.4568452338377635, val_loss: 0.9647382497787476, val_acc: 0.5625
epoch: 317: train_loss: 1.0702943836243153, train_acc: 0.4122023781140645, val_loss: 1.0174505412578583, val_acc: 0.484375
epoch: 318: train_loss: 1.0702882436500203, train_acc: 0.3020833333333333, val_loss: 0.953357994556427, val_acc: 0.625
epoch: 319: train_loss: 1.0700491001208627, train_acc: 0.4032738109429677, val_loss: 0.9510354697704315, val_acc: 0.609375
epoch: 320: train_loss: 1.070027309909912, train_acc: 0.3675595223903656, val_loss: 0.9584916830062866, val_acc: 0.609375
epoch: 321: train_loss: 1.0698514127830052, train_acc: 0.3839285671710968, val_loss: 0.9957536160945892, val_acc: 0.515625
epoch: 322: train_loss: 1.0694927915207992, train_acc: 0.4895833333333333, val_loss: 0.986434668302536, val_acc: 0.59375
epoch: 323: train_loss: 1.0695687355205359, train_acc: 0.3139880994955699, val_loss: 0.9702264964580536, val_acc: 0.59375
epoch: 324: train_loss: 1.0694185208051636, train_acc: 0.3973214328289032, val_loss: 0.9893541038036346, val_acc: 0.515625
epoch: 325: train_loss: 1.0693229495991476, train_acc: 0.3690476218859355, val_loss: 0.9740351438522339, val_acc: 0.546875
epoch: 326: train_loss: 1.0692468569913047, train_acc: 0.3913690447807312, val_loss: 0.9880906045436859, val_acc: 0.453125
epoch: 327: train_loss: 1.0689358921433858, train_acc: 0.4538690447807312, val_loss: 0.9820139110088348, val_acc: 0.5
epoch: 328: train_loss: 1.068643233817158, train_acc: 0.4717261989911397, val_loss: 1.0136251151561737, val_acc: 0.421875
epoch: 329: train_loss: 1.0684144602279477, train_acc: 0.3958333333333333, val_loss: 1.0095796585083008, val_acc: 0.46875
epoch: 330: train_loss: 1.0683637893452032, train_acc: 0.3779761890570323, val_loss: 0.9513108134269714, val_acc: 0.625
epoch: 331: train_loss: 1.0680983347825743, train_acc: 0.4389880895614624, val_loss: 0.9926948547363281, val_acc: 0.453125
epoch: 332: train_loss: 1.068014642617128, train_acc: 0.3869047661622365, val_loss: 0.9734518229961395, val_acc: 0.546875
epoch: 333: train_loss: 1.0677936328147468, train_acc: 0.4523809552192688, val_loss: 0.9883700311183929, val_acc: 0.546875
epoch: 334: train_loss: 1.067617235610735, train_acc: 0.4017857114473979, val_loss: 1.0029214918613434, val_acc: 0.546875
epoch: 335: train_loss: 1.0675073001711148, train_acc: 0.3794642885526021, val_loss: 0.9861276745796204, val_acc: 0.609375
epoch: 336: train_loss: 1.0672236304844411, train_acc: 0.4925595223903656, val_loss: 0.9800893366336823, val_acc: 0.5625
epoch: 337: train_loss: 1.0669838612719171, train_acc: 0.4300595223903656, val_loss: 0.967525988817215, val_acc: 0.65625
epoch: 338: train_loss: 1.0668241507066978, train_acc: 0.4375, val_loss: 0.9761192202568054, val_acc: 0.625
epoch: 339: train_loss: 1.06676790685046, train_acc: 0.3422619005044301, val_loss: 0.972392350435257, val_acc: 0.5625
epoch: 340: train_loss: 1.0664880416726559, train_acc: 0.4241071442763011, val_loss: 0.9832809567451477, val_acc: 0.578125
epoch: 341: train_loss: 1.0662153297000465, train_acc: 0.4761904776096344, val_loss: 0.981164962053299, val_acc: 0.625
epoch: 342: train_loss: 1.0660137524873574, train_acc: 0.4360119005044301, val_loss: 0.9697084128856659, val_acc: 0.578125
epoch: 343: train_loss: 1.0657321319792628, train_acc: 0.4360119005044301, val_loss: 1.0115187466144562, val_acc: 0.5
epoch: 344: train_loss: 1.0655232894247861, train_acc: 0.4360119005044301, val_loss: 0.9849118292331696, val_acc: 0.53125
epoch: 345: train_loss: 1.0653624497856484, train_acc: 0.3988095223903656, val_loss: 0.970293790102005, val_acc: 0.546875
epoch: 346: train_loss: 1.0652277071125573, train_acc: 0.4568452338377635, val_loss: 0.981796145439148, val_acc: 0.625
epoch: 347: train_loss: 1.0650354543294034, train_acc: 0.3898809552192688, val_loss: 0.9672835171222687, val_acc: 0.609375
epoch: 348: train_loss: 1.0647623474641201, train_acc: 0.4553571442763011, val_loss: 0.9987367689609528, val_acc: 0.5625
epoch: 349: train_loss: 1.0645135035968967, train_acc: 0.5, val_loss: 0.9770380556583405, val_acc: 0.53125
epoch: 350: train_loss: 1.0643383863418308, train_acc: 0.4449404776096344, val_loss: 0.9633243083953857, val_acc: 0.546875
epoch: 351: train_loss: 1.0642461680214517, train_acc: 0.3854166666666667, val_loss: 0.9606932699680328, val_acc: 0.703125
epoch: 352: train_loss: 1.0641108818274836, train_acc: 0.4107142885526021, val_loss: 0.9798759818077087, val_acc: 0.53125
epoch: 353: train_loss: 1.06389621264966, train_acc: 0.4389880895614624, val_loss: 0.9703242480754852, val_acc: 0.625
epoch: 354: train_loss: 1.0637939856085983, train_acc: 0.3928571442763011, val_loss: 0.9527981281280518, val_acc: 0.6875
epoch: 355: train_loss: 1.0637239229478197, train_acc: 0.4107142885526021, val_loss: 0.9754160344600677, val_acc: 0.5625
epoch: 356: train_loss: 1.063665723878574, train_acc: 0.3883928656578064, val_loss: 0.9881505370140076, val_acc: 0.53125
epoch: 357: train_loss: 1.0636394091047614, train_acc: 0.3422619054714839, val_loss: 0.9861492812633514, val_acc: 0.578125
epoch: 358: train_loss: 1.0633075784065158, train_acc: 0.523809532324473, val_loss: 0.9857377707958221, val_acc: 0.5625
epoch: 359: train_loss: 1.0630482559402787, train_acc: 0.4270833333333333, val_loss: 0.9692409336566925, val_acc: 0.640625
epoch: 360: train_loss: 1.0627795606989174, train_acc: 0.4672619005044301, val_loss: 1.0011380314826965, val_acc: 0.640625
epoch: 361: train_loss: 1.0626967665679332, train_acc: 0.4047619005044301, val_loss: 0.9427409172058105, val_acc: 0.71875
epoch: 362: train_loss: 1.0624617015253637, train_acc: 0.4464285671710968, val_loss: 0.978904664516449, val_acc: 0.625
epoch: 363: train_loss: 1.0623773779624552, train_acc: 0.4464285671710968, val_loss: 1.006519377231598, val_acc: 0.359375
epoch: 364: train_loss: 1.0621041896136387, train_acc: 0.4761904776096344, val_loss: 0.9609372317790985, val_acc: 0.609375
epoch: 365: train_loss: 1.061821859119586, train_acc: 0.4836309552192688, val_loss: 0.983922928571701, val_acc: 0.484375
epoch: 366: train_loss: 1.0616343994231578, train_acc: 0.4434523781140645, val_loss: 0.9423182308673859, val_acc: 0.734375
epoch: 367: train_loss: 1.0615701778535398, train_acc: 0.3645833333333333, val_loss: 0.9567174911499023, val_acc: 0.65625
epoch: 368: train_loss: 1.061454800570884, train_acc: 0.3720238109429677, val_loss: 0.9306102693080902, val_acc: 0.703125
epoch: 369: train_loss: 1.0613257519296702, train_acc: 0.3913690447807312, val_loss: 0.9609719812870026, val_acc: 0.609375
epoch: 370: train_loss: 1.0611485673410879, train_acc: 0.4196428557236989, val_loss: 0.9497118592262268, val_acc: 0.65625
epoch: 371: train_loss: 1.0610074847402544, train_acc: 0.4136904776096344, val_loss: 0.94648277759552, val_acc: 0.71875
epoch: 372: train_loss: 1.060823158721396, train_acc: 0.4404761890570323, val_loss: 0.9654683768749237, val_acc: 0.578125
epoch: 373: train_loss: 1.0608430905881843, train_acc: 0.3839285671710968, val_loss: 0.9516466557979584, val_acc: 0.59375
epoch: 374: train_loss: 1.0606548102696742, train_acc: 0.4508928656578064, val_loss: 0.9545730352401733, val_acc: 0.671875
epoch: 375: train_loss: 1.0605182544136729, train_acc: 0.4196428557236989, val_loss: 0.9561424851417542, val_acc: 0.546875
epoch: 376: train_loss: 1.0602414194827166, train_acc: 0.4672619005044301, val_loss: 0.9873131215572357, val_acc: 0.625
epoch: 377: train_loss: 1.06016609656117, train_acc: 0.3913690447807312, val_loss: 1.0134061872959137, val_acc: 0.453125
epoch: 378: train_loss: 1.0598847098589577, train_acc: 0.4851190447807312, val_loss: 0.9800640940666199, val_acc: 0.53125
epoch: 379: train_loss: 1.0595576468266943, train_acc: 0.4925595323244731, val_loss: 0.9710375368595123, val_acc: 0.546875
epoch: 380: train_loss: 1.0592751181761846, train_acc: 0.5208333333333334, val_loss: 0.9872937798500061, val_acc: 0.53125
epoch: 381: train_loss: 1.0592013640137456, train_acc: 0.3020833333333333, val_loss: 0.9405659735202789, val_acc: 0.625
epoch: 382: train_loss: 1.0590218792177475, train_acc: 0.4107142885526021, val_loss: 0.970962405204773, val_acc: 0.53125
epoch: 383: train_loss: 1.058947189090153, train_acc: 0.3586309552192688, val_loss: 0.9368845224380493, val_acc: 0.65625
epoch: 384: train_loss: 1.0585845688720807, train_acc: 0.5252976218859354, val_loss: 0.9701837003231049, val_acc: 0.5625
epoch: 385: train_loss: 1.0582875637192806, train_acc: 0.5044642885526022, val_loss: 0.9652571976184845, val_acc: 0.59375
epoch: 386: train_loss: 1.058116084280762, train_acc: 0.4583333333333333, val_loss: 0.9672732949256897, val_acc: 0.59375
epoch: 387: train_loss: 1.0578595435701288, train_acc: 0.4925595323244731, val_loss: 1.0073321759700775, val_acc: 0.4375
epoch: 388: train_loss: 1.057577661039897, train_acc: 0.5163690447807312, val_loss: 0.9578496813774109, val_acc: 0.59375
epoch: 389: train_loss: 1.057375926441617, train_acc: 0.4553571442763011, val_loss: 0.9680219292640686, val_acc: 0.625
epoch: 390: train_loss: 1.0572592083552104, train_acc: 0.4196428557236989, val_loss: 0.9357418715953827, val_acc: 0.640625
epoch: 391: train_loss: 1.0571301166905842, train_acc: 0.4226190447807312, val_loss: 0.9545817673206329, val_acc: 0.609375
epoch: 392: train_loss: 1.056927941884906, train_acc: 0.4241071442763011, val_loss: 0.960606724023819, val_acc: 0.640625
epoch: 393: train_loss: 1.0567445386888215, train_acc: 0.4389880895614624, val_loss: 0.9737533628940582, val_acc: 0.578125
epoch: 394: train_loss: 1.0565214366852484, train_acc: 0.4895833333333333, val_loss: 0.9409621059894562, val_acc: 0.59375
epoch: 395: train_loss: 1.056338434267526, train_acc: 0.4508928656578064, val_loss: 0.9506886005401611, val_acc: 0.609375
epoch: 396: train_loss: 1.056124057767774, train_acc: 0.4553571442763011, val_loss: 0.9493266046047211, val_acc: 0.640625
epoch: 397: train_loss: 1.0558484316471237, train_acc: 0.4880952338377635, val_loss: 0.9651931822299957, val_acc: 0.546875
epoch: 398: train_loss: 1.0557329818021288, train_acc: 0.3883928557236989, val_loss: 0.9858432114124298, val_acc: 0.609375
epoch: 399: train_loss: 1.0556030245125296, train_acc: 0.4107142885526021, val_loss: 0.9847188889980316, val_acc: 0.484375
epoch: 400: train_loss: 1.0553283621545446, train_acc: 0.5074404776096344, val_loss: 0.9711057841777802, val_acc: 0.65625
epoch: 401: train_loss: 1.0551122567349212, train_acc: 0.4538690447807312, val_loss: 0.9711486101150513, val_acc: 0.578125
epoch: 402: train_loss: 1.055068572214362, train_acc: 0.3898809552192688, val_loss: 0.9291281700134277, val_acc: 0.6875
epoch: 403: train_loss: 1.0548108190593157, train_acc: 0.4791666666666667, val_loss: 0.9987675547599792, val_acc: 0.609375
epoch: 404: train_loss: 1.0546003397109582, train_acc: 0.4360119005044301, val_loss: 0.925962507724762, val_acc: 0.734375
epoch: 405: train_loss: 1.0546432700748323, train_acc: 0.3125, val_loss: 0.9893474876880646, val_acc: 0.484375
epoch: 406: train_loss: 1.0545272279430085, train_acc: 0.4241071442763011, val_loss: 0.9847188889980316, val_acc: 0.546875
epoch: 407: train_loss: 1.054289194152636, train_acc: 0.46875, val_loss: 0.9704952836036682, val_acc: 0.625
epoch: 408: train_loss: 1.0540543406983172, train_acc: 0.5133928656578064, val_loss: 0.9491869807243347, val_acc: 0.65625
epoch: 409: train_loss: 1.0538493971999103, train_acc: 0.4642857114473979, val_loss: 0.9812248349189758, val_acc: 0.5
epoch: 410: train_loss: 1.0536007748226377, train_acc: 0.5208333333333334, val_loss: 0.9519302546977997, val_acc: 0.65625
epoch: 411: train_loss: 1.0533030552964384, train_acc: 0.519345243771871, val_loss: 0.9586663544178009, val_acc: 0.59375
epoch: 412: train_loss: 1.052987253117504, train_acc: 0.4925595323244731, val_loss: 0.9594801068305969, val_acc: 0.625
epoch: 413: train_loss: 1.052754966821072, train_acc: 0.5029761989911398, val_loss: 0.9679217636585236, val_acc: 0.546875
epoch: 414: train_loss: 1.0525512775264116, train_acc: 0.4434523781140645, val_loss: 0.9350584745407104, val_acc: 0.671875
epoch: 415: train_loss: 1.052480810345748, train_acc: 0.3735119005044301, val_loss: 0.928151547908783, val_acc: 0.703125
epoch: 416: train_loss: 1.052308975030288, train_acc: 0.46875, val_loss: 0.9575221240520477, val_acc: 0.71875
epoch: 417: train_loss: 1.052051291273732, train_acc: 0.4910714228947957, val_loss: 0.9497416615486145, val_acc: 0.671875
epoch: 418: train_loss: 1.051917163218026, train_acc: 0.4345238109429677, val_loss: 0.9125956594944, val_acc: 0.640625
epoch: 419: train_loss: 1.0515949467817949, train_acc: 0.4985119005044301, val_loss: 0.9729297161102295, val_acc: 0.546875
epoch: 420: train_loss: 1.051423874386511, train_acc: 0.4568452338377635, val_loss: 0.9318542778491974, val_acc: 0.578125
epoch: 421: train_loss: 1.0512530744829078, train_acc: 0.4449404776096344, val_loss: 0.9563897550106049, val_acc: 0.671875
epoch: 422: train_loss: 1.051065435035667, train_acc: 0.4836309552192688, val_loss: 0.9312587380409241, val_acc: 0.625
epoch: 423: train_loss: 1.051016327813737, train_acc: 0.3883928557236989, val_loss: 0.9450599849224091, val_acc: 0.609375
epoch: 424: train_loss: 1.0507660670841448, train_acc: 0.5104166666666666, val_loss: 0.9463467299938202, val_acc: 0.734375
epoch: 425: train_loss: 1.0505274429287716, train_acc: 0.4895833333333333, val_loss: 0.9567601680755615, val_acc: 0.671875
epoch: 426: train_loss: 1.05038913681952, train_acc: 0.46875, val_loss: 0.943898469209671, val_acc: 0.71875
epoch: 427: train_loss: 1.0502286419226008, train_acc: 0.4940476218859355, val_loss: 0.948228269815445, val_acc: 0.75
epoch: 428: train_loss: 1.0500327057401253, train_acc: 0.4419642885526021, val_loss: 0.9657103419303894, val_acc: 0.625
epoch: 429: train_loss: 1.0497816322847862, train_acc: 0.523809532324473, val_loss: 0.9306047260761261, val_acc: 0.6875
epoch: 430: train_loss: 1.0495091726497827, train_acc: 0.4970238109429677, val_loss: 0.9454519748687744, val_acc: 0.765625
epoch: 431: train_loss: 1.0493750735933407, train_acc: 0.4404761890570323, val_loss: 0.9195981919765472, val_acc: 0.65625
epoch: 432: train_loss: 1.0492560628104712, train_acc: 0.3779761890570323, val_loss: 0.9797647893428802, val_acc: 0.5625
epoch: 433: train_loss: 1.0488869010852786, train_acc: 0.5788690447807312, val_loss: 0.9602901637554169, val_acc: 0.5625
epoch: 434: train_loss: 1.048687826445277, train_acc: 0.5014880895614624, val_loss: 0.921183317899704, val_acc: 0.828125
epoch: 435: train_loss: 1.048625119189969, train_acc: 0.3422619054714839, val_loss: 0.970428854227066, val_acc: 0.671875
epoch: 436: train_loss: 1.0485596819109455, train_acc: 0.4151785671710968, val_loss: 0.9455538690090179, val_acc: 0.6875
epoch: 437: train_loss: 1.0482591019101346, train_acc: 0.53125, val_loss: 0.8939894735813141, val_acc: 0.71875
epoch: 438: train_loss: 1.0481424027984585, train_acc: 0.4419642885526021, val_loss: 0.9698942005634308, val_acc: 0.625
epoch: 439: train_loss: 1.048101172076934, train_acc: 0.40625, val_loss: 0.9386593997478485, val_acc: 0.65625
epoch: 440: train_loss: 1.0479923082587406, train_acc: 0.4449404776096344, val_loss: 0.969329446554184, val_acc: 0.671875
epoch: 441: train_loss: 1.0478764909514842, train_acc: 0.4434523781140645, val_loss: 0.9161439538002014, val_acc: 0.8125
epoch: 442: train_loss: 1.0476759578430006, train_acc: 0.4910714228947957, val_loss: 0.9515438973903656, val_acc: 0.578125
epoch: 443: train_loss: 1.0475897488740837, train_acc: 0.4270833333333333, val_loss: 0.9237570464611053, val_acc: 0.703125
epoch: 444: train_loss: 1.0474775841620092, train_acc: 0.4523809552192688, val_loss: 0.9320860505104065, val_acc: 0.734375
epoch: 445: train_loss: 1.0472369619014559, train_acc: 0.5074404776096344, val_loss: 0.9134695827960968, val_acc: 0.75
epoch: 446: train_loss: 1.0469886563679434, train_acc: 0.5029761989911398, val_loss: 0.9313525855541229, val_acc: 0.78125
epoch: 447: train_loss: 1.0466712122073492, train_acc: 0.543154756228129, val_loss: 0.9200198352336884, val_acc: 0.828125
epoch: 448: train_loss: 1.0464988251748406, train_acc: 0.4553571442763011, val_loss: 0.9788654744625092, val_acc: 0.5625
epoch: 449: train_loss: 1.0462025285650187, train_acc: 0.5223214228947958, val_loss: 0.9081248342990875, val_acc: 0.8125
epoch: 450: train_loss: 1.0459945852987347, train_acc: 0.5, val_loss: 0.9642980396747589, val_acc: 0.65625
epoch: 451: train_loss: 1.0457435001929607, train_acc: 0.5, val_loss: 0.9444195628166199, val_acc: 0.765625
epoch: 452: train_loss: 1.0455411380928528, train_acc: 0.4494047661622365, val_loss: 0.9446245431900024, val_acc: 0.6875
epoch: 453: train_loss: 1.04539072202866, train_acc: 0.4642857114473979, val_loss: 0.9404756128787994, val_acc: 0.796875
epoch: 454: train_loss: 1.0452214102168667, train_acc: 0.5089285671710968, val_loss: 0.9244451224803925, val_acc: 0.75
epoch: 455: train_loss: 1.0450488818232084, train_acc: 0.4583333333333333, val_loss: 0.9375647306442261, val_acc: 0.71875
epoch: 456: train_loss: 1.0449011120632743, train_acc: 0.4345238109429677, val_loss: 0.9516478478908539, val_acc: 0.75
epoch: 457: train_loss: 1.044886319137557, train_acc: 0.3735119005044301, val_loss: 0.9495112597942352, val_acc: 0.6875
epoch: 458: train_loss: 1.044617621597382, train_acc: 0.543154756228129, val_loss: 0.9306195378303528, val_acc: 0.78125
epoch: 459: train_loss: 1.044449020468671, train_acc: 0.4553571442763011, val_loss: 0.9504012167453766, val_acc: 0.734375
epoch: 460: train_loss: 1.0443149077142402, train_acc: 0.4330357114473979, val_loss: 0.9299361109733582, val_acc: 0.828125
epoch: 461: train_loss: 1.0440281589137876, train_acc: 0.5327380895614624, val_loss: 0.924073725938797, val_acc: 0.75
epoch: 462: train_loss: 1.0437426960579868, train_acc: 0.5446428656578064, val_loss: 0.9306230843067169, val_acc: 0.703125
epoch: 463: train_loss: 1.0436385759505737, train_acc: 0.4002976218859355, val_loss: 0.930793970823288, val_acc: 0.6875
epoch: 464: train_loss: 1.04356937703266, train_acc: 0.3973214328289032, val_loss: 0.9246228635311127, val_acc: 0.75
epoch: 465: train_loss: 1.0433964820549386, train_acc: 0.4553571442763011, val_loss: 0.9097360372543335, val_acc: 0.734375
epoch: 466: train_loss: 1.043351529965139, train_acc: 0.3720238109429677, val_loss: 0.9251173436641693, val_acc: 0.734375
epoch: 467: train_loss: 1.0431841179261525, train_acc: 0.4226190447807312, val_loss: 0.9159842431545258, val_acc: 0.734375
epoch: 468: train_loss: 1.0429800085845198, train_acc: 0.4627976218859355, val_loss: 0.9328405857086182, val_acc: 0.875
epoch: 469: train_loss: 1.0428541285771855, train_acc: 0.4866071442763011, val_loss: 0.9552181661128998, val_acc: 0.734375
epoch: 470: train_loss: 1.0425106909202504, train_acc: 0.5625, val_loss: 0.9577108323574066, val_acc: 0.78125
epoch: 471: train_loss: 1.0423196376127715, train_acc: 0.4538690447807312, val_loss: 0.9376847147941589, val_acc: 0.75
epoch: 472: train_loss: 1.0422212141260792, train_acc: 0.4315476218859355, val_loss: 0.9303984940052032, val_acc: 0.6875
epoch: 473: train_loss: 1.0420324633896103, train_acc: 0.4851190447807312, val_loss: 0.9234817326068878, val_acc: 0.84375
epoch: 474: train_loss: 1.0418045141822416, train_acc: 0.5059523781140646, val_loss: 0.958417534828186, val_acc: 0.703125
epoch: 475: train_loss: 1.0415445904270946, train_acc: 0.5595238010088602, val_loss: 0.9210376143455505, val_acc: 0.84375
epoch: 476: train_loss: 1.0413660800681928, train_acc: 0.4776785671710968, val_loss: 0.9358188807964325, val_acc: 0.71875
epoch: 477: train_loss: 1.041198259118212, train_acc: 0.4657738109429677, val_loss: 0.9721092283725739, val_acc: 0.609375
epoch: 478: train_loss: 1.0410105996689372, train_acc: 0.4494047562281291, val_loss: 0.9070537686347961, val_acc: 0.828125
epoch: 479: train_loss: 1.0409709225512216, train_acc: 0.3705357114473979, val_loss: 0.9474405646324158, val_acc: 0.875
epoch: 480: train_loss: 1.0408364844999627, train_acc: 0.4226190447807312, val_loss: 0.9092395305633545, val_acc: 0.859375
epoch: 481: train_loss: 1.0405729699101853, train_acc: 0.5208333333333334, val_loss: 0.9180790781974792, val_acc: 0.671875
epoch: 482: train_loss: 1.0404196822536331, train_acc: 0.4657738109429677, val_loss: 0.927930474281311, val_acc: 0.765625
epoch: 483: train_loss: 1.0402626902193408, train_acc: 0.4360119005044301, val_loss: 0.9452874660491943, val_acc: 0.765625
epoch: 484: train_loss: 1.0397466450212751, train_acc: 0.6860119104385376, val_loss: 0.9145081341266632, val_acc: 0.890625
epoch: 485: train_loss: 1.0395126600324374, train_acc: 0.5297619104385376, val_loss: 0.9307082891464233, val_acc: 0.875
epoch: 486: train_loss: 1.0393801300106273, train_acc: 0.4717261989911397, val_loss: 0.9110734462738037, val_acc: 0.8125
epoch: 487: train_loss: 1.0393426060432296, train_acc: 0.3809523781140645, val_loss: 0.9171058535575867, val_acc: 0.8125
epoch: 488: train_loss: 1.039242389093229, train_acc: 0.4181547562281291, val_loss: 0.9224627017974854, val_acc: 0.90625
epoch: 489: train_loss: 1.0391106508621557, train_acc: 0.4241071442763011, val_loss: 0.9412010610103607, val_acc: 0.75
epoch: 490: train_loss: 1.0388580304946695, train_acc: 0.5520833333333334, val_loss: 0.9484250545501709, val_acc: 0.671875
epoch: 491: train_loss: 1.0385984297044235, train_acc: 0.5, val_loss: 0.9111438095569611, val_acc: 0.765625
epoch: 492: train_loss: 1.0384955276823917, train_acc: 0.4479166666666667, val_loss: 0.935489296913147, val_acc: 0.703125
epoch: 493: train_loss: 1.0382934116561084, train_acc: 0.5, val_loss: 0.927364706993103, val_acc: 0.71875
epoch: 494: train_loss: 1.0380588775933393, train_acc: 0.511904756228129, val_loss: 0.9044001698493958, val_acc: 0.796875
epoch: 495: train_loss: 1.0378804187620843, train_acc: 0.4925595323244731, val_loss: 0.9145195782184601, val_acc: 0.828125
epoch: 496: train_loss: 1.0377329007080944, train_acc: 0.4404761890570323, val_loss: 0.9269500374794006, val_acc: 0.765625
epoch: 497: train_loss: 1.0374759280059234, train_acc: 0.5446428656578064, val_loss: 0.8966706693172455, val_acc: 0.875
epoch: 498: train_loss: 1.0373046514823905, train_acc: 0.4776785671710968, val_loss: 0.9048337638378143, val_acc: 0.734375
epoch: 499: train_loss: 1.0371478414535524, train_acc: 0.4627976218859355, val_loss: 0.9584790170192719, val_acc: 0.84375
0.9493630230426788 0.59375
Accuracy of the network on the test images: 53%
F1 Score: 0.37375415282392027
precision_score: 0.2869897959183673
recall_total: 0.5357142857142857
Accuracy for class: Alligator Cracks is 0.0 %
F1 score: 0.0
precision_score: 0.0
recall_score: 0.0
Accuracy for class: Longitudinal Cracks is 0.0 %
F1 score: 0.0
precision_score: 0.0
recall_score: 0.0
Accuracy for class: Transverse Cracks is 100.0 %
F1 score: 0.37375415282392027
precision_score: 0.2869897959183673
recall_score: 0.5357142857142857
