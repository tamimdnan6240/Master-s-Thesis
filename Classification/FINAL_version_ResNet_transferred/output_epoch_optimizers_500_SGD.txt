Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Transverse Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Alligator Cracks Alligator Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.1394989490509033, train_acc: 0.3809523781140645, val_loss: 1.0905093550682068, val_acc: 0.609375
epoch: 1: train_loss: 1.1272940436999002, train_acc: 0.4181547562281291, val_loss: 1.087636411190033, val_acc: 0.5625
epoch: 2: train_loss: 1.1225172016355724, train_acc: 0.3794642885526021, val_loss: 1.0907409191131592, val_acc: 0.546875
epoch: 3: train_loss: 1.1263722777366636, train_acc: 0.3258928557236989, val_loss: 1.0905954241752625, val_acc: 0.59375
epoch: 4: train_loss: 1.1234977881113686, train_acc: 0.3616071442763011, val_loss: 1.0976118445396423, val_acc: 0.40625
epoch: 5: train_loss: 1.1252513925234475, train_acc: 0.3348214328289032, val_loss: 1.0824936032295227, val_acc: 0.625
epoch: 6: train_loss: 1.1180545375460669, train_acc: 0.4776785671710968, val_loss: 1.0841043591499329, val_acc: 0.546875
epoch: 7: train_loss: 1.1190078407526014, train_acc: 0.3645833333333333, val_loss: 1.0746055245399475, val_acc: 0.609375
epoch: 8: train_loss: 1.1177222817032424, train_acc: 0.4508928656578064, val_loss: 1.0785114765167236, val_acc: 0.53125
epoch: 9: train_loss: 1.117020817597707, train_acc: 0.3571428557236989, val_loss: 1.0885844230651855, val_acc: 0.453125
epoch: 10: train_loss: 1.1222374872727825, train_acc: 0.28125, val_loss: 1.0890238881111145, val_acc: 0.390625
epoch: 11: train_loss: 1.1200849281416998, train_acc: 0.3913690447807312, val_loss: 1.0772131085395813, val_acc: 0.53125
epoch: 12: train_loss: 1.120009055504432, train_acc: 0.3869047661622365, val_loss: 1.0830504894256592, val_acc: 0.4375
epoch: 13: train_loss: 1.1193738267535254, train_acc: 0.3571428557236989, val_loss: 1.0718874335289001, val_acc: 0.609375
epoch: 14: train_loss: 1.1214294301138983, train_acc: 0.3720238109429677, val_loss: 1.0794515013694763, val_acc: 0.46875
epoch: 15: train_loss: 1.1225770910580952, train_acc: 0.4092261989911397, val_loss: 1.0800079703330994, val_acc: 0.515625
epoch: 16: train_loss: 1.119667410850525, train_acc: 0.4642857114473979, val_loss: 1.0763061046600342, val_acc: 0.46875
epoch: 17: train_loss: 1.1216011731712907, train_acc: 0.3050595223903656, val_loss: 1.0697025656700134, val_acc: 0.5625
epoch: 18: train_loss: 1.123370783370838, train_acc: 0.3839285671710968, val_loss: 1.071395456790924, val_acc: 0.515625
epoch: 19: train_loss: 1.1238918046156567, train_acc: 0.3467261890570323, val_loss: 1.0725514888763428, val_acc: 0.5625
epoch: 20: train_loss: 1.123357027296036, train_acc: 0.3735119005044301, val_loss: 1.0579407811164856, val_acc: 0.609375
epoch: 21: train_loss: 1.1223706476616138, train_acc: 0.4330357114473979, val_loss: 1.0604386925697327, val_acc: 0.5625
epoch: 22: train_loss: 1.1212252292080203, train_acc: 0.3913690447807312, val_loss: 1.0651416182518005, val_acc: 0.515625
epoch: 23: train_loss: 1.1195672187540266, train_acc: 0.4270833333333333, val_loss: 1.0772293210029602, val_acc: 0.46875
epoch: 24: train_loss: 1.1190627320607505, train_acc: 0.3303571442763011, val_loss: 1.0813772678375244, val_acc: 0.515625
epoch: 25: train_loss: 1.1190811166396508, train_acc: 0.3958333333333333, val_loss: 1.079795479774475, val_acc: 0.421875
epoch: 26: train_loss: 1.1198140971454575, train_acc: 0.3422619054714839, val_loss: 1.0824908018112183, val_acc: 0.421875
epoch: 27: train_loss: 1.1200591978572665, train_acc: 0.3422619054714839, val_loss: 1.0653895735740662, val_acc: 0.53125
epoch: 28: train_loss: 1.1198191135779196, train_acc: 0.3720238109429677, val_loss: 1.070687174797058, val_acc: 0.515625
epoch: 29: train_loss: 1.119366110695733, train_acc: 0.3645833333333333, val_loss: 1.0612602233886719, val_acc: 0.5625
epoch: 30: train_loss: 1.1189704287436704, train_acc: 0.4017857114473979, val_loss: 1.0610932111740112, val_acc: 0.515625
epoch: 31: train_loss: 1.1178054797152681, train_acc: 0.3422619054714839, val_loss: 1.0828740000724792, val_acc: 0.46875
epoch: 32: train_loss: 1.1178805828094485, train_acc: 0.3125, val_loss: 1.075875699520111, val_acc: 0.46875
epoch: 33: train_loss: 1.1174086706311097, train_acc: 0.3497023781140645, val_loss: 1.0537593364715576, val_acc: 0.5625
epoch: 34: train_loss: 1.117101812362671, train_acc: 0.3913690447807312, val_loss: 1.0742148160934448, val_acc: 0.484375
epoch: 35: train_loss: 1.1156034193657065, train_acc: 0.4181547562281291, val_loss: 1.0778960585594177, val_acc: 0.46875
epoch: 36: train_loss: 1.1154208365861362, train_acc: 0.3556547661622365, val_loss: 1.0744418501853943, val_acc: 0.484375
epoch: 37: train_loss: 1.1154689705162721, train_acc: 0.4017857114473979, val_loss: 1.0653669834136963, val_acc: 0.515625
epoch: 38: train_loss: 1.1147684602655918, train_acc: 0.3958333333333333, val_loss: 1.06602543592453, val_acc: 0.53125
epoch: 39: train_loss: 1.1149026304483418, train_acc: 0.3318452388048172, val_loss: 1.070665717124939, val_acc: 0.515625
epoch: 40: train_loss: 1.113823740462947, train_acc: 0.4241071442763011, val_loss: 1.060640811920166, val_acc: 0.5625
epoch: 41: train_loss: 1.113844670946636, train_acc: 0.4151785721381505, val_loss: 1.0640361309051514, val_acc: 0.53125
epoch: 42: train_loss: 1.113804985386457, train_acc: 0.3482142885526021, val_loss: 1.0601994395256042, val_acc: 0.53125
epoch: 43: train_loss: 1.1140994411526306, train_acc: 0.3363095223903656, val_loss: 1.0632587671279907, val_acc: 0.5625
epoch: 44: train_loss: 1.1136283318201703, train_acc: 0.3898809552192688, val_loss: 1.0813596844673157, val_acc: 0.46875
epoch: 45: train_loss: 1.1134618399799736, train_acc: 0.3645833333333333, val_loss: 1.0437654256820679, val_acc: 0.65625
epoch: 46: train_loss: 1.1134051101427553, train_acc: 0.3690476218859355, val_loss: 1.077547550201416, val_acc: 0.421875
epoch: 47: train_loss: 1.1137069513400397, train_acc: 0.3497023781140645, val_loss: 1.0718034505844116, val_acc: 0.46875
epoch: 48: train_loss: 1.1124487657125306, train_acc: 0.4508928656578064, val_loss: 1.0441316366195679, val_acc: 0.625
epoch: 49: train_loss: 1.112423162062963, train_acc: 0.3125, val_loss: 1.0665723085403442, val_acc: 0.515625
epoch: 50: train_loss: 1.1131727886355782, train_acc: 0.3065476218859355, val_loss: 1.0441058278083801, val_acc: 0.609375
epoch: 51: train_loss: 1.112456614772479, train_acc: 0.3883928557236989, val_loss: 1.061985731124878, val_acc: 0.46875
epoch: 52: train_loss: 1.1119155692604354, train_acc: 0.3526785671710968, val_loss: 1.0543381571769714, val_acc: 0.515625
epoch: 53: train_loss: 1.112128540689563, train_acc: 0.3154761890570323, val_loss: 1.0673490762710571, val_acc: 0.53125
epoch: 54: train_loss: 1.1115948485605647, train_acc: 0.3943452338377635, val_loss: 1.0924606323242188, val_acc: 0.453125
epoch: 55: train_loss: 1.111529095896653, train_acc: 0.3779761890570323, val_loss: 1.0599026083946228, val_acc: 0.515625
epoch: 56: train_loss: 1.1116638340448082, train_acc: 0.3407738109429677, val_loss: 1.064071774482727, val_acc: 0.53125
epoch: 57: train_loss: 1.1117409878763662, train_acc: 0.3764880994955699, val_loss: 1.0678617358207703, val_acc: 0.53125
epoch: 58: train_loss: 1.112432410488021, train_acc: 0.2976190447807312, val_loss: 1.047019124031067, val_acc: 0.625
epoch: 59: train_loss: 1.1112255887852776, train_acc: 0.4389880994955699, val_loss: 1.0426982641220093, val_acc: 0.609375
epoch: 60: train_loss: 1.111020356905265, train_acc: 0.3928571442763011, val_loss: 1.0670286417007446, val_acc: 0.421875
epoch: 61: train_loss: 1.1112640759637278, train_acc: 0.3392857114473979, val_loss: 1.0436411499977112, val_acc: 0.671875
epoch: 62: train_loss: 1.1114863488409255, train_acc: 0.3244047661622365, val_loss: 1.0572078824043274, val_acc: 0.53125
epoch: 63: train_loss: 1.110763408554097, train_acc: 0.4241071442763011, val_loss: 1.0369698405265808, val_acc: 0.609375
epoch: 64: train_loss: 1.1108755866686502, train_acc: 0.3363095223903656, val_loss: 1.0561394095420837, val_acc: 0.515625
epoch: 65: train_loss: 1.1109101883690766, train_acc: 0.3556547661622365, val_loss: 1.0452317595481873, val_acc: 0.59375
epoch: 66: train_loss: 1.110361970775756, train_acc: 0.4151785721381505, val_loss: 1.047621726989746, val_acc: 0.5625
epoch: 67: train_loss: 1.109507859629743, train_acc: 0.4702380895614624, val_loss: 1.0453706979751587, val_acc: 0.5625
epoch: 68: train_loss: 1.109806342113421, train_acc: 0.2797619054714839, val_loss: 1.0452377796173096, val_acc: 0.578125
epoch: 69: train_loss: 1.109822539772306, train_acc: 0.3690476218859355, val_loss: 1.048222005367279, val_acc: 0.59375
epoch: 70: train_loss: 1.1099557375684028, train_acc: 0.3065476218859355, val_loss: 1.0344308614730835, val_acc: 0.609375
epoch: 71: train_loss: 1.1099294570309142, train_acc: 0.3973214228947957, val_loss: 1.0504188537597656, val_acc: 0.578125
epoch: 72: train_loss: 1.1100664304816013, train_acc: 0.3705357114473979, val_loss: 1.0565689206123352, val_acc: 0.5625
epoch: 73: train_loss: 1.109970473222904, train_acc: 0.3601190447807312, val_loss: 1.0700268745422363, val_acc: 0.421875
epoch: 74: train_loss: 1.1096403039826284, train_acc: 0.3779761890570323, val_loss: 1.0493725538253784, val_acc: 0.515625
epoch: 75: train_loss: 1.1091316282226327, train_acc: 0.3601190447807312, val_loss: 1.0712931156158447, val_acc: 0.46875
epoch: 76: train_loss: 1.1092706271064228, train_acc: 0.3377976218859355, val_loss: 1.0496450662612915, val_acc: 0.515625
epoch: 77: train_loss: 1.109302354419333, train_acc: 0.3675595223903656, val_loss: 1.06477290391922, val_acc: 0.46875
epoch: 78: train_loss: 1.1089812554890595, train_acc: 0.4568452338377635, val_loss: 1.030872106552124, val_acc: 0.6875
epoch: 79: train_loss: 1.108060118804375, train_acc: 0.4776785671710968, val_loss: 1.0483417510986328, val_acc: 0.53125
epoch: 80: train_loss: 1.107535849137561, train_acc: 0.3794642885526021, val_loss: 1.0249691009521484, val_acc: 0.640625
epoch: 81: train_loss: 1.1075469398401614, train_acc: 0.3348214328289032, val_loss: 1.0560413599014282, val_acc: 0.546875
epoch: 82: train_loss: 1.1073092411320848, train_acc: 0.3273809552192688, val_loss: 1.0684774518013, val_acc: 0.453125
epoch: 83: train_loss: 1.1068813571381189, train_acc: 0.3779761890570323, val_loss: 1.0357178449630737, val_acc: 0.5625
epoch: 84: train_loss: 1.1075303636345206, train_acc: 0.2767857164144516, val_loss: 1.0453801155090332, val_acc: 0.5625
epoch: 85: train_loss: 1.1070616635703299, train_acc: 0.3839285671710968, val_loss: 1.0290944576263428, val_acc: 0.5625
epoch: 86: train_loss: 1.106573830847539, train_acc: 0.4166666666666667, val_loss: 1.0374395251274109, val_acc: 0.546875
epoch: 87: train_loss: 1.1058821811368968, train_acc: 0.4226190447807312, val_loss: 1.0376306772232056, val_acc: 0.625
epoch: 88: train_loss: 1.1058122154032244, train_acc: 0.3571428557236989, val_loss: 1.0499985218048096, val_acc: 0.46875
epoch: 89: train_loss: 1.1057662652598483, train_acc: 0.3467261890570323, val_loss: 1.0565977096557617, val_acc: 0.515625
epoch: 90: train_loss: 1.105660992445963, train_acc: 0.3898809552192688, val_loss: 1.0486809611320496, val_acc: 0.46875
epoch: 91: train_loss: 1.1053636400164035, train_acc: 0.4032738109429677, val_loss: 1.0470282435417175, val_acc: 0.515625
epoch: 92: train_loss: 1.105679577183125, train_acc: 0.3020833333333333, val_loss: 1.0552505254745483, val_acc: 0.484375
epoch: 93: train_loss: 1.1060500485254514, train_acc: 0.3497023781140645, val_loss: 1.0385292768478394, val_acc: 0.5625
epoch: 94: train_loss: 1.1053001456093368, train_acc: 0.4657738109429677, val_loss: 1.0606324672698975, val_acc: 0.515625
epoch: 95: train_loss: 1.1052925042394133, train_acc: 0.3720238109429677, val_loss: 1.0338048338890076, val_acc: 0.5625
epoch: 96: train_loss: 1.1049748976615694, train_acc: 0.3571428557236989, val_loss: 1.0320706367492676, val_acc: 0.625
epoch: 97: train_loss: 1.10493466582428, train_acc: 0.2916666666666667, val_loss: 1.0423721671104431, val_acc: 0.5625
epoch: 98: train_loss: 1.1050363391738145, train_acc: 0.2827380994955699, val_loss: 1.0440052151679993, val_acc: 0.53125
epoch: 99: train_loss: 1.1050788364807762, train_acc: 0.4002976218859355, val_loss: 1.0399929285049438, val_acc: 0.640625
epoch: 100: train_loss: 1.1047486611718784, train_acc: 0.4032738109429677, val_loss: 1.0448561906814575, val_acc: 0.515625
epoch: 101: train_loss: 1.1043512351762232, train_acc: 0.4241071442763011, val_loss: 1.0316202640533447, val_acc: 0.59375
epoch: 102: train_loss: 1.1042559442010895, train_acc: 0.3303571442763011, val_loss: 1.0553451180458069, val_acc: 0.484375
epoch: 103: train_loss: 1.1038999952949007, train_acc: 0.3511904776096344, val_loss: 1.0192327499389648, val_acc: 0.671875
epoch: 104: train_loss: 1.1037086208661395, train_acc: 0.3913690447807312, val_loss: 1.0361355543136597, val_acc: 0.546875
epoch: 105: train_loss: 1.1034205842692895, train_acc: 0.3467261890570323, val_loss: 1.0516390800476074, val_acc: 0.4375
epoch: 106: train_loss: 1.1027801036834715, train_acc: 0.3928571442763011, val_loss: 1.0200750827789307, val_acc: 0.609375
epoch: 107: train_loss: 1.1024941500322316, train_acc: 0.3928571442763011, val_loss: 1.063230812549591, val_acc: 0.4375
epoch: 108: train_loss: 1.1022321160780177, train_acc: 0.3675595223903656, val_loss: 1.0627010464668274, val_acc: 0.421875
epoch: 109: train_loss: 1.102033783450271, train_acc: 0.3898809552192688, val_loss: 1.0384052991867065, val_acc: 0.53125
epoch: 110: train_loss: 1.101593019368054, train_acc: 0.3869047661622365, val_loss: 1.0268816947937012, val_acc: 0.59375
epoch: 111: train_loss: 1.101443994258131, train_acc: 0.3467261890570323, val_loss: 1.0362196564674377, val_acc: 0.546875
epoch: 112: train_loss: 1.1014783403514758, train_acc: 0.3065476218859355, val_loss: 1.024638295173645, val_acc: 0.5625
epoch: 113: train_loss: 1.101627500782236, train_acc: 0.3377976218859355, val_loss: 1.0273703336715698, val_acc: 0.53125
epoch: 114: train_loss: 1.1015146915463434, train_acc: 0.3452380994955699, val_loss: 1.0331267714500427, val_acc: 0.59375
epoch: 115: train_loss: 1.1014564472368393, train_acc: 0.3720238109429677, val_loss: 1.044128656387329, val_acc: 0.5
epoch: 116: train_loss: 1.1013503278422558, train_acc: 0.3869047661622365, val_loss: 1.031267523765564, val_acc: 0.578125
epoch: 117: train_loss: 1.10142652328405, train_acc: 0.3392857114473979, val_loss: 1.0225935578346252, val_acc: 0.65625
epoch: 118: train_loss: 1.101247611499968, train_acc: 0.3943452338377635, val_loss: 1.013064205646515, val_acc: 0.578125
epoch: 119: train_loss: 1.1014980928765403, train_acc: 0.3571428557236989, val_loss: 1.036174714565277, val_acc: 0.546875
epoch: 120: train_loss: 1.1010803641038165, train_acc: 0.40625, val_loss: 1.047884464263916, val_acc: 0.453125
epoch: 121: train_loss: 1.1011556087295866, train_acc: 0.3258928557236989, val_loss: 1.0332000255584717, val_acc: 0.5625
epoch: 122: train_loss: 1.101032437670845, train_acc: 0.3809523781140645, val_loss: 1.0451009273529053, val_acc: 0.484375
epoch: 123: train_loss: 1.1010605545454129, train_acc: 0.3154761890570323, val_loss: 1.024871826171875, val_acc: 0.625
epoch: 124: train_loss: 1.1007838320732117, train_acc: 0.3511904776096344, val_loss: 1.060265600681305, val_acc: 0.4375
epoch: 125: train_loss: 1.1005179543028434, train_acc: 0.3869047562281291, val_loss: 1.04267156124115, val_acc: 0.484375
epoch: 126: train_loss: 1.1009437085136653, train_acc: 0.2842261890570323, val_loss: 1.03364759683609, val_acc: 0.53125
epoch: 127: train_loss: 1.1009302665479481, train_acc: 0.2961309552192688, val_loss: 1.0343496203422546, val_acc: 0.5625
epoch: 128: train_loss: 1.1009444671391824, train_acc: 0.28125, val_loss: 1.0478536486625671, val_acc: 0.5625
epoch: 129: train_loss: 1.1004703330687986, train_acc: 0.3809523781140645, val_loss: 1.0570470094680786, val_acc: 0.421875
epoch: 130: train_loss: 1.1001370112404567, train_acc: 0.3809523781140645, val_loss: 1.0457594394683838, val_acc: 0.5
epoch: 131: train_loss: 1.0998570083668735, train_acc: 0.3779761890570323, val_loss: 1.0175428092479706, val_acc: 0.640625
epoch: 132: train_loss: 1.099662569381838, train_acc: 0.375, val_loss: 1.025898516178131, val_acc: 0.59375
epoch: 133: train_loss: 1.0995008422071066, train_acc: 0.3526785721381505, val_loss: 1.0177221894264221, val_acc: 0.5625
epoch: 134: train_loss: 1.0990421458526891, train_acc: 0.4389880994955699, val_loss: 1.0095671117305756, val_acc: 0.625
epoch: 135: train_loss: 1.0991174153545322, train_acc: 0.2797619054714839, val_loss: 0.9976946413516998, val_acc: 0.671875
epoch: 136: train_loss: 1.0988145338647843, train_acc: 0.3735119054714839, val_loss: 1.0109602808952332, val_acc: 0.59375
epoch: 137: train_loss: 1.0986715969832046, train_acc: 0.375, val_loss: 1.0389447212219238, val_acc: 0.53125
epoch: 138: train_loss: 1.0986158033069087, train_acc: 0.40625, val_loss: 1.0232288837432861, val_acc: 0.640625
epoch: 139: train_loss: 1.0986938391413006, train_acc: 0.3377976218859355, val_loss: 1.0197060704231262, val_acc: 0.59375
epoch: 140: train_loss: 1.0985963588347105, train_acc: 0.3452380994955699, val_loss: 1.0317489504814148, val_acc: 0.53125
epoch: 141: train_loss: 1.0984108961523975, train_acc: 0.4151785671710968, val_loss: 1.0423439741134644, val_acc: 0.484375
epoch: 142: train_loss: 1.0982461919039832, train_acc: 0.3675595223903656, val_loss: 1.0575897693634033, val_acc: 0.453125
epoch: 143: train_loss: 1.0980490891745793, train_acc: 0.3913690447807312, val_loss: 1.0314081907272339, val_acc: 0.5
epoch: 144: train_loss: 1.0976276351117538, train_acc: 0.4508928656578064, val_loss: 1.0233781337738037, val_acc: 0.59375
epoch: 145: train_loss: 1.0978016570278497, train_acc: 0.2916666666666667, val_loss: 1.0440089702606201, val_acc: 0.5625
epoch: 146: train_loss: 1.0976172718061068, train_acc: 0.3764880994955699, val_loss: 1.0441023707389832, val_acc: 0.5
epoch: 147: train_loss: 1.097570011207649, train_acc: 0.34375, val_loss: 1.0236122012138367, val_acc: 0.5625
epoch: 148: train_loss: 1.0971375150435188, train_acc: 0.4241071442763011, val_loss: 1.0237983465194702, val_acc: 0.671875
epoch: 149: train_loss: 1.097001204093297, train_acc: 0.3943452338377635, val_loss: 1.0107233822345734, val_acc: 0.59375
epoch: 150: train_loss: 1.0968330094619563, train_acc: 0.3928571442763011, val_loss: 1.038727343082428, val_acc: 0.53125
epoch: 151: train_loss: 1.0966547279242882, train_acc: 0.3720238109429677, val_loss: 1.0252436399459839, val_acc: 0.625
epoch: 152: train_loss: 1.0963394110758578, train_acc: 0.4032738109429677, val_loss: 1.011505663394928, val_acc: 0.625
epoch: 153: train_loss: 1.09585707747575, train_acc: 0.3898809552192688, val_loss: 1.026462435722351, val_acc: 0.5
epoch: 154: train_loss: 1.0959007831029992, train_acc: 0.3586309552192688, val_loss: 1.0375279188156128, val_acc: 0.625
epoch: 155: train_loss: 1.0957676074188996, train_acc: 0.3363095223903656, val_loss: 1.0100467801094055, val_acc: 0.734375
epoch: 156: train_loss: 1.0956048194769838, train_acc: 0.3511904776096344, val_loss: 1.0026435852050781, val_acc: 0.75
epoch: 157: train_loss: 1.0955359174983912, train_acc: 0.3511904776096344, val_loss: 1.041918158531189, val_acc: 0.484375
epoch: 158: train_loss: 1.0954799250986587, train_acc: 0.3660714328289032, val_loss: 1.0114020705223083, val_acc: 0.71875
epoch: 159: train_loss: 1.095406173790494, train_acc: 0.3601190447807312, val_loss: 1.003977119922638, val_acc: 0.71875
epoch: 160: train_loss: 1.0951944820382329, train_acc: 0.3392857114473979, val_loss: 1.0249736905097961, val_acc: 0.546875
epoch: 161: train_loss: 1.0947915389214027, train_acc: 0.4345238109429677, val_loss: 1.0227689146995544, val_acc: 0.625
epoch: 162: train_loss: 1.0950464375911309, train_acc: 0.3125, val_loss: 0.9986632764339447, val_acc: 0.65625
epoch: 163: train_loss: 1.0949719342274389, train_acc: 0.3616071442763011, val_loss: 1.029672384262085, val_acc: 0.59375
epoch: 164: train_loss: 1.094917091456326, train_acc: 0.3616071442763011, val_loss: 1.0194976925849915, val_acc: 0.59375
epoch: 165: train_loss: 1.0950378483557792, train_acc: 0.3482142885526021, val_loss: 0.9971012473106384, val_acc: 0.671875
epoch: 166: train_loss: 1.0948122018825504, train_acc: 0.4285714228947957, val_loss: 1.030038058757782, val_acc: 0.609375
epoch: 167: train_loss: 1.094828275461045, train_acc: 0.2901785721381505, val_loss: 1.0188122391700745, val_acc: 0.5625
epoch: 168: train_loss: 1.0946584791826774, train_acc: 0.3854166666666667, val_loss: 1.0464678704738617, val_acc: 0.59375
epoch: 169: train_loss: 1.0944620672394243, train_acc: 0.3794642885526021, val_loss: 1.0114912390708923, val_acc: 0.625
epoch: 170: train_loss: 1.0944467829449596, train_acc: 0.4136904776096344, val_loss: 1.0371859073638916, val_acc: 0.53125
epoch: 171: train_loss: 1.0940379036027326, train_acc: 0.4806547562281291, val_loss: 1.0186319947242737, val_acc: 0.53125
epoch: 172: train_loss: 1.093846215907778, train_acc: 0.4419642885526021, val_loss: 1.021192491054535, val_acc: 0.578125
epoch: 173: train_loss: 1.0933819048934508, train_acc: 0.4583333333333333, val_loss: 1.0394043028354645, val_acc: 0.5
epoch: 174: train_loss: 1.093048714342571, train_acc: 0.40625, val_loss: 1.023213803768158, val_acc: 0.609375
epoch: 175: train_loss: 1.093089798070264, train_acc: 0.3556547661622365, val_loss: 1.0237866044044495, val_acc: 0.5625
epoch: 176: train_loss: 1.0930342482308204, train_acc: 0.3467261890570323, val_loss: 1.0009238123893738, val_acc: 0.625
epoch: 177: train_loss: 1.0929048366091219, train_acc: 0.3601190447807312, val_loss: 1.0172642469406128, val_acc: 0.625
epoch: 178: train_loss: 1.0927848310887918, train_acc: 0.3824404776096344, val_loss: 0.9997934699058533, val_acc: 0.5625
epoch: 179: train_loss: 1.0926377847238817, train_acc: 0.3883928557236989, val_loss: 1.015914499759674, val_acc: 0.609375
epoch: 180: train_loss: 1.0923817485955094, train_acc: 0.4136904776096344, val_loss: 1.0228301882743835, val_acc: 0.5625
epoch: 181: train_loss: 1.0921248115681026, train_acc: 0.3779761989911397, val_loss: 0.9870595037937164, val_acc: 0.71875
epoch: 182: train_loss: 1.0922731830991244, train_acc: 0.3229166666666667, val_loss: 1.0173601508140564, val_acc: 0.640625
epoch: 183: train_loss: 1.0921890078031493, train_acc: 0.3809523781140645, val_loss: 1.018921136856079, val_acc: 0.546875
epoch: 184: train_loss: 1.0917211970767455, train_acc: 0.4880952338377635, val_loss: 0.998475968837738, val_acc: 0.671875
epoch: 185: train_loss: 1.0913127676774093, train_acc: 0.4672619005044301, val_loss: 0.9897438287734985, val_acc: 0.765625
epoch: 186: train_loss: 1.0912609097154378, train_acc: 0.3467261890570323, val_loss: 1.004709005355835, val_acc: 0.640625
epoch: 187: train_loss: 1.091113519055623, train_acc: 0.3764880994955699, val_loss: 1.0016689598560333, val_acc: 0.640625
epoch: 188: train_loss: 1.0907809190859445, train_acc: 0.4151785671710968, val_loss: 1.0396505892276764, val_acc: 0.515625
epoch: 189: train_loss: 1.0906333489376197, train_acc: 0.4092261890570323, val_loss: 0.9922785758972168, val_acc: 0.75
epoch: 190: train_loss: 1.090604246286821, train_acc: 0.3943452338377635, val_loss: 1.0073949098587036, val_acc: 0.65625
epoch: 191: train_loss: 1.090276265620357, train_acc: 0.4196428557236989, val_loss: 0.9988107085227966, val_acc: 0.671875
epoch: 192: train_loss: 1.0901614171235667, train_acc: 0.3482142885526021, val_loss: 1.0064631700515747, val_acc: 0.5625
epoch: 193: train_loss: 1.0899428038048167, train_acc: 0.46875, val_loss: 1.0179123282432556, val_acc: 0.625
epoch: 194: train_loss: 1.0897219530537594, train_acc: 0.4136904776096344, val_loss: 1.0160816311836243, val_acc: 0.546875
epoch: 195: train_loss: 1.0893346220254894, train_acc: 0.4836309552192688, val_loss: 1.0110894441604614, val_acc: 0.609375
epoch: 196: train_loss: 1.089189449240108, train_acc: 0.4226190447807312, val_loss: 0.9997544288635254, val_acc: 0.65625
epoch: 197: train_loss: 1.0888578874695578, train_acc: 0.4627976218859355, val_loss: 1.009621024131775, val_acc: 0.65625
epoch: 198: train_loss: 1.0890443559667369, train_acc: 0.3348214328289032, val_loss: 1.0239367485046387, val_acc: 0.5625
epoch: 199: train_loss: 1.0887773508826888, train_acc: 0.3898809552192688, val_loss: 1.0083225667476654, val_acc: 0.625
epoch: 200: train_loss: 1.0886391925179144, train_acc: 0.4122023781140645, val_loss: 0.9977635443210602, val_acc: 0.671875
epoch: 201: train_loss: 1.0883689991908496, train_acc: 0.4523809552192688, val_loss: 1.0035203695297241, val_acc: 0.625
epoch: 202: train_loss: 1.0881872430615038, train_acc: 0.3898809552192688, val_loss: 1.0003067553043365, val_acc: 0.609375
epoch: 203: train_loss: 1.0880482119867223, train_acc: 0.40625, val_loss: 1.0031245946884155, val_acc: 0.609375
epoch: 204: train_loss: 1.0875840189011114, train_acc: 0.4806547562281291, val_loss: 1.0012295544147491, val_acc: 0.625
epoch: 205: train_loss: 1.0875004217462632, train_acc: 0.3705357114473979, val_loss: 1.0185925364494324, val_acc: 0.546875
epoch: 206: train_loss: 1.087107964855845, train_acc: 0.4583333333333333, val_loss: 1.0207380652427673, val_acc: 0.578125
epoch: 207: train_loss: 1.0868391774785824, train_acc: 0.4627976218859355, val_loss: 1.023436814546585, val_acc: 0.59375
epoch: 208: train_loss: 1.0867817896214778, train_acc: 0.4151785671710968, val_loss: 0.9873511791229248, val_acc: 0.703125
epoch: 209: train_loss: 1.0865700367897275, train_acc: 0.4449404776096344, val_loss: 1.0166073739528656, val_acc: 0.671875
epoch: 210: train_loss: 1.0863534863714546, train_acc: 0.4449404776096344, val_loss: 1.0139311254024506, val_acc: 0.703125
epoch: 211: train_loss: 1.0860717380946536, train_acc: 0.4345238109429677, val_loss: 1.0103222727775574, val_acc: 0.578125
epoch: 212: train_loss: 1.0861949062496656, train_acc: 0.3035714328289032, val_loss: 0.9843650460243225, val_acc: 0.671875
epoch: 213: train_loss: 1.0860310933114583, train_acc: 0.3913690447807312, val_loss: 0.9802801012992859, val_acc: 0.78125
epoch: 214: train_loss: 1.085945718972258, train_acc: 0.4077380994955699, val_loss: 0.9895045459270477, val_acc: 0.703125
epoch: 215: train_loss: 1.0859716380084004, train_acc: 0.3482142885526021, val_loss: 1.0419382452964783, val_acc: 0.53125
epoch: 216: train_loss: 1.0857455872354054, train_acc: 0.4345238109429677, val_loss: 0.9682718813419342, val_acc: 0.78125
epoch: 217: train_loss: 1.0855029692890448, train_acc: 0.4538690447807312, val_loss: 0.9913897514343262, val_acc: 0.71875
epoch: 218: train_loss: 1.0853140202650016, train_acc: 0.3869047661622365, val_loss: 1.00026336312294, val_acc: 0.609375
epoch: 219: train_loss: 1.0850624096212966, train_acc: 0.4166666666666667, val_loss: 0.9885662198066711, val_acc: 0.671875
epoch: 220: train_loss: 1.0849002985393301, train_acc: 0.4211309552192688, val_loss: 0.9888589084148407, val_acc: 0.6875
epoch: 221: train_loss: 1.084782299784211, train_acc: 0.4017857114473979, val_loss: 0.9961083829402924, val_acc: 0.734375
epoch: 222: train_loss: 1.084658520995056, train_acc: 0.3630952388048172, val_loss: 0.9969300329685211, val_acc: 0.65625
epoch: 223: train_loss: 1.0843488143845685, train_acc: 0.4479166666666667, val_loss: 0.9913112819194794, val_acc: 0.6875
epoch: 224: train_loss: 1.0841730646733887, train_acc: 0.4017857114473979, val_loss: 0.9968593418598175, val_acc: 0.703125
epoch: 225: train_loss: 1.084033638238907, train_acc: 0.4017857114473979, val_loss: 1.0379203855991364, val_acc: 0.625
epoch: 226: train_loss: 1.0840221770359384, train_acc: 0.3333333333333333, val_loss: 0.9877257943153381, val_acc: 0.765625
epoch: 227: train_loss: 1.0839099372513814, train_acc: 0.3794642885526021, val_loss: 0.9833359718322754, val_acc: 0.6875
epoch: 228: train_loss: 1.0838621588426562, train_acc: 0.3616071442763011, val_loss: 0.9955534040927887, val_acc: 0.59375
epoch: 229: train_loss: 1.0841728660507481, train_acc: 0.2857142885526021, val_loss: 1.0195590555667877, val_acc: 0.65625
epoch: 230: train_loss: 1.0840364273148357, train_acc: 0.4613095323244731, val_loss: 0.9917160272598267, val_acc: 0.640625
epoch: 231: train_loss: 1.083489008676047, train_acc: 0.53125, val_loss: 1.0195394456386566, val_acc: 0.59375
epoch: 232: train_loss: 1.0833118330255602, train_acc: 0.4241071442763011, val_loss: 0.9860326051712036, val_acc: 0.6875
epoch: 233: train_loss: 1.0831351100209776, train_acc: 0.4092261890570323, val_loss: 1.0050305426120758, val_acc: 0.625
epoch: 234: train_loss: 1.0830200826022647, train_acc: 0.3839285721381505, val_loss: 0.9799270927906036, val_acc: 0.703125
epoch: 235: train_loss: 1.0828167410556884, train_acc: 0.3988095223903656, val_loss: 0.9933743476867676, val_acc: 0.71875
epoch: 236: train_loss: 1.082596331457549, train_acc: 0.4047619005044301, val_loss: 0.9987340569496155, val_acc: 0.703125
epoch: 237: train_loss: 1.0821977082420802, train_acc: 0.4970238109429677, val_loss: 0.9988039135932922, val_acc: 0.71875
epoch: 238: train_loss: 1.0819609256635323, train_acc: 0.4583333333333333, val_loss: 1.0149874687194824, val_acc: 0.671875
epoch: 239: train_loss: 1.0817323149078428, train_acc: 0.4345238109429677, val_loss: 1.010772168636322, val_acc: 0.640625
epoch: 240: train_loss: 1.0813669737938536, train_acc: 0.4479166666666667, val_loss: 1.0241256952285767, val_acc: 0.546875
epoch: 241: train_loss: 1.0811296253985945, train_acc: 0.4791666666666667, val_loss: 1.0238827764987946, val_acc: 0.609375
epoch: 242: train_loss: 1.0806985235835633, train_acc: 0.4672619005044301, val_loss: 0.99764084815979, val_acc: 0.609375
epoch: 243: train_loss: 1.0806065178145483, train_acc: 0.3616071442763011, val_loss: 0.986079066991806, val_acc: 0.671875
epoch: 244: train_loss: 1.080625767853796, train_acc: 0.3571428557236989, val_loss: 1.0122278928756714, val_acc: 0.703125
epoch: 245: train_loss: 1.080357456918009, train_acc: 0.40625, val_loss: 1.0155652463436127, val_acc: 0.5625
epoch: 246: train_loss: 1.080162080193339, train_acc: 0.4032738109429677, val_loss: 0.992900550365448, val_acc: 0.6875
epoch: 247: train_loss: 1.0798185336333455, train_acc: 0.4345238109429677, val_loss: 1.0095981359481812, val_acc: 0.609375
epoch: 248: train_loss: 1.0793086963804217, train_acc: 0.5267857114473978, val_loss: 1.006497710943222, val_acc: 0.65625
epoch: 249: train_loss: 1.079175156831742, train_acc: 0.3809523781140645, val_loss: 1.0025704205036163, val_acc: 0.734375
epoch: 250: train_loss: 1.0787705684088147, train_acc: 0.5208333333333334, val_loss: 1.0135816931724548, val_acc: 0.5625
epoch: 251: train_loss: 1.0786910557715357, train_acc: 0.3690476218859355, val_loss: 0.986527144908905, val_acc: 0.6875
epoch: 252: train_loss: 1.07846374599673, train_acc: 0.4389880895614624, val_loss: 0.9816211462020874, val_acc: 0.6875
epoch: 253: train_loss: 1.0780991320534963, train_acc: 0.4553571442763011, val_loss: 1.016745686531067, val_acc: 0.625
epoch: 254: train_loss: 1.0780622716043518, train_acc: 0.3943452338377635, val_loss: 1.0024332106113434, val_acc: 0.671875
epoch: 255: train_loss: 1.0778444389191775, train_acc: 0.4181547562281291, val_loss: 0.9844279587268829, val_acc: 0.640625
epoch: 256: train_loss: 1.0775945519813466, train_acc: 0.4583333333333333, val_loss: 1.0047570168972015, val_acc: 0.6875
epoch: 257: train_loss: 1.0775749134308925, train_acc: 0.3854166666666667, val_loss: 1.0001716017723083, val_acc: 0.640625
epoch: 258: train_loss: 1.0773755114818027, train_acc: 0.4479166666666667, val_loss: 1.0145421922206879, val_acc: 0.59375
epoch: 259: train_loss: 1.0770035483898268, train_acc: 0.5, val_loss: 0.9732739925384521, val_acc: 0.765625
epoch: 260: train_loss: 1.0768688348518027, train_acc: 0.4211309552192688, val_loss: 1.007802426815033, val_acc: 0.640625
epoch: 261: train_loss: 1.076618925912387, train_acc: 0.4419642885526021, val_loss: 0.9823453724384308, val_acc: 0.6875
epoch: 262: train_loss: 1.0763448275087457, train_acc: 0.4732142885526021, val_loss: 1.0286492705345154, val_acc: 0.625
epoch: 263: train_loss: 1.0761715790087534, train_acc: 0.4047619005044301, val_loss: 0.9789963066577911, val_acc: 0.734375
epoch: 264: train_loss: 1.0759356738636336, train_acc: 0.4598214228947957, val_loss: 1.0142121315002441, val_acc: 0.59375
epoch: 265: train_loss: 1.0757884199457963, train_acc: 0.4508928557236989, val_loss: 0.9717038869857788, val_acc: 0.78125
epoch: 266: train_loss: 1.0753563196025098, train_acc: 0.5297619104385376, val_loss: 0.9862442910671234, val_acc: 0.71875
epoch: 267: train_loss: 1.0752156861682443, train_acc: 0.3869047661622365, val_loss: 0.9626340866088867, val_acc: 0.78125
epoch: 268: train_loss: 1.0748283384457846, train_acc: 0.5089285671710968, val_loss: 0.9972650706768036, val_acc: 0.6875
epoch: 269: train_loss: 1.0743209321557747, train_acc: 0.6026785771052042, val_loss: 0.9590822160243988, val_acc: 0.734375
epoch: 270: train_loss: 1.074070467206972, train_acc: 0.4642857114473979, val_loss: 0.9769011437892914, val_acc: 0.71875
epoch: 271: train_loss: 1.0738419542709994, train_acc: 0.4642857114473979, val_loss: 0.9774489104747772, val_acc: 0.734375
epoch: 272: train_loss: 1.0737029787093881, train_acc: 0.3705357114473979, val_loss: 0.9893264770507812, val_acc: 0.6875
epoch: 273: train_loss: 1.0735186524054727, train_acc: 0.4315476218859355, val_loss: 0.9816568195819855, val_acc: 0.671875
epoch: 274: train_loss: 1.073324343507941, train_acc: 0.4196428557236989, val_loss: 1.0063681900501251, val_acc: 0.640625
epoch: 275: train_loss: 1.0731514573529155, train_acc: 0.4345238109429677, val_loss: 0.9932495355606079, val_acc: 0.65625
epoch: 276: train_loss: 1.0727216006759854, train_acc: 0.5491071343421936, val_loss: 0.989260733127594, val_acc: 0.734375
epoch: 277: train_loss: 1.072731508268156, train_acc: 0.3556547661622365, val_loss: 0.9621964693069458, val_acc: 0.765625
epoch: 278: train_loss: 1.0726341190731237, train_acc: 0.4226190447807312, val_loss: 1.002956211566925, val_acc: 0.703125
epoch: 279: train_loss: 1.0723777067803213, train_acc: 0.4553571442763011, val_loss: 0.972737193107605, val_acc: 0.78125
epoch: 280: train_loss: 1.0722542973599833, train_acc: 0.4464285671710968, val_loss: 0.9890325963497162, val_acc: 0.6875
epoch: 281: train_loss: 1.0721798923139605, train_acc: 0.3720238109429677, val_loss: 0.9768863618373871, val_acc: 0.71875
epoch: 282: train_loss: 1.0719242753072689, train_acc: 0.4672619005044301, val_loss: 0.9904031753540039, val_acc: 0.65625
epoch: 283: train_loss: 1.0716479120539957, train_acc: 0.4255952338377635, val_loss: 0.9941412806510925, val_acc: 0.65625
epoch: 284: train_loss: 1.0711752234146619, train_acc: 0.5565476218859354, val_loss: 0.9981682896614075, val_acc: 0.6875
epoch: 285: train_loss: 1.0709297545167404, train_acc: 0.46875, val_loss: 0.9757848381996155, val_acc: 0.65625
epoch: 286: train_loss: 1.0709022508542585, train_acc: 0.3348214328289032, val_loss: 0.9591974318027496, val_acc: 0.71875
epoch: 287: train_loss: 1.0707347400624454, train_acc: 0.4032738109429677, val_loss: 0.9811601340770721, val_acc: 0.671875
epoch: 288: train_loss: 1.0707045662224373, train_acc: 0.3407738109429677, val_loss: 0.9903805255889893, val_acc: 0.671875
epoch: 289: train_loss: 1.0704265762334593, train_acc: 0.4642857114473979, val_loss: 0.9613638818264008, val_acc: 0.734375
epoch: 290: train_loss: 1.0702120966927713, train_acc: 0.4598214228947957, val_loss: 0.9848279654979706, val_acc: 0.625
epoch: 291: train_loss: 1.0701019082167387, train_acc: 0.4181547661622365, val_loss: 0.99427929520607, val_acc: 0.625
epoch: 292: train_loss: 1.0699373848473532, train_acc: 0.4538690447807312, val_loss: 0.9625401794910431, val_acc: 0.8125
epoch: 293: train_loss: 1.069758179912222, train_acc: 0.3973214228947957, val_loss: 0.9879222810268402, val_acc: 0.65625
epoch: 294: train_loss: 1.0695440790747528, train_acc: 0.4345238109429677, val_loss: 0.9714561998844147, val_acc: 0.78125
epoch: 295: train_loss: 1.0692723827721844, train_acc: 0.4479166666666667, val_loss: 0.9554708003997803, val_acc: 0.8125
epoch: 296: train_loss: 1.0690369072185228, train_acc: 0.4583333333333333, val_loss: 0.984011173248291, val_acc: 0.609375
epoch: 297: train_loss: 1.0689914586560045, train_acc: 0.3809523781140645, val_loss: 0.9838553071022034, val_acc: 0.75
epoch: 298: train_loss: 1.0687259350599128, train_acc: 0.4345238109429677, val_loss: 0.9913738965988159, val_acc: 0.6875
epoch: 299: train_loss: 1.0685437944862584, train_acc: 0.4047619005044301, val_loss: 0.9426044523715973, val_acc: 0.765625
epoch: 300: train_loss: 1.068412350459749, train_acc: 0.4136904776096344, val_loss: 0.9580038189888, val_acc: 0.796875
epoch: 301: train_loss: 1.0681944059878263, train_acc: 0.4657738109429677, val_loss: 0.9414132237434387, val_acc: 0.796875
epoch: 302: train_loss: 1.067963747170368, train_acc: 0.4717261989911397, val_loss: 0.9657909274101257, val_acc: 0.75
epoch: 303: train_loss: 1.0677923688240223, train_acc: 0.4285714228947957, val_loss: 0.948178231716156, val_acc: 0.796875
epoch: 304: train_loss: 1.0677564290051909, train_acc: 0.3794642885526021, val_loss: 0.9555671811103821, val_acc: 0.828125
epoch: 305: train_loss: 1.06752101469923, train_acc: 0.4985119005044301, val_loss: 0.9848244786262512, val_acc: 0.6875
epoch: 306: train_loss: 1.067382868156889, train_acc: 0.4226190447807312, val_loss: 0.9914049208164215, val_acc: 0.625
epoch: 307: train_loss: 1.0673125717805065, train_acc: 0.3943452338377635, val_loss: 0.959793895483017, val_acc: 0.78125
epoch: 308: train_loss: 1.0673715094789595, train_acc: 0.3258928557236989, val_loss: 0.9566428363323212, val_acc: 0.734375
epoch: 309: train_loss: 1.067006744941076, train_acc: 0.4940476218859355, val_loss: 0.9909385442733765, val_acc: 0.65625
epoch: 310: train_loss: 1.0668788346574754, train_acc: 0.4181547562281291, val_loss: 0.9755703508853912, val_acc: 0.703125
epoch: 311: train_loss: 1.0665842422053349, train_acc: 0.511904756228129, val_loss: 0.9787587225437164, val_acc: 0.75
epoch: 312: train_loss: 1.0662651072802767, train_acc: 0.517857144276301, val_loss: 0.9536696374416351, val_acc: 0.75
epoch: 313: train_loss: 1.0661245055512516, train_acc: 0.4196428557236989, val_loss: 0.9781605899333954, val_acc: 0.6875
epoch: 314: train_loss: 1.0658984782834537, train_acc: 0.4389880895614624, val_loss: 0.929590106010437, val_acc: 0.796875
epoch: 315: train_loss: 1.065633589771227, train_acc: 0.5, val_loss: 0.9646348059177399, val_acc: 0.765625
epoch: 316: train_loss: 1.0655214683114547, train_acc: 0.4122023781140645, val_loss: 0.9583858549594879, val_acc: 0.765625
epoch: 317: train_loss: 1.0651437733908125, train_acc: 0.5372023781140646, val_loss: 0.991333544254303, val_acc: 0.6875
epoch: 318: train_loss: 1.064930176473337, train_acc: 0.4122023781140645, val_loss: 0.966183990240097, val_acc: 0.703125
epoch: 319: train_loss: 1.0645451867332067, train_acc: 0.5342261989911398, val_loss: 0.9894072115421295, val_acc: 0.578125
epoch: 320: train_loss: 1.0644371287349854, train_acc: 0.4122023781140645, val_loss: 0.9851893186569214, val_acc: 0.65625
epoch: 321: train_loss: 1.0642240608330846, train_acc: 0.4761904776096344, val_loss: 1.000307857990265, val_acc: 0.65625
epoch: 322: train_loss: 1.0638534413156626, train_acc: 0.5193452338377634, val_loss: 0.9468716382980347, val_acc: 0.6875
epoch: 323: train_loss: 1.0635862704289802, train_acc: 0.4672619005044301, val_loss: 0.9748353958129883, val_acc: 0.703125
epoch: 324: train_loss: 1.0631363690205118, train_acc: 0.5565476218859354, val_loss: 0.9640565514564514, val_acc: 0.671875
epoch: 325: train_loss: 1.0629017193380803, train_acc: 0.5029761989911398, val_loss: 0.9387176632881165, val_acc: 0.78125
epoch: 326: train_loss: 1.0626946488408633, train_acc: 0.4836309552192688, val_loss: 0.9691829681396484, val_acc: 0.75
epoch: 327: train_loss: 1.06262001116586, train_acc: 0.3779761890570323, val_loss: 0.9734995365142822, val_acc: 0.75
epoch: 328: train_loss: 1.0624217556481916, train_acc: 0.4747023781140645, val_loss: 0.9461311101913452, val_acc: 0.765625
epoch: 329: train_loss: 1.0623892283800884, train_acc: 0.3720238109429677, val_loss: 0.9576191902160645, val_acc: 0.71875
epoch: 330: train_loss: 1.0619767826849609, train_acc: 0.5461309552192688, val_loss: 0.9549329578876495, val_acc: 0.734375
epoch: 331: train_loss: 1.061796684162206, train_acc: 0.4553571442763011, val_loss: 0.9875147640705109, val_acc: 0.640625
epoch: 332: train_loss: 1.061547535138804, train_acc: 0.4895833333333333, val_loss: 0.9417144656181335, val_acc: 0.78125
epoch: 333: train_loss: 1.061369574533966, train_acc: 0.4345238109429677, val_loss: 0.9584330320358276, val_acc: 0.734375
epoch: 334: train_loss: 1.0613004617430093, train_acc: 0.4136904776096344, val_loss: 0.9622925221920013, val_acc: 0.75
epoch: 335: train_loss: 1.0611901081625439, train_acc: 0.4345238109429677, val_loss: 0.9810348749160767, val_acc: 0.671875
epoch: 336: train_loss: 1.0609886046686945, train_acc: 0.4360119005044301, val_loss: 0.9543056488037109, val_acc: 0.703125
epoch: 337: train_loss: 1.060975393658795, train_acc: 0.3467261890570323, val_loss: 0.9581160247325897, val_acc: 0.8125
epoch: 338: train_loss: 1.0606463820533423, train_acc: 0.5476190447807312, val_loss: 0.9501938819885254, val_acc: 0.78125
epoch: 339: train_loss: 1.0605411264826279, train_acc: 0.4196428557236989, val_loss: 0.9454241991043091, val_acc: 0.8125
epoch: 340: train_loss: 1.0602085871780382, train_acc: 0.5029761989911398, val_loss: 0.9427147209644318, val_acc: 0.796875
epoch: 341: train_loss: 1.060115462332442, train_acc: 0.4196428557236989, val_loss: 0.9705337882041931, val_acc: 0.75
epoch: 342: train_loss: 1.0600274925792654, train_acc: 0.4017857114473979, val_loss: 0.9718469977378845, val_acc: 0.65625
epoch: 343: train_loss: 1.0598755064167726, train_acc: 0.4196428557236989, val_loss: 0.9342789947986603, val_acc: 0.796875
epoch: 344: train_loss: 1.059653798335993, train_acc: 0.4449404776096344, val_loss: 0.9831435680389404, val_acc: 0.625
epoch: 345: train_loss: 1.05935123143628, train_acc: 0.5372023781140646, val_loss: 0.9506548047065735, val_acc: 0.734375
epoch: 346: train_loss: 1.0590251598875768, train_acc: 0.5357142885526022, val_loss: 0.9476181864738464, val_acc: 0.734375
epoch: 347: train_loss: 1.0588438413837413, train_acc: 0.4494047562281291, val_loss: 0.9367000758647919, val_acc: 0.75
epoch: 348: train_loss: 1.0586465844793793, train_acc: 0.4672619005044301, val_loss: 0.9739187657833099, val_acc: 0.734375
epoch: 349: train_loss: 1.0584376883506785, train_acc: 0.4360119005044301, val_loss: 0.9421773254871368, val_acc: 0.765625
epoch: 350: train_loss: 1.0579901529513203, train_acc: 0.5892857114473978, val_loss: 0.9517219662666321, val_acc: 0.671875
epoch: 351: train_loss: 1.057960969029052, train_acc: 0.3779761890570323, val_loss: 0.9650611579418182, val_acc: 0.703125
epoch: 352: train_loss: 1.0576874350916121, train_acc: 0.5133928656578064, val_loss: 0.9438043832778931, val_acc: 0.765625
epoch: 353: train_loss: 1.0575987535569187, train_acc: 0.3616071442763011, val_loss: 0.9844608902931213, val_acc: 0.703125
epoch: 354: train_loss: 1.057327535790458, train_acc: 0.5208333333333334, val_loss: 0.9668612480163574, val_acc: 0.671875
epoch: 355: train_loss: 1.057145993696171, train_acc: 0.4851190447807312, val_loss: 0.9673589468002319, val_acc: 0.6875
epoch: 356: train_loss: 1.0570141400387094, train_acc: 0.4508928656578064, val_loss: 0.9465760290622711, val_acc: 0.765625
epoch: 357: train_loss: 1.0567702597191002, train_acc: 0.5104166666666666, val_loss: 0.9796359837055206, val_acc: 0.6875
epoch: 358: train_loss: 1.0564211574880313, train_acc: 0.5476190447807312, val_loss: 0.9490182399749756, val_acc: 0.75
epoch: 359: train_loss: 1.05624008719568, train_acc: 0.4598214228947957, val_loss: 0.9393339157104492, val_acc: 0.765625
epoch: 360: train_loss: 1.0560763893805503, train_acc: 0.4672619005044301, val_loss: 0.9469543397426605, val_acc: 0.796875
epoch: 361: train_loss: 1.0559254493941708, train_acc: 0.4568452338377635, val_loss: 0.9362537562847137, val_acc: 0.734375
epoch: 362: train_loss: 1.0558549140548366, train_acc: 0.3779761890570323, val_loss: 0.9235763251781464, val_acc: 0.765625
epoch: 363: train_loss: 1.0555638111991332, train_acc: 0.4940476218859355, val_loss: 0.9998809695243835, val_acc: 0.640625
epoch: 364: train_loss: 1.0554007529123743, train_acc: 0.4211309552192688, val_loss: 0.9658779501914978, val_acc: 0.765625
epoch: 365: train_loss: 1.055241368507428, train_acc: 0.4419642885526021, val_loss: 0.9503588378429413, val_acc: 0.765625
epoch: 366: train_loss: 1.055091303928023, train_acc: 0.4389880895614624, val_loss: 0.947566419839859, val_acc: 0.828125
epoch: 367: train_loss: 1.0548511097828557, train_acc: 0.5059523781140646, val_loss: 0.9528019726276398, val_acc: 0.734375
epoch: 368: train_loss: 1.0547064007028557, train_acc: 0.4434523781140645, val_loss: 0.9570671021938324, val_acc: 0.6875
epoch: 369: train_loss: 1.0545063338301213, train_acc: 0.5089285671710968, val_loss: 0.9094932675361633, val_acc: 0.859375
epoch: 370: train_loss: 1.054286084620673, train_acc: 0.538690467675527, val_loss: 0.9348806738853455, val_acc: 0.796875
epoch: 371: train_loss: 1.0541258148822317, train_acc: 0.4508928656578064, val_loss: 0.9598460793495178, val_acc: 0.78125
epoch: 372: train_loss: 1.0538501264369315, train_acc: 0.5104166666666666, val_loss: 0.9368787705898285, val_acc: 0.8125
epoch: 373: train_loss: 1.0535431065980134, train_acc: 0.5327380895614624, val_loss: 0.9293112456798553, val_acc: 0.75
epoch: 374: train_loss: 1.053227990468344, train_acc: 0.5520833333333334, val_loss: 0.9200271666049957, val_acc: 0.8125
epoch: 375: train_loss: 1.0530898499679067, train_acc: 0.4613095323244731, val_loss: 0.9607396125793457, val_acc: 0.703125
epoch: 376: train_loss: 1.0529807628940218, train_acc: 0.4315476218859355, val_loss: 0.9459219574928284, val_acc: 0.75
epoch: 377: train_loss: 1.052800780198352, train_acc: 0.4434523781140645, val_loss: 0.9911126494407654, val_acc: 0.625
epoch: 378: train_loss: 1.0526377090048988, train_acc: 0.4657738109429677, val_loss: 0.9666795432567596, val_acc: 0.71875
epoch: 379: train_loss: 1.0525051919514683, train_acc: 0.40625, val_loss: 0.9323307275772095, val_acc: 0.75
epoch: 380: train_loss: 1.0523059390035523, train_acc: 0.46875, val_loss: 0.9663426280021667, val_acc: 0.6875
epoch: 381: train_loss: 1.0519923526683204, train_acc: 0.5580357114473978, val_loss: 0.9288002252578735, val_acc: 0.75
epoch: 382: train_loss: 1.0519345282885806, train_acc: 0.4107142885526021, val_loss: 0.940596878528595, val_acc: 0.71875
epoch: 383: train_loss: 1.051601222095391, train_acc: 0.555059532324473, val_loss: 0.947075217962265, val_acc: 0.75
epoch: 384: train_loss: 1.0514733095189719, train_acc: 0.4568452338377635, val_loss: 0.9631958603858948, val_acc: 0.71875
epoch: 385: train_loss: 1.05128755177241, train_acc: 0.4523809552192688, val_loss: 0.9748061299324036, val_acc: 0.671875
epoch: 386: train_loss: 1.0510537799938713, train_acc: 0.5074404776096344, val_loss: 0.9538325667381287, val_acc: 0.71875
epoch: 387: train_loss: 1.0508689023170283, train_acc: 0.4761904776096344, val_loss: 0.9547896087169647, val_acc: 0.71875
epoch: 388: train_loss: 1.0508265164333637, train_acc: 0.4345238109429677, val_loss: 0.9637216925621033, val_acc: 0.703125
epoch: 389: train_loss: 1.0506493376361008, train_acc: 0.4598214228947957, val_loss: 0.9211987555027008, val_acc: 0.8125
epoch: 390: train_loss: 1.0505239810581817, train_acc: 0.4434523781140645, val_loss: 0.919506311416626, val_acc: 0.796875
epoch: 391: train_loss: 1.0503661224529866, train_acc: 0.4568452338377635, val_loss: 0.9248991906642914, val_acc: 0.828125
epoch: 392: train_loss: 1.05027625487961, train_acc: 0.4032738109429677, val_loss: 0.9290592074394226, val_acc: 0.765625
epoch: 393: train_loss: 1.0501850641721808, train_acc: 0.4270833333333333, val_loss: 0.9312683343887329, val_acc: 0.765625
epoch: 394: train_loss: 1.0499768210362792, train_acc: 0.4985119005044301, val_loss: 0.9383607804775238, val_acc: 0.78125
epoch: 395: train_loss: 1.0495680149657163, train_acc: 0.6279761989911398, val_loss: 0.9448893964290619, val_acc: 0.765625
epoch: 396: train_loss: 1.0493790908184106, train_acc: 0.4523809552192688, val_loss: 0.9385949671268463, val_acc: 0.703125
epoch: 397: train_loss: 1.0492313869634473, train_acc: 0.4568452338377635, val_loss: 0.9196979105472565, val_acc: 0.859375
epoch: 398: train_loss: 1.04908669721911, train_acc: 0.4880952338377635, val_loss: 0.9288789331912994, val_acc: 0.8125
epoch: 399: train_loss: 1.0488565444946296, train_acc: 0.4895833333333333, val_loss: 0.9541380405426025, val_acc: 0.78125
epoch: 400: train_loss: 1.0485157493443074, train_acc: 0.5729166666666666, val_loss: 0.9525784254074097, val_acc: 0.765625
epoch: 401: train_loss: 1.0483455731105655, train_acc: 0.4568452338377635, val_loss: 0.9165776669979095, val_acc: 0.78125
epoch: 402: train_loss: 1.0480539358973608, train_acc: 0.5163690447807312, val_loss: 0.9408684074878693, val_acc: 0.828125
epoch: 403: train_loss: 1.0479119980689329, train_acc: 0.4598214228947957, val_loss: 0.9486158788204193, val_acc: 0.71875
epoch: 404: train_loss: 1.0477978906023167, train_acc: 0.4345238109429677, val_loss: 0.9492630362510681, val_acc: 0.6875
epoch: 405: train_loss: 1.047487322479634, train_acc: 0.5416666666666666, val_loss: 0.9242661595344543, val_acc: 0.765625
epoch: 406: train_loss: 1.0472670250696599, train_acc: 0.4523809552192688, val_loss: 0.9360118210315704, val_acc: 0.765625
epoch: 407: train_loss: 1.0472228950242597, train_acc: 0.3928571442763011, val_loss: 0.9721696376800537, val_acc: 0.65625
epoch: 408: train_loss: 1.0469447309735747, train_acc: 0.5401785671710968, val_loss: 0.9508109986782074, val_acc: 0.765625
epoch: 409: train_loss: 1.0467076702815736, train_acc: 0.4851190447807312, val_loss: 0.9639054834842682, val_acc: 0.71875
epoch: 410: train_loss: 1.0464864990523655, train_acc: 0.5357142885526022, val_loss: 0.9209820032119751, val_acc: 0.78125
epoch: 411: train_loss: 1.0464001644680034, train_acc: 0.4672619005044301, val_loss: 0.9408049285411835, val_acc: 0.71875
epoch: 412: train_loss: 1.046267070387332, train_acc: 0.4464285671710968, val_loss: 0.9142899811267853, val_acc: 0.8125
epoch: 413: train_loss: 1.046026050948483, train_acc: 0.4776785671710968, val_loss: 0.9333527386188507, val_acc: 0.765625
epoch: 414: train_loss: 1.0458936022467409, train_acc: 0.4717261989911397, val_loss: 0.9345146417617798, val_acc: 0.78125
epoch: 415: train_loss: 1.0456922047126758, train_acc: 0.5133928656578064, val_loss: 0.919535219669342, val_acc: 0.859375
epoch: 416: train_loss: 1.0454439191128335, train_acc: 0.4836309552192688, val_loss: 0.9045714139938354, val_acc: 0.8125
epoch: 417: train_loss: 1.0452456515656712, train_acc: 0.511904756228129, val_loss: 0.930667370557785, val_acc: 0.8125
epoch: 418: train_loss: 1.045070605374749, train_acc: 0.4821428656578064, val_loss: 0.9454026818275452, val_acc: 0.71875
epoch: 419: train_loss: 1.044834891718532, train_acc: 0.5297619005044302, val_loss: 0.9162346422672272, val_acc: 0.796875
epoch: 420: train_loss: 1.0448537723751936, train_acc: 0.3809523781140645, val_loss: 0.9227626025676727, val_acc: 0.78125
epoch: 421: train_loss: 1.0446228077061375, train_acc: 0.5297619005044302, val_loss: 0.9436750113964081, val_acc: 0.796875
epoch: 422: train_loss: 1.044363302823782, train_acc: 0.5595238010088602, val_loss: 0.9161828756332397, val_acc: 0.765625
epoch: 423: train_loss: 1.0441522658246125, train_acc: 0.4747023781140645, val_loss: 0.9449523985385895, val_acc: 0.78125
epoch: 424: train_loss: 1.043985077960819, train_acc: 0.5074404776096344, val_loss: 0.9459503591060638, val_acc: 0.71875
epoch: 425: train_loss: 1.0435409958653614, train_acc: 0.6473214228947958, val_loss: 0.9011431634426117, val_acc: 0.84375
epoch: 426: train_loss: 1.043269845305897, train_acc: 0.5327380895614624, val_loss: 0.9261118471622467, val_acc: 0.765625
epoch: 427: train_loss: 1.0429948770275748, train_acc: 0.5208333333333334, val_loss: 0.9551737606525421, val_acc: 0.703125
epoch: 428: train_loss: 1.0427083533775379, train_acc: 0.555059532324473, val_loss: 0.9316147863864899, val_acc: 0.765625
epoch: 429: train_loss: 1.0424184840316928, train_acc: 0.5580357114473978, val_loss: 0.9015189111232758, val_acc: 0.859375
epoch: 430: train_loss: 1.0421959440651007, train_acc: 0.4985119005044301, val_loss: 0.9330125749111176, val_acc: 0.78125
epoch: 431: train_loss: 1.0422136365539505, train_acc: 0.3869047661622365, val_loss: 0.9686594307422638, val_acc: 0.625
epoch: 432: train_loss: 1.0421217819009045, train_acc: 0.4226190447807312, val_loss: 0.8949677646160126, val_acc: 0.8125
epoch: 433: train_loss: 1.0418885228912227, train_acc: 0.5223214228947958, val_loss: 0.9155420958995819, val_acc: 0.828125
epoch: 434: train_loss: 1.0416564571446394, train_acc: 0.4985119104385376, val_loss: 0.905635803937912, val_acc: 0.84375
epoch: 435: train_loss: 1.0414860110946393, train_acc: 0.4702380895614624, val_loss: 0.9101322591304779, val_acc: 0.8125
epoch: 436: train_loss: 1.0412838764994503, train_acc: 0.4985119005044301, val_loss: 0.8847991228103638, val_acc: 0.796875
epoch: 437: train_loss: 1.0410674730120193, train_acc: 0.511904756228129, val_loss: 0.9724294543266296, val_acc: 0.65625
epoch: 438: train_loss: 1.040828546567134, train_acc: 0.5148809552192688, val_loss: 0.9074249565601349, val_acc: 0.78125
epoch: 439: train_loss: 1.0407152492891667, train_acc: 0.4449404776096344, val_loss: 0.9357759058475494, val_acc: 0.859375
epoch: 440: train_loss: 1.0405267822138764, train_acc: 0.4925595323244731, val_loss: 0.9281669557094574, val_acc: 0.796875
epoch: 441: train_loss: 1.0403872753520185, train_acc: 0.4821428656578064, val_loss: 0.9111839830875397, val_acc: 0.765625
epoch: 442: train_loss: 1.0402565137437636, train_acc: 0.4479166666666667, val_loss: 0.9220729172229767, val_acc: 0.84375
epoch: 443: train_loss: 1.0401324749261418, train_acc: 0.4345238109429677, val_loss: 0.9265090823173523, val_acc: 0.78125
epoch: 444: train_loss: 1.0399270959114768, train_acc: 0.4955357114473979, val_loss: 0.9094779193401337, val_acc: 0.8125
epoch: 445: train_loss: 1.0398061034034372, train_acc: 0.4464285671710968, val_loss: 0.9238075613975525, val_acc: 0.78125
epoch: 446: train_loss: 1.0395524730262793, train_acc: 0.523809532324473, val_loss: 0.9284514784812927, val_acc: 0.796875
epoch: 447: train_loss: 1.0393881443887956, train_acc: 0.4851190447807312, val_loss: 0.913162112236023, val_acc: 0.78125
epoch: 448: train_loss: 1.039198991362042, train_acc: 0.4836309552192688, val_loss: 0.9088494777679443, val_acc: 0.765625
epoch: 449: train_loss: 1.0390884948218317, train_acc: 0.4613095323244731, val_loss: 0.9319983124732971, val_acc: 0.765625
epoch: 450: train_loss: 1.0389988212257697, train_acc: 0.3988095223903656, val_loss: 0.9464591145515442, val_acc: 0.71875
epoch: 451: train_loss: 1.038705784135161, train_acc: 0.5491071343421936, val_loss: 0.9451753497123718, val_acc: 0.765625
epoch: 452: train_loss: 1.0384317982538918, train_acc: 0.5446428656578064, val_loss: 0.8984934091567993, val_acc: 0.84375
epoch: 453: train_loss: 1.0382145227052049, train_acc: 0.5104166666666666, val_loss: 0.932815283536911, val_acc: 0.765625
epoch: 454: train_loss: 1.0380825181147126, train_acc: 0.4122023781140645, val_loss: 0.9142519533634186, val_acc: 0.796875
epoch: 455: train_loss: 1.0378750693379795, train_acc: 0.5, val_loss: 0.9371842741966248, val_acc: 0.703125
epoch: 456: train_loss: 1.0376555630101685, train_acc: 0.555059532324473, val_loss: 0.9114963710308075, val_acc: 0.765625
epoch: 457: train_loss: 1.0374032622515035, train_acc: 0.5357142885526022, val_loss: 0.9166519641876221, val_acc: 0.75
epoch: 458: train_loss: 1.0372036295086045, train_acc: 0.4940476218859355, val_loss: 0.8787769675254822, val_acc: 0.84375
epoch: 459: train_loss: 1.0369844163673518, train_acc: 0.5282738010088602, val_loss: 0.9349661767482758, val_acc: 0.796875
epoch: 460: train_loss: 1.036790973624715, train_acc: 0.4985119005044301, val_loss: 0.950883686542511, val_acc: 0.671875
epoch: 461: train_loss: 1.0366765476150437, train_acc: 0.4285714328289032, val_loss: 0.9186792373657227, val_acc: 0.75
epoch: 462: train_loss: 1.0365223609946295, train_acc: 0.4806547562281291, val_loss: 0.9162347614765167, val_acc: 0.734375
epoch: 463: train_loss: 1.0363446306651358, train_acc: 0.4747023781140645, val_loss: 0.9133281111717224, val_acc: 0.828125
epoch: 464: train_loss: 1.0360739616510268, train_acc: 0.5163690447807312, val_loss: 0.8913997411727905, val_acc: 0.859375
epoch: 465: train_loss: 1.0358238842695404, train_acc: 0.4985119005044301, val_loss: 0.920389860868454, val_acc: 0.8125
epoch: 466: train_loss: 1.0357116973459686, train_acc: 0.4419642885526021, val_loss: 0.9189761281013489, val_acc: 0.78125
epoch: 467: train_loss: 1.03549304218204, train_acc: 0.5104166666666666, val_loss: 0.9231486320495605, val_acc: 0.71875
epoch: 468: train_loss: 1.0353999379770766, train_acc: 0.4211309552192688, val_loss: 0.9116387367248535, val_acc: 0.796875
epoch: 469: train_loss: 1.0351959178633732, train_acc: 0.5029761989911398, val_loss: 0.9659766256809235, val_acc: 0.65625
epoch: 470: train_loss: 1.0349846019107072, train_acc: 0.4761904776096344, val_loss: 0.9118239283561707, val_acc: 0.765625
epoch: 471: train_loss: 1.0347344665288258, train_acc: 0.5535714228947958, val_loss: 0.9007294476032257, val_acc: 0.8125
epoch: 472: train_loss: 1.0345560374858103, train_acc: 0.5163690447807312, val_loss: 0.9180768728256226, val_acc: 0.796875
epoch: 473: train_loss: 1.0343051673909958, train_acc: 0.523809532324473, val_loss: 0.8703339099884033, val_acc: 0.796875
epoch: 474: train_loss: 1.0340298991454284, train_acc: 0.569940467675527, val_loss: 0.8905681073665619, val_acc: 0.75
epoch: 475: train_loss: 1.0338343633573606, train_acc: 0.4761904776096344, val_loss: 0.9166847467422485, val_acc: 0.734375
epoch: 476: train_loss: 1.0336091724581697, train_acc: 0.5476190447807312, val_loss: 0.8809002339839935, val_acc: 0.78125
epoch: 477: train_loss: 1.0333370216388258, train_acc: 0.581845243771871, val_loss: 0.8579717874526978, val_acc: 0.859375
epoch: 478: train_loss: 1.0331595528847497, train_acc: 0.4568452338377635, val_loss: 0.8816842436790466, val_acc: 0.796875
epoch: 479: train_loss: 1.0327726253204885, train_acc: 0.625, val_loss: 0.8947092890739441, val_acc: 0.8125
epoch: 480: train_loss: 1.0325863439014045, train_acc: 0.5044642885526022, val_loss: 0.9621895849704742, val_acc: 0.671875
epoch: 481: train_loss: 1.032412261389104, train_acc: 0.4880952338377635, val_loss: 0.9138542115688324, val_acc: 0.84375
epoch: 482: train_loss: 1.0321126548564383, train_acc: 0.5372023781140646, val_loss: 0.9286198914051056, val_acc: 0.671875
epoch: 483: train_loss: 1.031756462924409, train_acc: 0.6383928656578064, val_loss: 0.9225381910800934, val_acc: 0.78125
epoch: 484: train_loss: 1.0315544114489743, train_acc: 0.5163690447807312, val_loss: 0.8667949736118317, val_acc: 0.890625
epoch: 485: train_loss: 1.0313864246889732, train_acc: 0.4747023781140645, val_loss: 0.8930656015872955, val_acc: 0.75
epoch: 486: train_loss: 1.0311595969702763, train_acc: 0.5104166666666666, val_loss: 0.8792117536067963, val_acc: 0.875
epoch: 487: train_loss: 1.0310145840455933, train_acc: 0.4211309552192688, val_loss: 0.9270623028278351, val_acc: 0.6875
epoch: 488: train_loss: 1.0308273633727436, train_acc: 0.4880952338377635, val_loss: 0.8677817583084106, val_acc: 0.859375
epoch: 489: train_loss: 1.0306002057733998, train_acc: 0.5252976218859354, val_loss: 0.8890111744403839, val_acc: 0.84375
epoch: 490: train_loss: 1.0304268579538072, train_acc: 0.4761904776096344, val_loss: 0.8439071476459503, val_acc: 0.875
epoch: 491: train_loss: 1.0301525831868663, train_acc: 0.5520833333333334, val_loss: 0.9408592283725739, val_acc: 0.734375
epoch: 492: train_loss: 1.0300217906795224, train_acc: 0.4494047562281291, val_loss: 0.8802661001682281, val_acc: 0.859375
epoch: 493: train_loss: 1.0297981671154426, train_acc: 0.5342261989911398, val_loss: 0.9128560721874237, val_acc: 0.734375
epoch: 494: train_loss: 1.0296182376367082, train_acc: 0.4657738109429677, val_loss: 0.8787831664085388, val_acc: 0.8125
epoch: 495: train_loss: 1.029459513683794, train_acc: 0.4806547562281291, val_loss: 0.892703115940094, val_acc: 0.765625
epoch: 496: train_loss: 1.0292416241727051, train_acc: 0.5044642885526022, val_loss: 0.8903452157974243, val_acc: 0.796875
epoch: 497: train_loss: 1.0290184633798873, train_acc: 0.5014880895614624, val_loss: 0.9199026823043823, val_acc: 0.734375
epoch: 498: train_loss: 1.028844690673258, train_acc: 0.4494047562281291, val_loss: 0.937939316034317, val_acc: 0.65625
epoch: 499: train_loss: 1.0286941760778434, train_acc: 0.4434523781140645, val_loss: 0.8892144560813904, val_acc: 0.84375
0.9144509136676788 0.640625
GroundTruth:  Alligator Cracks Alligator Cracks Alligator Cracks Alligator Cracks
Accuracy of the network on the test images: 64 %
Accuracy for class: Alligator Cracks is 0.0 %
Accuracy for class: Longitudinal Cracks is 19.0 %
Accuracy for class: Transverse Cracks is 100.0 %
