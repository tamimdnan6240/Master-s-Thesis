cpu
Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Alligator Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Transverse Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.2033617893854778, train_acc: 0.2380952388048172, val_loss: 1.066470205783844, val_acc: 0.40625
epoch: 1: train_loss: 1.1909266908963523, train_acc: 0.4508928557236989, val_loss: 1.1291286945343018, val_acc: 0.40625
epoch: 2: train_loss: 1.1560314695040386, train_acc: 0.4791666666666667, val_loss: 1.1369611024856567, val_acc: 0.40625
epoch: 3: train_loss: 1.1539500306049983, train_acc: 0.4360119005044301, val_loss: 1.0899140238761902, val_acc: 0.453125
epoch: 4: train_loss: 1.138364311059316, train_acc: 0.5372023781140646, val_loss: 1.1304325461387634, val_acc: 0.40625
epoch: 5: train_loss: 1.1407349507013957, train_acc: 0.3779761890570323, val_loss: 1.016136646270752, val_acc: 0.40625
epoch: 6: train_loss: 1.119254143465133, train_acc: 0.4613095323244731, val_loss: 0.9756135940551758, val_acc: 0.625
epoch: 7: train_loss: 1.095750908056895, train_acc: 0.4196428557236989, val_loss: 0.9281303286552429, val_acc: 0.515625
epoch: 8: train_loss: 1.0798745111182884, train_acc: 0.4032738109429677, val_loss: 0.9193629920482635, val_acc: 0.671875
epoch: 9: train_loss: 1.0541990498701732, train_acc: 0.5565476218859354, val_loss: 0.8889289498329163, val_acc: 0.734375
epoch: 10: train_loss: 1.0348230022372622, train_acc: 0.5148809552192688, val_loss: 0.8229805827140808, val_acc: 0.75
epoch: 11: train_loss: 1.0114066352446873, train_acc: 0.5461309552192688, val_loss: 0.7741813361644745, val_acc: 0.765625
epoch: 12: train_loss: 0.9962582832727678, train_acc: 0.4985119005044301, val_loss: 0.7128100693225861, val_acc: 0.890625
epoch: 13: train_loss: 0.9772839418479374, train_acc: 0.5684523781140646, val_loss: 0.7634355425834656, val_acc: 0.828125
epoch: 14: train_loss: 0.9595400770505269, train_acc: 0.5461309552192688, val_loss: 0.7170131206512451, val_acc: 0.859375
epoch: 15: train_loss: 0.9477483754356703, train_acc: 0.5, val_loss: 0.6462995707988739, val_acc: 0.96875
epoch: 16: train_loss: 0.9342120967659296, train_acc: 0.5342261989911398, val_loss: 0.7506132125854492, val_acc: 0.8125
epoch: 17: train_loss: 0.9242991573280759, train_acc: 0.46875, val_loss: 0.7103475630283356, val_acc: 0.8125
epoch: 18: train_loss: 0.9187063522506178, train_acc: 0.4047619005044301, val_loss: 0.6996695399284363, val_acc: 0.8125
epoch: 19: train_loss: 0.9083086440960566, train_acc: 0.5654761989911398, val_loss: 0.7309566140174866, val_acc: 0.84375
epoch: 20: train_loss: 0.9004006556102208, train_acc: 0.5327380895614624, val_loss: 0.6494212448596954, val_acc: 0.90625
epoch: 21: train_loss: 0.8923650266546193, train_acc: 0.5342261989911398, val_loss: 0.7306489050388336, val_acc: 0.8125
epoch: 22: train_loss: 0.8851504541825558, train_acc: 0.4836309552192688, val_loss: 0.6362832188606262, val_acc: 0.921875
epoch: 23: train_loss: 0.8789632485972511, train_acc: 0.519345243771871, val_loss: 0.6410458087921143, val_acc: 0.875
epoch: 24: train_loss: 0.8727477900187175, train_acc: 0.4732142885526021, val_loss: 0.667903870344162, val_acc: 0.875
epoch: 25: train_loss: 0.8636139807028649, train_acc: 0.569940467675527, val_loss: 0.6300780177116394, val_acc: 0.90625
epoch: 26: train_loss: 0.8560027325594867, train_acc: 0.5684523781140646, val_loss: 0.5748775601387024, val_acc: 1.0
epoch: 27: train_loss: 0.8531870870363144, train_acc: 0.4747023781140645, val_loss: 0.5809783637523651, val_acc: 1.0
epoch: 28: train_loss: 0.8454063294947832, train_acc: 0.5952380895614624, val_loss: 0.6809688210487366, val_acc: 0.8125
epoch: 29: train_loss: 0.8403559327125549, train_acc: 0.5520833333333334, val_loss: 0.7501153349876404, val_acc: 0.75
epoch: 30: train_loss: 0.8357532184611084, train_acc: 0.543154756228129, val_loss: 0.6244947612285614, val_acc: 0.921875
epoch: 31: train_loss: 0.830662186567982, train_acc: 0.543154756228129, val_loss: 0.5748237669467926, val_acc: 0.984375
epoch: 32: train_loss: 0.8281717348580407, train_acc: 0.4642857114473979, val_loss: 0.6339032053947449, val_acc: 0.921875
epoch: 33: train_loss: 0.8269228911867328, train_acc: 0.4330357114473979, val_loss: 0.5889476239681244, val_acc: 0.96875
epoch: 34: train_loss: 0.8231040613991872, train_acc: 0.523809532324473, val_loss: 0.6050677299499512, val_acc: 0.96875
epoch: 35: train_loss: 0.8205786833056696, train_acc: 0.4866071442763011, val_loss: 0.5899842381477356, val_acc: 0.984375
epoch: 36: train_loss: 0.8148874108855788, train_acc: 0.6160714228947958, val_loss: 0.5740521252155304, val_acc: 0.984375
epoch: 37: train_loss: 0.8105562287464475, train_acc: 0.5773809552192688, val_loss: 0.5775533020496368, val_acc: 1.0
epoch: 38: train_loss: 0.8075259526570637, train_acc: 0.5089285671710968, val_loss: 0.5758731365203857, val_acc: 0.984375
epoch: 39: train_loss: 0.8044547870755195, train_acc: 0.5386904776096344, val_loss: 0.5996391773223877, val_acc: 0.96875
epoch: 40: train_loss: 0.8006602569324214, train_acc: 0.5833333333333334, val_loss: 0.5679412186145782, val_acc: 0.984375
epoch: 41: train_loss: 0.7977390700862521, train_acc: 0.5535714228947958, val_loss: 0.6055544316768646, val_acc: 0.921875
epoch: 42: train_loss: 0.7958203697389411, train_acc: 0.5133928656578064, val_loss: 0.609912782907486, val_acc: 0.9375
epoch: 43: train_loss: 0.7920858480713584, train_acc: 0.5639880895614624, val_loss: 0.5736563205718994, val_acc: 0.984375
epoch: 44: train_loss: 0.789483043882582, train_acc: 0.5625, val_loss: 0.5765687823295593, val_acc: 0.984375
epoch: 45: train_loss: 0.7883617346701415, train_acc: 0.4717261989911397, val_loss: 0.5743385255336761, val_acc: 0.984375
epoch: 46: train_loss: 0.7881119923388704, train_acc: 0.4122023781140645, val_loss: 0.5577800273895264, val_acc: 1.0
epoch: 47: train_loss: 0.7869573069943322, train_acc: 0.4419642885526021, val_loss: 0.5641827881336212, val_acc: 1.0
epoch: 48: train_loss: 0.7838271657625834, train_acc: 0.5446428656578064, val_loss: 0.5775034427642822, val_acc: 1.0
epoch: 49: train_loss: 0.7807712292671204, train_acc: 0.5684523781140646, val_loss: 0.5596458613872528, val_acc: 1.0
epoch: 50: train_loss: 0.7790395001959957, train_acc: 0.5357142885526022, val_loss: 0.5756167471408844, val_acc: 0.984375
epoch: 51: train_loss: 0.7771400874241805, train_acc: 0.5193452338377634, val_loss: 0.5947745740413666, val_acc: 0.921875
epoch: 52: train_loss: 0.7757655918223304, train_acc: 0.5208333333333334, val_loss: 0.584087461233139, val_acc: 1.0
epoch: 53: train_loss: 0.774773160616557, train_acc: 0.4985119005044301, val_loss: 0.5875063240528107, val_acc: 0.984375
epoch: 54: train_loss: 0.7738006927750328, train_acc: 0.4866071442763011, val_loss: 0.5606468617916107, val_acc: 1.0
epoch: 55: train_loss: 0.7721085292952402, train_acc: 0.5208333333333334, val_loss: 0.5651629567146301, val_acc: 1.0
epoch: 56: train_loss: 0.7708985990251018, train_acc: 0.5223214228947958, val_loss: 0.6200135350227356, val_acc: 0.875
epoch: 57: train_loss: 0.7694343479200342, train_acc: 0.5357142885526022, val_loss: 0.5647404193878174, val_acc: 1.0
epoch: 58: train_loss: 0.7684725128998192, train_acc: 0.4955357114473979, val_loss: 0.5667557716369629, val_acc: 1.0
epoch: 59: train_loss: 0.7670975198348364, train_acc: 0.5193452338377634, val_loss: 0.5633976459503174, val_acc: 0.984375
epoch: 60: train_loss: 0.7678764999889938, train_acc: 0.3988095223903656, val_loss: 0.5683857500553131, val_acc: 1.0
epoch: 61: train_loss: 0.7669429038801502, train_acc: 0.53125, val_loss: 0.6163335740566254, val_acc: 0.96875
epoch: 62: train_loss: 0.7660434889414955, train_acc: 0.511904756228129, val_loss: 0.6146273016929626, val_acc: 0.9375
epoch: 63: train_loss: 0.7655436014756561, train_acc: 0.4598214228947957, val_loss: 0.6032316386699677, val_acc: 0.9375
epoch: 64: train_loss: 0.7643764190184765, train_acc: 0.5252976218859354, val_loss: 0.5696119070053101, val_acc: 0.96875
epoch: 65: train_loss: 0.7631239427460565, train_acc: 0.5223214228947958, val_loss: 0.592216283082962, val_acc: 0.96875
epoch: 66: train_loss: 0.7630905775881527, train_acc: 0.4821428656578064, val_loss: 0.575463593006134, val_acc: 0.96875
epoch: 67: train_loss: 0.7628561608931599, train_acc: 0.4791666666666667, val_loss: 0.5706385970115662, val_acc: 1.0
epoch: 68: train_loss: 0.761974570831815, train_acc: 0.4791666666666667, val_loss: 0.5865173637866974, val_acc: 0.96875
epoch: 69: train_loss: 0.7608691133203962, train_acc: 0.5044642885526022, val_loss: 0.582186222076416, val_acc: 1.0
epoch: 70: train_loss: 0.7591855800207793, train_acc: 0.5669642885526022, val_loss: 0.556726336479187, val_acc: 1.0
epoch: 71: train_loss: 0.7583802761854951, train_acc: 0.4985119005044301, val_loss: 0.5857091248035431, val_acc: 1.0
epoch: 72: train_loss: 0.7579139315918703, train_acc: 0.4985119005044301, val_loss: 0.5696069002151489, val_acc: 0.984375
epoch: 73: train_loss: 0.7561668954991008, train_acc: 0.586309532324473, val_loss: 0.5571195781230927, val_acc: 1.0
epoch: 74: train_loss: 0.7536499630080331, train_acc: 0.6235119104385376, val_loss: 0.5631784200668335, val_acc: 1.0
epoch: 75: train_loss: 0.7534332667526448, train_acc: 0.4553571442763011, val_loss: 0.5766288936138153, val_acc: 1.0
epoch: 76: train_loss: 0.7524925413585846, train_acc: 0.5565476218859354, val_loss: 0.5668483376502991, val_acc: 1.0
epoch: 77: train_loss: 0.7523367463523508, train_acc: 0.4806547562281291, val_loss: 0.5621707141399384, val_acc: 1.0
epoch: 78: train_loss: 0.7503488916385025, train_acc: 0.6116071343421936, val_loss: 0.5545026957988739, val_acc: 1.0
epoch: 79: train_loss: 0.7502229101955893, train_acc: 0.4717261989911397, val_loss: 0.586693286895752, val_acc: 0.984375
epoch: 80: train_loss: 0.7499591516859739, train_acc: 0.46875, val_loss: 0.5746419727802277, val_acc: 0.984375
epoch: 81: train_loss: 0.7488863676544129, train_acc: 0.555059532324473, val_loss: 0.6211642324924469, val_acc: 0.921875
epoch: 82: train_loss: 0.7484103891265441, train_acc: 0.5059523781140646, val_loss: 0.585687130689621, val_acc: 0.984375
epoch: 83: train_loss: 0.7488803369185282, train_acc: 0.4002976218859355, val_loss: 0.5762965083122253, val_acc: 0.984375
epoch: 84: train_loss: 0.7488770234818554, train_acc: 0.4747023781140645, val_loss: 0.5650413632392883, val_acc: 1.0
epoch: 85: train_loss: 0.7471280506877014, train_acc: 0.59375, val_loss: 0.5601525902748108, val_acc: 1.0
epoch: 86: train_loss: 0.746867329224773, train_acc: 0.4955357114473979, val_loss: 0.5589714646339417, val_acc: 1.0
epoch: 87: train_loss: 0.7457369385343611, train_acc: 0.5714285671710968, val_loss: 0.5612083077430725, val_acc: 0.984375
epoch: 88: train_loss: 0.745848581139068, train_acc: 0.4449404776096344, val_loss: 0.5639028549194336, val_acc: 0.984375
epoch: 89: train_loss: 0.7447819643550448, train_acc: 0.5684523781140646, val_loss: 0.5674700140953064, val_acc: 0.984375
epoch: 90: train_loss: 0.7438757282037002, train_acc: 0.5342261989911398, val_loss: 0.5616629719734192, val_acc: 1.0
epoch: 91: train_loss: 0.743192648758059, train_acc: 0.5267857114473978, val_loss: 0.5598692893981934, val_acc: 1.0
epoch: 92: train_loss: 0.7420911406530701, train_acc: 0.5491071343421936, val_loss: 0.5594491064548492, val_acc: 1.0
epoch: 93: train_loss: 0.7405662760666921, train_acc: 0.5967261989911398, val_loss: 0.5941333472728729, val_acc: 0.96875
epoch: 94: train_loss: 0.7398855847224853, train_acc: 0.5535714228947958, val_loss: 0.5694261789321899, val_acc: 1.0
epoch: 95: train_loss: 0.7395996018830272, train_acc: 0.4851190447807312, val_loss: 0.5596964955329895, val_acc: 1.0
epoch: 96: train_loss: 0.7385002358262891, train_acc: 0.5595238010088602, val_loss: 0.5597833395004272, val_acc: 1.0
epoch: 97: train_loss: 0.738201446070963, train_acc: 0.4806547562281291, val_loss: 0.5569997131824493, val_acc: 1.0
epoch: 98: train_loss: 0.7373749748223558, train_acc: 0.5654761989911398, val_loss: 0.5546345114707947, val_acc: 1.0
epoch: 99: train_loss: 0.7373851718505223, train_acc: 0.5014880895614624, val_loss: 0.5931416749954224, val_acc: 0.96875
epoch: 100: train_loss: 0.7365440391864713, train_acc: 0.53125, val_loss: 0.5595290958881378, val_acc: 1.0
epoch: 101: train_loss: 0.7365627902395585, train_acc: 0.4449404776096344, val_loss: 0.5690311193466187, val_acc: 0.984375
epoch: 102: train_loss: 0.7363926911045433, train_acc: 0.4910714228947957, val_loss: 0.5775512456893921, val_acc: 0.984375
epoch: 103: train_loss: 0.7354883249753562, train_acc: 0.5357142885526022, val_loss: 0.576625406742096, val_acc: 0.96875
epoch: 104: train_loss: 0.7354682114389208, train_acc: 0.4672619005044301, val_loss: 0.5724837183952332, val_acc: 1.0
epoch: 105: train_loss: 0.7353072479460975, train_acc: 0.4776785671710968, val_loss: 0.595823734998703, val_acc: 0.984375
epoch: 106: train_loss: 0.7354242266895615, train_acc: 0.4613095223903656, val_loss: 0.5570632219314575, val_acc: 1.0
epoch: 107: train_loss: 0.7348183662067226, train_acc: 0.5163690447807312, val_loss: 0.5640223026275635, val_acc: 0.984375
epoch: 108: train_loss: 0.7343984226203476, train_acc: 0.5029761989911398, val_loss: 0.5698712766170502, val_acc: 0.984375
epoch: 109: train_loss: 0.7336596582875108, train_acc: 0.5610119005044302, val_loss: 0.5682704150676727, val_acc: 0.984375
epoch: 110: train_loss: 0.7337249449423484, train_acc: 0.4419642885526021, val_loss: 0.5580762922763824, val_acc: 1.0
epoch: 111: train_loss: 0.7334484784376054, train_acc: 0.4791666666666667, val_loss: 0.5723702609539032, val_acc: 0.96875
epoch: 112: train_loss: 0.7330967947445086, train_acc: 0.4940476218859355, val_loss: 0.5667189061641693, val_acc: 0.984375
epoch: 113: train_loss: 0.7328788174523249, train_acc: 0.4776785671710968, val_loss: 0.5562867522239685, val_acc: 1.0
epoch: 114: train_loss: 0.7318274154179338, train_acc: 0.5982142885526022, val_loss: 0.5620794892311096, val_acc: 1.0
epoch: 115: train_loss: 0.7315335657404758, train_acc: 0.4836309552192688, val_loss: 0.6470533311367035, val_acc: 0.875
epoch: 116: train_loss: 0.7310926190808288, train_acc: 0.5342261989911398, val_loss: 0.5897081196308136, val_acc: 0.984375
epoch: 117: train_loss: 0.7303833405850297, train_acc: 0.5461309552192688, val_loss: 0.604999840259552, val_acc: 0.921875
epoch: 118: train_loss: 0.7300956018832552, train_acc: 0.550595243771871, val_loss: 0.5652894973754883, val_acc: 0.984375
epoch: 119: train_loss: 0.7292046134670576, train_acc: 0.5773809552192688, val_loss: 0.6177711188793182, val_acc: 0.9375
epoch: 120: train_loss: 0.728793436666494, train_acc: 0.517857144276301, val_loss: 0.5800400674343109, val_acc: 0.984375
epoch: 121: train_loss: 0.7280689648917464, train_acc: 0.5758928656578064, val_loss: 0.5700495839118958, val_acc: 1.0
epoch: 122: train_loss: 0.7274051302170689, train_acc: 0.5401785671710968, val_loss: 0.554116278886795, val_acc: 1.0
epoch: 123: train_loss: 0.7267357974603612, train_acc: 0.5401785671710968, val_loss: 0.5537725389003754, val_acc: 1.0
epoch: 124: train_loss: 0.7267704496383667, train_acc: 0.4747023781140645, val_loss: 0.5631963610649109, val_acc: 1.0
epoch: 125: train_loss: 0.726424004508074, train_acc: 0.5029761989911398, val_loss: 0.5609499514102936, val_acc: 1.0
epoch: 126: train_loss: 0.7261399892684355, train_acc: 0.4776785671710968, val_loss: 0.5577584207057953, val_acc: 1.0
epoch: 127: train_loss: 0.725959141117831, train_acc: 0.4880952338377635, val_loss: 0.5686095356941223, val_acc: 0.984375
epoch: 128: train_loss: 0.7255872565030435, train_acc: 0.53125, val_loss: 0.5653221905231476, val_acc: 0.984375
epoch: 129: train_loss: 0.7248787047007145, train_acc: 0.5967261989911398, val_loss: 0.568178653717041, val_acc: 0.984375
epoch: 130: train_loss: 0.7249305196694138, train_acc: 0.4702380895614624, val_loss: 0.7221813797950745, val_acc: 0.765625
epoch: 131: train_loss: 0.7249311467613836, train_acc: 0.543154756228129, val_loss: 0.555467963218689, val_acc: 1.0
epoch: 132: train_loss: 0.7239666118060137, train_acc: 0.617559532324473, val_loss: 0.6505732536315918, val_acc: 0.890625
epoch: 133: train_loss: 0.7231613863166885, train_acc: 0.5997023781140646, val_loss: 0.6236813366413116, val_acc: 0.9375
epoch: 134: train_loss: 0.7226774620421138, train_acc: 0.5148809552192688, val_loss: 0.594095766544342, val_acc: 0.984375
epoch: 135: train_loss: 0.7225545966157726, train_acc: 0.4895833333333333, val_loss: 0.6199372112751007, val_acc: 0.90625
epoch: 136: train_loss: 0.7221667662154149, train_acc: 0.5282738109429678, val_loss: 0.5698473155498505, val_acc: 0.984375
epoch: 137: train_loss: 0.7212018072605133, train_acc: 0.6086309552192688, val_loss: 0.5705108046531677, val_acc: 1.0
epoch: 138: train_loss: 0.7210731895135747, train_acc: 0.4970238010088603, val_loss: 0.6052427291870117, val_acc: 0.9375
epoch: 139: train_loss: 0.7209449457270759, train_acc: 0.4880952338377635, val_loss: 0.5589888095855713, val_acc: 1.0
epoch: 140: train_loss: 0.7201350141078868, train_acc: 0.6071428656578064, val_loss: 0.5650041997432709, val_acc: 1.0
epoch: 141: train_loss: 0.7196584932960814, train_acc: 0.5788690447807312, val_loss: 0.5943672358989716, val_acc: 0.984375
epoch: 142: train_loss: 0.7192747149712, train_acc: 0.5639880895614624, val_loss: 0.5611402094364166, val_acc: 0.984375
epoch: 143: train_loss: 0.7189466531078019, train_acc: 0.5, val_loss: 0.5825277268886566, val_acc: 0.984375
epoch: 144: train_loss: 0.7185865352893697, train_acc: 0.5372023781140646, val_loss: 0.5703700184822083, val_acc: 0.984375
epoch: 145: train_loss: 0.7182102227864199, train_acc: 0.5729166666666666, val_loss: 0.6047264635562897, val_acc: 0.9375
epoch: 146: train_loss: 0.7173692823267307, train_acc: 0.6101190447807312, val_loss: 0.5537658035755157, val_acc: 1.0
epoch: 147: train_loss: 0.7174977147364401, train_acc: 0.4657738109429677, val_loss: 0.5555374622344971, val_acc: 1.0
epoch: 148: train_loss: 0.7168830542489719, train_acc: 0.5758928656578064, val_loss: 0.5829446315765381, val_acc: 0.96875
epoch: 149: train_loss: 0.7166015982627869, train_acc: 0.5386904776096344, val_loss: 0.6140518188476562, val_acc: 0.9375
epoch: 150: train_loss: 0.7163240313529968, train_acc: 0.5089285671710968, val_loss: 0.6217301189899445, val_acc: 0.921875
epoch: 151: train_loss: 0.7160225598174229, train_acc: 0.5133928656578064, val_loss: 0.5561553239822388, val_acc: 1.0
epoch: 152: train_loss: 0.7154068840355134, train_acc: 0.6220238010088602, val_loss: 0.5560208261013031, val_acc: 1.0
epoch: 153: train_loss: 0.7150622656593074, train_acc: 0.5297619005044302, val_loss: 0.5565739870071411, val_acc: 1.0
epoch: 154: train_loss: 0.7149133183622872, train_acc: 0.5133928656578064, val_loss: 0.5561233460903168, val_acc: 1.0
epoch: 155: train_loss: 0.7142692828535013, train_acc: 0.586309532324473, val_loss: 0.5723748505115509, val_acc: 0.96875
epoch: 156: train_loss: 0.7139282770723799, train_acc: 0.5208333333333334, val_loss: 0.5621246099472046, val_acc: 0.984375
epoch: 157: train_loss: 0.7135561601522098, train_acc: 0.5654761989911398, val_loss: 0.5588699877262115, val_acc: 1.0
epoch: 158: train_loss: 0.7134817281359146, train_acc: 0.5104166666666666, val_loss: 0.644150048494339, val_acc: 0.90625
epoch: 159: train_loss: 0.7130389485508203, train_acc: 0.5476190447807312, val_loss: 0.5622151792049408, val_acc: 1.0
epoch: 160: train_loss: 0.7128499099680108, train_acc: 0.5178571343421936, val_loss: 0.5551953017711639, val_acc: 1.0
epoch: 161: train_loss: 0.7122183101167403, train_acc: 0.6294642885526022, val_loss: 0.5582651495933533, val_acc: 1.0
epoch: 162: train_loss: 0.7117333398762411, train_acc: 0.5684523781140646, val_loss: 0.5574109554290771, val_acc: 1.0
epoch: 163: train_loss: 0.7116680082266892, train_acc: 0.4747023781140645, val_loss: 0.55948206782341, val_acc: 1.0
epoch: 164: train_loss: 0.7118466199046433, train_acc: 0.4389880895614624, val_loss: 0.5738517940044403, val_acc: 0.984375
epoch: 165: train_loss: 0.711645191692444, train_acc: 0.543154756228129, val_loss: 0.55308997631073, val_acc: 1.0
epoch: 166: train_loss: 0.7115121694382077, train_acc: 0.5193452338377634, val_loss: 0.6378648281097412, val_acc: 0.90625
epoch: 167: train_loss: 0.7119171873681127, train_acc: 0.4330357114473979, val_loss: 0.5759322941303253, val_acc: 0.984375
epoch: 168: train_loss: 0.7117006311990334, train_acc: 0.5297619104385376, val_loss: 0.5531007051467896, val_acc: 1.0
epoch: 169: train_loss: 0.7114279874399595, train_acc: 0.5565476218859354, val_loss: 0.5723577737808228, val_acc: 0.984375
epoch: 170: train_loss: 0.7109990963461803, train_acc: 0.5669642885526022, val_loss: 0.5592862963676453, val_acc: 0.984375
epoch: 171: train_loss: 0.7108477109162381, train_acc: 0.5297619104385376, val_loss: 0.5542623698711395, val_acc: 1.0
epoch: 172: train_loss: 0.7105170421747342, train_acc: 0.5758928656578064, val_loss: 0.5620945394039154, val_acc: 1.0
epoch: 173: train_loss: 0.7100461666611417, train_acc: 0.5610119005044302, val_loss: 0.5566880702972412, val_acc: 1.0
epoch: 174: train_loss: 0.7098422836122057, train_acc: 0.5208333333333334, val_loss: 0.5565870106220245, val_acc: 1.0
epoch: 175: train_loss: 0.7095810152364499, train_acc: 0.5461309552192688, val_loss: 0.5517010986804962, val_acc: 1.0
epoch: 176: train_loss: 0.7088304783889352, train_acc: 0.6116071343421936, val_loss: 0.6200484037399292, val_acc: 0.9375
epoch: 177: train_loss: 0.7091138005703128, train_acc: 0.4553571442763011, val_loss: 0.5535321235656738, val_acc: 1.0
epoch: 178: train_loss: 0.7089075509396344, train_acc: 0.5252976218859354, val_loss: 0.6086846888065338, val_acc: 0.921875
epoch: 179: train_loss: 0.7091147171126471, train_acc: 0.4806547562281291, val_loss: 0.5558567345142365, val_acc: 1.0
epoch: 180: train_loss: 0.7088708278223954, train_acc: 0.5372023781140646, val_loss: 0.5720157027244568, val_acc: 0.984375
epoch: 181: train_loss: 0.7085360771133786, train_acc: 0.6086309552192688, val_loss: 0.5935107469558716, val_acc: 0.953125
epoch: 182: train_loss: 0.7084219019269683, train_acc: 0.5282738109429678, val_loss: 0.6124283969402313, val_acc: 0.921875
epoch: 183: train_loss: 0.7079700700182845, train_acc: 0.5877976218859354, val_loss: 0.6472575664520264, val_acc: 0.890625
epoch: 184: train_loss: 0.708051619658599, train_acc: 0.511904756228129, val_loss: 0.5982585549354553, val_acc: 0.96875
epoch: 185: train_loss: 0.7078801778055006, train_acc: 0.538690467675527, val_loss: 0.5563790798187256, val_acc: 1.0
epoch: 186: train_loss: 0.7075123894235879, train_acc: 0.5654761989911398, val_loss: 0.5551231503486633, val_acc: 1.0
epoch: 187: train_loss: 0.7071849962709643, train_acc: 0.5446428656578064, val_loss: 0.565780758857727, val_acc: 0.984375
epoch: 188: train_loss: 0.7072478467614982, train_acc: 0.4866071442763011, val_loss: 0.5689879357814789, val_acc: 0.984375
epoch: 189: train_loss: 0.707128951319477, train_acc: 0.4955357114473979, val_loss: 0.5645829141139984, val_acc: 0.984375
epoch: 190: train_loss: 0.7069668144663799, train_acc: 0.5282738010088602, val_loss: 0.5842889249324799, val_acc: 0.984375
epoch: 191: train_loss: 0.7068240798802838, train_acc: 0.4985119005044301, val_loss: 0.5538857877254486, val_acc: 1.0
epoch: 192: train_loss: 0.7066366399713954, train_acc: 0.5133928656578064, val_loss: 0.5525718331336975, val_acc: 1.0
epoch: 193: train_loss: 0.7062986438831512, train_acc: 0.5684523781140646, val_loss: 0.5796848833560944, val_acc: 0.9375
epoch: 194: train_loss: 0.705732536010253, train_acc: 0.6116071343421936, val_loss: 0.5544897317886353, val_acc: 1.0
epoch: 195: train_loss: 0.7055473003257698, train_acc: 0.5223214228947958, val_loss: 0.5670748651027679, val_acc: 1.0
epoch: 196: train_loss: 0.7054870840660004, train_acc: 0.5104166666666666, val_loss: 0.5518704652786255, val_acc: 1.0
epoch: 197: train_loss: 0.7050484341603738, train_acc: 0.586309532324473, val_loss: 0.5951103568077087, val_acc: 0.9375
epoch: 198: train_loss: 0.7053575357999433, train_acc: 0.5461309552192688, val_loss: 0.6050223112106323, val_acc: 0.9375
epoch: 199: train_loss: 0.7055533517400423, train_acc: 0.4970238109429677, val_loss: 0.8730381727218628, val_acc: 0.671875
epoch: 200: train_loss: 0.7071368337468326, train_acc: 0.3764880994955699, val_loss: 0.9002649784088135, val_acc: 0.640625
epoch: 201: train_loss: 0.7081468109250462, train_acc: 0.5133928656578064, val_loss: 0.565479964017868, val_acc: 0.984375
epoch: 202: train_loss: 0.7080774179037372, train_acc: 0.5, val_loss: 0.5593662858009338, val_acc: 1.0
epoch: 203: train_loss: 0.7083360800166535, train_acc: 0.4880952338377635, val_loss: 0.8394952416419983, val_acc: 0.6875
epoch: 204: train_loss: 0.7089547134027249, train_acc: 0.4449404776096344, val_loss: 0.7552683353424072, val_acc: 0.78125
epoch: 205: train_loss: 0.7098499987889262, train_acc: 0.3854166666666667, val_loss: 0.5577504634857178, val_acc: 1.0
epoch: 206: train_loss: 0.7097102562010577, train_acc: 0.4970238109429677, val_loss: 0.6427378952503204, val_acc: 0.921875
epoch: 207: train_loss: 0.7099262464504975, train_acc: 0.5208333333333334, val_loss: 0.6930804550647736, val_acc: 0.828125
epoch: 208: train_loss: 0.7104942097047869, train_acc: 0.4255952338377635, val_loss: 0.5719937980175018, val_acc: 0.984375
epoch: 209: train_loss: 0.7106233605316707, train_acc: 0.4955357114473979, val_loss: 0.6243841648101807, val_acc: 0.9375
epoch: 210: train_loss: 0.710387279637052, train_acc: 0.574404756228129, val_loss: 0.5531481206417084, val_acc: 1.0
epoch: 211: train_loss: 0.7101981145968228, train_acc: 0.5505952338377634, val_loss: 0.6192382872104645, val_acc: 0.953125
epoch: 212: train_loss: 0.7102467976452227, train_acc: 0.4985119005044301, val_loss: 0.6375457346439362, val_acc: 0.90625
epoch: 213: train_loss: 0.7101481529039758, train_acc: 0.5386904776096344, val_loss: 0.5711817741394043, val_acc: 0.984375
epoch: 214: train_loss: 0.710095847022626, train_acc: 0.4895833333333333, val_loss: 0.5530833005905151, val_acc: 1.0
epoch: 215: train_loss: 0.7100199717614387, train_acc: 0.5104166666666666, val_loss: 0.5670261681079865, val_acc: 0.984375
epoch: 216: train_loss: 0.7100145855257589, train_acc: 0.5327380895614624, val_loss: 0.5707326531410217, val_acc: 0.984375
epoch: 217: train_loss: 0.7098468447314856, train_acc: 0.5669642885526022, val_loss: 0.57310351729393, val_acc: 0.984375
epoch: 218: train_loss: 0.7098957143236333, train_acc: 0.4553571442763011, val_loss: 0.5875588655471802, val_acc: 0.9375
epoch: 219: train_loss: 0.7097518678867458, train_acc: 0.5104166666666666, val_loss: 0.6048840582370758, val_acc: 0.921875
epoch: 220: train_loss: 0.7097377881385265, train_acc: 0.5267857114473978, val_loss: 0.5777522325515747, val_acc: 0.9375
epoch: 221: train_loss: 0.7094491141157467, train_acc: 0.5446428656578064, val_loss: 0.552163302898407, val_acc: 1.0
epoch: 222: train_loss: 0.709174675346312, train_acc: 0.5654761989911398, val_loss: 0.5631646811962128, val_acc: 1.0
epoch: 223: train_loss: 0.7090886338126093, train_acc: 0.4985119005044301, val_loss: 0.5530011355876923, val_acc: 1.0
epoch: 224: train_loss: 0.7087467230690853, train_acc: 0.6071428656578064, val_loss: 0.5659234821796417, val_acc: 0.984375
epoch: 225: train_loss: 0.7085353381338375, train_acc: 0.5386904776096344, val_loss: 0.5996602475643158, val_acc: 0.953125
epoch: 226: train_loss: 0.7087248535758426, train_acc: 0.4598214228947957, val_loss: 0.5761461555957794, val_acc: 0.96875
epoch: 227: train_loss: 0.7088658655421778, train_acc: 0.4747023781140645, val_loss: 0.5526224672794342, val_acc: 1.0
epoch: 228: train_loss: 0.7088601276864134, train_acc: 0.4776785671710968, val_loss: 0.5818229019641876, val_acc: 0.9375
epoch: 229: train_loss: 0.708500619383826, train_acc: 0.5773809552192688, val_loss: 0.5606047511100769, val_acc: 1.0
epoch: 230: train_loss: 0.7084494607273121, train_acc: 0.511904756228129, val_loss: 0.5633377730846405, val_acc: 0.984375
epoch: 231: train_loss: 0.7085446117595697, train_acc: 0.4508928656578064, val_loss: 0.5715381801128387, val_acc: 1.0
epoch: 232: train_loss: 0.7082338015920617, train_acc: 0.5625, val_loss: 0.5636094510555267, val_acc: 0.984375
epoch: 233: train_loss: 0.707940642735218, train_acc: 0.5997023781140646, val_loss: 0.6166971325874329, val_acc: 0.921875
epoch: 234: train_loss: 0.7081680623352108, train_acc: 0.4136904776096344, val_loss: 0.5632596015930176, val_acc: 1.0
epoch: 235: train_loss: 0.7080452311678798, train_acc: 0.511904756228129, val_loss: 0.5545061528682709, val_acc: 1.0
epoch: 236: train_loss: 0.7077438858993951, train_acc: 0.5386904776096344, val_loss: 0.5538314878940582, val_acc: 1.0
epoch: 237: train_loss: 0.7074960860885494, train_acc: 0.5461309552192688, val_loss: 0.5531787574291229, val_acc: 1.0
epoch: 238: train_loss: 0.7073090300899173, train_acc: 0.5386904776096344, val_loss: 0.5590800344944, val_acc: 0.984375
epoch: 239: train_loss: 0.7071632362902167, train_acc: 0.5401785671710968, val_loss: 0.5619859397411346, val_acc: 0.984375
epoch: 240: train_loss: 0.707015898217799, train_acc: 0.555059532324473, val_loss: 0.5551580786705017, val_acc: 1.0
epoch: 241: train_loss: 0.7069288814035003, train_acc: 0.4955357114473979, val_loss: 0.5556640028953552, val_acc: 1.0
epoch: 242: train_loss: 0.7068264997872143, train_acc: 0.5089285671710968, val_loss: 0.5568029582500458, val_acc: 1.0
epoch: 243: train_loss: 0.706362810372655, train_acc: 0.6205357114473978, val_loss: 0.5640459656715393, val_acc: 0.984375
epoch: 244: train_loss: 0.7062751503217789, train_acc: 0.5133928656578064, val_loss: 0.5747372806072235, val_acc: 1.0
epoch: 245: train_loss: 0.7061730604669267, train_acc: 0.5208333333333334, val_loss: 0.6155282855033875, val_acc: 0.9375
epoch: 246: train_loss: 0.7056035683383506, train_acc: 0.6383928656578064, val_loss: 0.5787879824638367, val_acc: 0.9375
epoch: 247: train_loss: 0.7053921015993244, train_acc: 0.5505952338377634, val_loss: 0.5707276165485382, val_acc: 0.984375
epoch: 248: train_loss: 0.7052443124842613, train_acc: 0.5372023781140646, val_loss: 0.569702535867691, val_acc: 1.0
epoch: 249: train_loss: 0.7049429372151694, train_acc: 0.6086309552192688, val_loss: 0.5586076378822327, val_acc: 1.0
epoch: 250: train_loss: 0.7046433904098168, train_acc: 0.5922619104385376, val_loss: 0.5669521391391754, val_acc: 0.984375
epoch: 251: train_loss: 0.704759219020763, train_acc: 0.4642857114473979, val_loss: 0.5607575178146362, val_acc: 1.0
epoch: 252: train_loss: 0.7046906390523096, train_acc: 0.4985119005044301, val_loss: 0.5966484546661377, val_acc: 0.953125
epoch: 253: train_loss: 0.7045459007340781, train_acc: 0.5148809552192688, val_loss: 0.5907257497310638, val_acc: 0.984375
epoch: 254: train_loss: 0.704445257685543, train_acc: 0.5446428656578064, val_loss: 0.5520676076412201, val_acc: 1.0
epoch: 255: train_loss: 0.7043853735085578, train_acc: 0.5104166666666666, val_loss: 0.5675517320632935, val_acc: 0.984375
epoch: 256: train_loss: 0.7044281862125323, train_acc: 0.4895833333333333, val_loss: 0.5525965988636017, val_acc: 1.0
epoch: 257: train_loss: 0.7043365590486108, train_acc: 0.5089285671710968, val_loss: 0.5673463642597198, val_acc: 0.984375
epoch: 258: train_loss: 0.7041552995776271, train_acc: 0.5580357114473978, val_loss: 0.5594945251941681, val_acc: 1.0
epoch: 259: train_loss: 0.7038065782724283, train_acc: 0.617559532324473, val_loss: 0.5688854157924652, val_acc: 0.984375
epoch: 260: train_loss: 0.7040909888796119, train_acc: 0.3898809552192688, val_loss: 0.5931648910045624, val_acc: 0.984375
epoch: 261: train_loss: 0.7042510879555429, train_acc: 0.4568452338377635, val_loss: 0.6719190180301666, val_acc: 0.859375
epoch: 262: train_loss: 0.7041176607400293, train_acc: 0.5877976218859354, val_loss: 0.5938341617584229, val_acc: 0.9375
epoch: 263: train_loss: 0.704190305387131, train_acc: 0.5133928656578064, val_loss: 0.6479910910129547, val_acc: 0.890625
epoch: 264: train_loss: 0.7043667554105604, train_acc: 0.5223214228947958, val_loss: 0.641889363527298, val_acc: 0.890625
epoch: 265: train_loss: 0.7045429820255529, train_acc: 0.4508928557236989, val_loss: 0.5721012949943542, val_acc: 0.984375
epoch: 266: train_loss: 0.7047122731042117, train_acc: 0.4494047661622365, val_loss: 0.5658970773220062, val_acc: 0.984375
epoch: 267: train_loss: 0.7048904438368717, train_acc: 0.5267857114473978, val_loss: 0.6596096456050873, val_acc: 0.890625
epoch: 268: train_loss: 0.7050447429275277, train_acc: 0.4880952338377635, val_loss: 0.5834625363349915, val_acc: 0.96875
epoch: 269: train_loss: 0.7050609645284253, train_acc: 0.5029761989911398, val_loss: 0.6117103695869446, val_acc: 0.921875
epoch: 270: train_loss: 0.705273622443022, train_acc: 0.4226190447807312, val_loss: 0.5666806101799011, val_acc: 0.984375
epoch: 271: train_loss: 0.70561960500245, train_acc: 0.4404761890570323, val_loss: 0.5725373923778534, val_acc: 0.984375
epoch: 272: train_loss: 0.705280402932266, train_acc: 0.5877976218859354, val_loss: 0.5802097320556641, val_acc: 0.96875
epoch: 273: train_loss: 0.7050847846951218, train_acc: 0.5416666666666666, val_loss: 0.5658243298530579, val_acc: 0.984375
epoch: 274: train_loss: 0.7047372305031979, train_acc: 0.6071428656578064, val_loss: 0.6107156276702881, val_acc: 0.921875
epoch: 275: train_loss: 0.7046045406861006, train_acc: 0.5223214228947958, val_loss: 0.552721381187439, val_acc: 1.0
epoch: 276: train_loss: 0.7045083307043597, train_acc: 0.5148809552192688, val_loss: 0.5629312694072723, val_acc: 0.984375
epoch: 277: train_loss: 0.7043853412858017, train_acc: 0.5446428656578064, val_loss: 0.5521914958953857, val_acc: 1.0
epoch: 278: train_loss: 0.7044991684928448, train_acc: 0.4404761890570323, val_loss: 0.5686727166175842, val_acc: 0.984375
epoch: 279: train_loss: 0.7044817645634923, train_acc: 0.4895833333333333, val_loss: 0.5665084719657898, val_acc: 0.984375
epoch: 280: train_loss: 0.7045868666287984, train_acc: 0.4642857114473979, val_loss: 0.5536295771598816, val_acc: 1.0
epoch: 281: train_loss: 0.7044780388227873, train_acc: 0.5401785671710968, val_loss: 0.5542880296707153, val_acc: 1.0
epoch: 282: train_loss: 0.704193853026144, train_acc: 0.5788690447807312, val_loss: 0.583563506603241, val_acc: 0.96875
epoch: 283: train_loss: 0.7040828551764778, train_acc: 0.5327380895614624, val_loss: 0.5877252519130707, val_acc: 0.96875
epoch: 284: train_loss: 0.7039669743058276, train_acc: 0.5223214228947958, val_loss: 0.5690968334674835, val_acc: 0.984375
epoch: 285: train_loss: 0.7040275263758526, train_acc: 0.4613095223903656, val_loss: 0.572228729724884, val_acc: 0.984375
epoch: 286: train_loss: 0.7040529970369548, train_acc: 0.4821428557236989, val_loss: 0.583807647228241, val_acc: 0.96875
epoch: 287: train_loss: 0.7041822676029468, train_acc: 0.4583333333333333, val_loss: 0.5640673339366913, val_acc: 1.0
epoch: 288: train_loss: 0.7043109428923846, train_acc: 0.4568452338377635, val_loss: 0.5857404172420502, val_acc: 0.96875
epoch: 289: train_loss: 0.704216132903921, train_acc: 0.5223214228947958, val_loss: 0.5517356097698212, val_acc: 1.0
epoch: 290: train_loss: 0.7040750212276102, train_acc: 0.5461309552192688, val_loss: 0.5538946390151978, val_acc: 1.0
epoch: 291: train_loss: 0.7035626650673067, train_acc: 0.6741071343421936, val_loss: 0.6651936173439026, val_acc: 0.859375
epoch: 292: train_loss: 0.70341604914139, train_acc: 0.543154756228129, val_loss: 0.6026625335216522, val_acc: 0.953125
epoch: 293: train_loss: 0.7034542394333143, train_acc: 0.5104166666666666, val_loss: 0.589481770992279, val_acc: 0.953125
epoch: 294: train_loss: 0.7034269442666047, train_acc: 0.4702380895614624, val_loss: 0.5627111494541168, val_acc: 0.984375
epoch: 295: train_loss: 0.7032934262140376, train_acc: 0.5491071343421936, val_loss: 0.556267112493515, val_acc: 1.0
epoch: 296: train_loss: 0.703615568168503, train_acc: 0.4419642885526021, val_loss: 0.6049371659755707, val_acc: 0.953125
epoch: 297: train_loss: 0.7035154154236682, train_acc: 0.53125, val_loss: 0.551946371793747, val_acc: 1.0
epoch: 298: train_loss: 0.7033964263289799, train_acc: 0.523809532324473, val_loss: 0.5522811412811279, val_acc: 1.0
epoch: 299: train_loss: 0.7033511289623046, train_acc: 0.5029761989911398, val_loss: 0.5657843053340912, val_acc: 0.984375
epoch: 300: train_loss: 0.7034995710994976, train_acc: 0.5148809552192688, val_loss: 0.5517766177654266, val_acc: 1.0
epoch: 301: train_loss: 0.7034509518157038, train_acc: 0.5416666666666666, val_loss: 0.5525385737419128, val_acc: 1.0
epoch: 302: train_loss: 0.7035207511043651, train_acc: 0.4672619005044301, val_loss: 0.5548353493213654, val_acc: 1.0
epoch: 303: train_loss: 0.7034154892490619, train_acc: 0.5297619005044302, val_loss: 0.5606670081615448, val_acc: 1.0
epoch: 304: train_loss: 0.7033742406003459, train_acc: 0.5282738109429678, val_loss: 0.5713067948818207, val_acc: 0.984375
epoch: 305: train_loss: 0.7032292526466912, train_acc: 0.543154756228129, val_loss: 0.5583094954490662, val_acc: 0.984375
epoch: 306: train_loss: 0.7029616315965412, train_acc: 0.6145833333333334, val_loss: 0.5676828622817993, val_acc: 0.984375
epoch: 307: train_loss: 0.7027231689977953, train_acc: 0.5669642885526022, val_loss: 0.5522322058677673, val_acc: 1.0
epoch: 308: train_loss: 0.7027346132706924, train_acc: 0.5, val_loss: 0.5598120987415314, val_acc: 0.984375
epoch: 309: train_loss: 0.7027128487184482, train_acc: 0.5014880895614624, val_loss: 0.5525602400302887, val_acc: 1.0
epoch: 310: train_loss: 0.70271591227282, train_acc: 0.4955357114473979, val_loss: 0.552073061466217, val_acc: 1.0
epoch: 311: train_loss: 0.702592276004899, train_acc: 0.5401785771052042, val_loss: 0.6246438026428223, val_acc: 0.921875
epoch: 312: train_loss: 0.7025466286780606, train_acc: 0.5104166666666666, val_loss: 0.6020462214946747, val_acc: 0.9375
epoch: 313: train_loss: 0.7026171847197151, train_acc: 0.4568452338377635, val_loss: 0.5661305785179138, val_acc: 0.984375
epoch: 314: train_loss: 0.7024554202480919, train_acc: 0.5535714228947958, val_loss: 0.5524310767650604, val_acc: 1.0
epoch: 315: train_loss: 0.7023972000942449, train_acc: 0.4970238109429677, val_loss: 0.5637811422348022, val_acc: 0.984375
epoch: 316: train_loss: 0.7025623938576028, train_acc: 0.4315476218859355, val_loss: 0.5591060221195221, val_acc: 0.984375
epoch: 317: train_loss: 0.7025805394666997, train_acc: 0.4895833333333333, val_loss: 0.5521597266197205, val_acc: 1.0
epoch: 318: train_loss: 0.7025438344403865, train_acc: 0.5074404776096344, val_loss: 0.5766362845897675, val_acc: 1.0
epoch: 319: train_loss: 0.702583931541691, train_acc: 0.5133928656578064, val_loss: 0.5527627170085907, val_acc: 1.0
epoch: 320: train_loss: 0.7024758189823765, train_acc: 0.5342261989911398, val_loss: 0.5514699816703796, val_acc: 1.0
epoch: 321: train_loss: 0.7023738818570938, train_acc: 0.5327380895614624, val_loss: 0.551798939704895, val_acc: 1.0
epoch: 322: train_loss: 0.7023264209983024, train_acc: 0.5014880994955698, val_loss: 0.6088619232177734, val_acc: 0.9375
epoch: 323: train_loss: 0.7023231639476959, train_acc: 0.5089285671710968, val_loss: 0.551552414894104, val_acc: 1.0
epoch: 324: train_loss: 0.7020443286956882, train_acc: 0.5877976218859354, val_loss: 0.5617274641990662, val_acc: 0.984375
epoch: 325: train_loss: 0.7018899628416402, train_acc: 0.5505952338377634, val_loss: 0.5624147355556488, val_acc: 1.0
epoch: 326: train_loss: 0.7017573929039587, train_acc: 0.5133928656578064, val_loss: 0.5664584338665009, val_acc: 0.984375
epoch: 327: train_loss: 0.701519588780839, train_acc: 0.5535714228947958, val_loss: 0.5516862273216248, val_acc: 1.0
epoch: 328: train_loss: 0.7014483130992725, train_acc: 0.5223214228947958, val_loss: 0.5517063438892365, val_acc: 1.0
epoch: 329: train_loss: 0.7013774186372754, train_acc: 0.523809532324473, val_loss: 0.6119814813137054, val_acc: 0.921875
epoch: 330: train_loss: 0.7011150805671648, train_acc: 0.5848214228947958, val_loss: 0.5547347664833069, val_acc: 1.0
epoch: 331: train_loss: 0.7010779829927715, train_acc: 0.5089285671710968, val_loss: 0.5677289068698883, val_acc: 0.984375
epoch: 332: train_loss: 0.7009960702649344, train_acc: 0.5416666666666666, val_loss: 0.5537297427654266, val_acc: 1.0
epoch: 333: train_loss: 0.7009504378258347, train_acc: 0.5267857114473978, val_loss: 0.5521677136421204, val_acc: 1.0
epoch: 334: train_loss: 0.700749765996316, train_acc: 0.5520833333333334, val_loss: 0.5521200299263, val_acc: 1.0
epoch: 335: train_loss: 0.7007711982088427, train_acc: 0.4672619005044301, val_loss: 0.5662612318992615, val_acc: 0.984375
epoch: 336: train_loss: 0.7009203304876557, train_acc: 0.4151785671710968, val_loss: 0.5563422441482544, val_acc: 1.0
epoch: 337: train_loss: 0.7007181286459135, train_acc: 0.5639880895614624, val_loss: 0.604191780090332, val_acc: 0.9375
epoch: 338: train_loss: 0.7006779107958399, train_acc: 0.523809532324473, val_loss: 0.5514873564243317, val_acc: 1.0
epoch: 339: train_loss: 0.7008376109249449, train_acc: 0.4672619005044301, val_loss: 0.5519425868988037, val_acc: 1.0
epoch: 340: train_loss: 0.7008693635988094, train_acc: 0.4821428656578064, val_loss: 0.5537171363830566, val_acc: 1.0
epoch: 341: train_loss: 0.7008669300269893, train_acc: 0.5014880895614624, val_loss: 0.5521693229675293, val_acc: 1.0
epoch: 342: train_loss: 0.7007445929349335, train_acc: 0.574404756228129, val_loss: 0.5584604442119598, val_acc: 1.0
epoch: 343: train_loss: 0.7004522574964417, train_acc: 0.613095243771871, val_loss: 0.5532020628452301, val_acc: 1.0
epoch: 344: train_loss: 0.7004136607266852, train_acc: 0.523809532324473, val_loss: 0.5689803063869476, val_acc: 0.984375
epoch: 345: train_loss: 0.700318820731488, train_acc: 0.5446428656578064, val_loss: 0.5685907006263733, val_acc: 0.984375
epoch: 346: train_loss: 0.7002916991653403, train_acc: 0.5074404776096344, val_loss: 0.5530293583869934, val_acc: 1.0
epoch: 347: train_loss: 0.7002403081262702, train_acc: 0.5059523781140646, val_loss: 0.6080388426780701, val_acc: 0.9375
epoch: 348: train_loss: 0.7001759944467624, train_acc: 0.5133928656578064, val_loss: 0.5539351403713226, val_acc: 1.0
epoch: 349: train_loss: 0.7000657648131958, train_acc: 0.5193452338377634, val_loss: 0.5802335143089294, val_acc: 0.96875
epoch: 350: train_loss: 0.7000032921462315, train_acc: 0.5148809552192688, val_loss: 0.5515070855617523, val_acc: 1.0
epoch: 351: train_loss: 0.6999700837627502, train_acc: 0.5223214228947958, val_loss: 0.5691843032836914, val_acc: 0.984375
epoch: 352: train_loss: 0.6998446301202709, train_acc: 0.5282738109429678, val_loss: 0.5553511679172516, val_acc: 1.0
epoch: 353: train_loss: 0.6998950156002617, train_acc: 0.4657738109429677, val_loss: 0.5691763758659363, val_acc: 0.984375
epoch: 354: train_loss: 0.6999632124609788, train_acc: 0.4627976218859355, val_loss: 0.5598428249359131, val_acc: 0.984375
epoch: 355: train_loss: 0.7000433310922163, train_acc: 0.5461309552192688, val_loss: 0.6232656836509705, val_acc: 0.921875
epoch: 356: train_loss: 0.6998842063626876, train_acc: 0.5833333333333334, val_loss: 0.5680535435676575, val_acc: 0.984375
epoch: 357: train_loss: 0.6997846214575916, train_acc: 0.5297619005044302, val_loss: 0.551455169916153, val_acc: 1.0
epoch: 358: train_loss: 0.6998535906039902, train_acc: 0.4479166666666667, val_loss: 0.5922365486621857, val_acc: 0.9375
epoch: 359: train_loss: 0.6998355516129068, train_acc: 0.4895833333333333, val_loss: 0.5853945314884186, val_acc: 0.984375
epoch: 360: train_loss: 0.6999205116419909, train_acc: 0.4404761890570323, val_loss: 0.5633396208286285, val_acc: 0.984375
epoch: 361: train_loss: 0.6997365040454194, train_acc: 0.574404756228129, val_loss: 0.5761241018772125, val_acc: 1.0
epoch: 362: train_loss: 0.6997722316707787, train_acc: 0.4657738109429677, val_loss: 0.5745436251163483, val_acc: 0.96875
epoch: 363: train_loss: 0.699909910395905, train_acc: 0.4419642885526021, val_loss: 0.5532374382019043, val_acc: 1.0
epoch: 364: train_loss: 0.6997309020120801, train_acc: 0.5729166666666666, val_loss: 0.5517200827598572, val_acc: 1.0
epoch: 365: train_loss: 0.6997412812840088, train_acc: 0.5133928656578064, val_loss: 0.6421081125736237, val_acc: 0.875
epoch: 366: train_loss: 0.699610494571204, train_acc: 0.5714285671710968, val_loss: 0.5721872746944427, val_acc: 0.984375
epoch: 367: train_loss: 0.6994270777896692, train_acc: 0.543154756228129, val_loss: 0.6316013634204865, val_acc: 0.921875
epoch: 368: train_loss: 0.6992592840659906, train_acc: 0.5505952338377634, val_loss: 0.58139568567276, val_acc: 0.96875
epoch: 369: train_loss: 0.699051583511335, train_acc: 0.6145833333333334, val_loss: 0.5698973834514618, val_acc: 0.984375
epoch: 370: train_loss: 0.6989905329727416, train_acc: 0.5520833333333334, val_loss: 0.555164635181427, val_acc: 1.0
epoch: 371: train_loss: 0.6988884792212514, train_acc: 0.511904756228129, val_loss: 0.5522505939006805, val_acc: 1.0
epoch: 372: train_loss: 0.69890716248479, train_acc: 0.5074404776096344, val_loss: 0.5517886579036713, val_acc: 1.0
epoch: 373: train_loss: 0.698763516019372, train_acc: 0.5654761989911398, val_loss: 0.572022557258606, val_acc: 1.0
epoch: 374: train_loss: 0.6987764188448585, train_acc: 0.4702380895614624, val_loss: 0.5518463551998138, val_acc: 1.0
epoch: 375: train_loss: 0.6989564606061215, train_acc: 0.4300595223903656, val_loss: 0.6099967062473297, val_acc: 0.9375
epoch: 376: train_loss: 0.6988700348439201, train_acc: 0.517857144276301, val_loss: 0.5711609125137329, val_acc: 0.984375
epoch: 377: train_loss: 0.6986696354295838, train_acc: 0.5639880895614624, val_loss: 0.5696236193180084, val_acc: 0.984375
epoch: 378: train_loss: 0.6984809203009489, train_acc: 0.5833333333333334, val_loss: 0.5641185939311981, val_acc: 1.0
epoch: 379: train_loss: 0.6984074850354275, train_acc: 0.5193452338377634, val_loss: 0.5919198989868164, val_acc: 0.984375
epoch: 380: train_loss: 0.6980520546540601, train_acc: 0.6547619104385376, val_loss: 0.5562463402748108, val_acc: 1.0
epoch: 381: train_loss: 0.6978882903567988, train_acc: 0.5520833333333334, val_loss: 0.575003981590271, val_acc: 0.96875
epoch: 382: train_loss: 0.6978831234914097, train_acc: 0.5223214228947958, val_loss: 0.5927663743495941, val_acc: 0.96875
epoch: 383: train_loss: 0.6978951679904838, train_acc: 0.4985119005044301, val_loss: 0.5871154069900513, val_acc: 1.0
epoch: 384: train_loss: 0.6979682374568209, train_acc: 0.4642857114473979, val_loss: 0.5521625578403473, val_acc: 1.0
epoch: 385: train_loss: 0.6977459009807547, train_acc: 0.5967261989911398, val_loss: 0.5778250992298126, val_acc: 0.96875
epoch: 386: train_loss: 0.6974135107977648, train_acc: 0.648809532324473, val_loss: 0.6138581931591034, val_acc: 0.9375
epoch: 387: train_loss: 0.6972928068174935, train_acc: 0.5535714228947958, val_loss: 0.6045204699039459, val_acc: 0.921875
epoch: 388: train_loss: 0.6971808638752474, train_acc: 0.5714285771052042, val_loss: 0.5515841841697693, val_acc: 1.0
epoch: 389: train_loss: 0.697056638073717, train_acc: 0.5877976218859354, val_loss: 0.5531443953514099, val_acc: 1.0
epoch: 390: train_loss: 0.6970761437651849, train_acc: 0.4836309552192688, val_loss: 0.5545881986618042, val_acc: 1.0
epoch: 391: train_loss: 0.696991340136852, train_acc: 0.5401785671710968, val_loss: 0.5650898516178131, val_acc: 0.984375
epoch: 392: train_loss: 0.696730886150761, train_acc: 0.6101190447807312, val_loss: 0.5837274789810181, val_acc: 0.9375
epoch: 393: train_loss: 0.6966766675716727, train_acc: 0.5148809552192688, val_loss: 0.6136380732059479, val_acc: 0.9375
epoch: 394: train_loss: 0.6966815632606858, train_acc: 0.4970238109429677, val_loss: 0.5769051313400269, val_acc: 0.96875
epoch: 395: train_loss: 0.6967085177950584, train_acc: 0.4940476218859355, val_loss: 0.5694582760334015, val_acc: 0.984375
epoch: 396: train_loss: 0.6969004783382141, train_acc: 0.4285714228947957, val_loss: 0.6346383988857269, val_acc: 0.921875
epoch: 397: train_loss: 0.6968340876713467, train_acc: 0.53125, val_loss: 0.645410418510437, val_acc: 0.90625
epoch: 398: train_loss: 0.6969551423139737, train_acc: 0.4345238109429677, val_loss: 0.6267545819282532, val_acc: 0.921875
epoch: 399: train_loss: 0.696930406540632, train_acc: 0.511904756228129, val_loss: 0.5516477525234222, val_acc: 1.0
epoch: 400: train_loss: 0.6968888952547769, train_acc: 0.523809532324473, val_loss: 0.5552297234535217, val_acc: 1.0
epoch: 401: train_loss: 0.6967870249083976, train_acc: 0.5714285671710968, val_loss: 0.5993406474590302, val_acc: 0.9375
epoch: 402: train_loss: 0.6970482708206837, train_acc: 0.3898809552192688, val_loss: 0.5550090670585632, val_acc: 1.0
epoch: 403: train_loss: 0.6969539672136303, train_acc: 0.555059532324473, val_loss: 0.598645031452179, val_acc: 0.9375
epoch: 404: train_loss: 0.6968554503142586, train_acc: 0.601190467675527, val_loss: 0.5515599250793457, val_acc: 1.0
epoch: 405: train_loss: 0.6966939060069458, train_acc: 0.5892857114473978, val_loss: 0.5716331601142883, val_acc: 1.0
epoch: 406: train_loss: 0.6966266222999875, train_acc: 0.5476190447807312, val_loss: 0.5644922852516174, val_acc: 0.984375
epoch: 407: train_loss: 0.6965760894460612, train_acc: 0.5357142885526022, val_loss: 0.5556193888187408, val_acc: 1.0
epoch: 408: train_loss: 0.6963132171729954, train_acc: 0.6041666666666666, val_loss: 0.5624157190322876, val_acc: 0.984375
epoch: 409: train_loss: 0.6963652836840324, train_acc: 0.4866071442763011, val_loss: 0.5608924627304077, val_acc: 1.0
epoch: 410: train_loss: 0.696163232623322, train_acc: 0.5654761989911398, val_loss: 0.5629390776157379, val_acc: 0.984375
epoch: 411: train_loss: 0.696157286009935, train_acc: 0.5044642885526022, val_loss: 0.5515294075012207, val_acc: 1.0
epoch: 412: train_loss: 0.6961877373700375, train_acc: 0.4657738109429677, val_loss: 0.5825661420822144, val_acc: 0.96875
epoch: 413: train_loss: 0.6960420153807133, train_acc: 0.5729166666666666, val_loss: 0.5624916255474091, val_acc: 0.984375
epoch: 414: train_loss: 0.6959656821915421, train_acc: 0.5327380895614624, val_loss: 0.5542662441730499, val_acc: 1.0
epoch: 415: train_loss: 0.6958800168134842, train_acc: 0.5223214228947958, val_loss: 0.551607757806778, val_acc: 1.0
epoch: 416: train_loss: 0.6957933098244531, train_acc: 0.5282738109429678, val_loss: 0.562697559595108, val_acc: 1.0
epoch: 417: train_loss: 0.6959731096143355, train_acc: 0.4821428656578064, val_loss: 0.551771491765976, val_acc: 1.0
epoch: 418: train_loss: 0.6958838877406682, train_acc: 0.5491071343421936, val_loss: 0.5516162812709808, val_acc: 1.0
epoch: 419: train_loss: 0.6957672703124226, train_acc: 0.580357144276301, val_loss: 0.6151784956455231, val_acc: 0.921875
epoch: 420: train_loss: 0.6958152491566874, train_acc: 0.4702380895614624, val_loss: 0.5674237906932831, val_acc: 0.984375
epoch: 421: train_loss: 0.6956803227346653, train_acc: 0.569940467675527, val_loss: 0.5676428079605103, val_acc: 0.984375
epoch: 422: train_loss: 0.6956504657805855, train_acc: 0.4985119005044301, val_loss: 0.5840421617031097, val_acc: 0.984375
epoch: 423: train_loss: 0.6957144208908452, train_acc: 0.5133928656578064, val_loss: 0.5562161505222321, val_acc: 1.0
epoch: 424: train_loss: 0.6957300648736016, train_acc: 0.4836309552192688, val_loss: 0.5693358778953552, val_acc: 1.0
epoch: 425: train_loss: 0.6956469926149442, train_acc: 0.5357142885526022, val_loss: 0.5670702159404755, val_acc: 0.984375
epoch: 426: train_loss: 0.6956546237048087, train_acc: 0.5104166666666666, val_loss: 0.5545520186424255, val_acc: 1.0
epoch: 427: train_loss: 0.6955573010342512, train_acc: 0.581845243771871, val_loss: 0.5666517615318298, val_acc: 0.984375
epoch: 428: train_loss: 0.6954671919067975, train_acc: 0.5818452338377634, val_loss: 0.5517882406711578, val_acc: 1.0
epoch: 429: train_loss: 0.6954981287089426, train_acc: 0.4642857114473979, val_loss: 0.5787744224071503, val_acc: 0.9375
epoch: 430: train_loss: 0.6954107458104662, train_acc: 0.5476190447807312, val_loss: 0.5547096729278564, val_acc: 1.0
epoch: 431: train_loss: 0.695367636526992, train_acc: 0.5133928656578064, val_loss: 0.5771148204803467, val_acc: 0.96875
epoch: 432: train_loss: 0.6952119018363435, train_acc: 0.5491071343421936, val_loss: 0.551598995923996, val_acc: 1.0
epoch: 433: train_loss: 0.6951232474474677, train_acc: 0.5148809552192688, val_loss: 0.5738343894481659, val_acc: 0.96875
epoch: 434: train_loss: 0.695214214338653, train_acc: 0.4449404776096344, val_loss: 0.567194938659668, val_acc: 0.984375
epoch: 435: train_loss: 0.6950518097215832, train_acc: 0.5758928656578064, val_loss: 0.5515056550502777, val_acc: 1.0
epoch: 436: train_loss: 0.6949869643223363, train_acc: 0.5133928656578064, val_loss: 0.5675765573978424, val_acc: 0.984375
epoch: 437: train_loss: 0.6949429844294869, train_acc: 0.523809532324473, val_loss: 0.5562133491039276, val_acc: 1.0
epoch: 438: train_loss: 0.6949325139403068, train_acc: 0.5208333333333334, val_loss: 0.5518055558204651, val_acc: 1.0
epoch: 439: train_loss: 0.6948824814097444, train_acc: 0.5163690447807312, val_loss: 0.558149516582489, val_acc: 0.984375
epoch: 440: train_loss: 0.6946997810788702, train_acc: 0.5758928656578064, val_loss: 0.5806624293327332, val_acc: 0.9375
epoch: 441: train_loss: 0.6945536193040337, train_acc: 0.5461309552192688, val_loss: 0.552139937877655, val_acc: 1.0
epoch: 442: train_loss: 0.6946009269910611, train_acc: 0.46875, val_loss: 0.5622805655002594, val_acc: 0.984375
epoch: 443: train_loss: 0.6943478344215283, train_acc: 0.632440467675527, val_loss: 0.5517399311065674, val_acc: 1.0
epoch: 444: train_loss: 0.6941046862789749, train_acc: 0.6071428656578064, val_loss: 0.5514973700046539, val_acc: 1.0
epoch: 445: train_loss: 0.6938751552627043, train_acc: 0.625, val_loss: 0.5610804259777069, val_acc: 0.984375
epoch: 446: train_loss: 0.6936877251871475, train_acc: 0.5922619104385376, val_loss: 0.5883289873600006, val_acc: 0.9375
epoch: 447: train_loss: 0.6935273034808531, train_acc: 0.5669642885526022, val_loss: 0.552720308303833, val_acc: 1.0
epoch: 448: train_loss: 0.6933931761214528, train_acc: 0.5520833333333334, val_loss: 0.6214045286178589, val_acc: 0.9375
epoch: 449: train_loss: 0.6934375759186562, train_acc: 0.4494047562281291, val_loss: 0.5578701794147491, val_acc: 1.0
epoch: 450: train_loss: 0.6934491519080379, train_acc: 0.4880952338377635, val_loss: 0.5521145164966583, val_acc: 1.0
epoch: 451: train_loss: 0.6933332785172793, train_acc: 0.555059532324473, val_loss: 0.5699264407157898, val_acc: 0.984375
epoch: 452: train_loss: 0.693316919804496, train_acc: 0.511904756228129, val_loss: 0.6136884689331055, val_acc: 0.9375
epoch: 453: train_loss: 0.6933340025674574, train_acc: 0.4836309552192688, val_loss: 0.5679457783699036, val_acc: 0.984375
epoch: 454: train_loss: 0.6931865342152419, train_acc: 0.5967261989911398, val_loss: 0.552653580904007, val_acc: 1.0
epoch: 455: train_loss: 0.6931057925504893, train_acc: 0.5818452338377634, val_loss: 0.5663157403469086, val_acc: 0.984375
epoch: 456: train_loss: 0.6929502721731888, train_acc: 0.6071428656578064, val_loss: 0.5787442624568939, val_acc: 0.984375
epoch: 457: train_loss: 0.6929907692268247, train_acc: 0.4747023781140645, val_loss: 0.5738366842269897, val_acc: 0.984375
epoch: 458: train_loss: 0.6930751522425381, train_acc: 0.4627976218859355, val_loss: 0.5680052042007446, val_acc: 0.984375
epoch: 459: train_loss: 0.6930035660016358, train_acc: 0.5773809552192688, val_loss: 0.5534537434577942, val_acc: 1.0
epoch: 460: train_loss: 0.6932123378619535, train_acc: 0.3839285671710968, val_loss: 0.5543917119503021, val_acc: 1.0
epoch: 461: train_loss: 0.6930722767570485, train_acc: 0.5610119005044302, val_loss: 0.5666176378726959, val_acc: 0.984375
epoch: 462: train_loss: 0.69311157248705, train_acc: 0.4940476218859355, val_loss: 0.5515746772289276, val_acc: 1.0
epoch: 463: train_loss: 0.6931810024483446, train_acc: 0.4672619005044301, val_loss: 0.6137474179267883, val_acc: 0.9375
epoch: 464: train_loss: 0.6931621859150543, train_acc: 0.5029761989911398, val_loss: 0.6147936284542084, val_acc: 0.9375
epoch: 465: train_loss: 0.6928965488771166, train_acc: 0.6428571343421936, val_loss: 0.5697636902332306, val_acc: 0.984375
epoch: 466: train_loss: 0.6928838869020987, train_acc: 0.511904756228129, val_loss: 0.5613169074058533, val_acc: 0.984375
epoch: 467: train_loss: 0.6926905704487079, train_acc: 0.5803571343421936, val_loss: 0.5770319402217865, val_acc: 0.984375
epoch: 468: train_loss: 0.6925170857303088, train_acc: 0.5684523781140646, val_loss: 0.5525752902030945, val_acc: 1.0
epoch: 469: train_loss: 0.6922866450767985, train_acc: 0.6101190447807312, val_loss: 0.5528959631919861, val_acc: 1.0
epoch: 470: train_loss: 0.6922726661747254, train_acc: 0.511904756228129, val_loss: 0.5522068440914154, val_acc: 1.0
epoch: 471: train_loss: 0.6922729706738959, train_acc: 0.4836309552192688, val_loss: 0.5705535709857941, val_acc: 0.96875
epoch: 472: train_loss: 0.6922841110936845, train_acc: 0.5104166666666666, val_loss: 0.5568061769008636, val_acc: 1.0
epoch: 473: train_loss: 0.6921677722080845, train_acc: 0.5580357114473978, val_loss: 0.5517790913581848, val_acc: 1.0
epoch: 474: train_loss: 0.692156768368001, train_acc: 0.5029761989911398, val_loss: 0.5568499565124512, val_acc: 1.0
epoch: 475: train_loss: 0.6921852366471819, train_acc: 0.46875, val_loss: 0.5667825043201447, val_acc: 0.984375
epoch: 476: train_loss: 0.6922493583043078, train_acc: 0.4583333333333333, val_loss: 0.551507294178009, val_acc: 1.0
epoch: 477: train_loss: 0.6924480528803212, train_acc: 0.3630952338377635, val_loss: 0.6194486618041992, val_acc: 0.9375
epoch: 478: train_loss: 0.6923479294934529, train_acc: 0.5610119005044302, val_loss: 0.5534672439098358, val_acc: 1.0
epoch: 479: train_loss: 0.6923288420463596, train_acc: 0.5059523781140646, val_loss: 0.5671018660068512, val_acc: 0.984375
epoch: 480: train_loss: 0.6924241023888122, train_acc: 0.4300595223903656, val_loss: 0.5529825091362, val_acc: 1.0
epoch: 481: train_loss: 0.6922975390077785, train_acc: 0.555059532324473, val_loss: 0.5517757833003998, val_acc: 1.0
epoch: 482: train_loss: 0.6921647885612484, train_acc: 0.5580357114473978, val_loss: 0.5514628887176514, val_acc: 1.0
epoch: 483: train_loss: 0.6923867625186917, train_acc: 0.4136904776096344, val_loss: 0.557403177022934, val_acc: 1.0
epoch: 484: train_loss: 0.6925795523366561, train_acc: 0.3869047661622365, val_loss: 0.5622971057891846, val_acc: 0.984375
epoch: 485: train_loss: 0.6925862823780339, train_acc: 0.5059523781140646, val_loss: 0.6143804490566254, val_acc: 0.9375
epoch: 486: train_loss: 0.6924942922176026, train_acc: 0.5580357114473978, val_loss: 0.5534570515155792, val_acc: 1.0
epoch: 487: train_loss: 0.6923600876388323, train_acc: 0.581845243771871, val_loss: 0.5657397508621216, val_acc: 0.984375
epoch: 488: train_loss: 0.6922061742774136, train_acc: 0.5952380895614624, val_loss: 0.5613053739070892, val_acc: 1.0
epoch: 489: train_loss: 0.692168059154432, train_acc: 0.5267857114473978, val_loss: 0.5741704404354095, val_acc: 0.96875
epoch: 490: train_loss: 0.6921577305107474, train_acc: 0.523809532324473, val_loss: 0.5633023083209991, val_acc: 0.984375
epoch: 491: train_loss: 0.6920591325456206, train_acc: 0.5565476218859354, val_loss: 0.552914023399353, val_acc: 1.0
epoch: 492: train_loss: 0.6919945548159431, train_acc: 0.5416666666666666, val_loss: 0.5786192417144775, val_acc: 0.984375
epoch: 493: train_loss: 0.6919430273103644, train_acc: 0.5252976218859354, val_loss: 0.5903255045413971, val_acc: 0.9375
epoch: 494: train_loss: 0.6918891059027772, train_acc: 0.5297619005044302, val_loss: 0.5516092479228973, val_acc: 1.0
epoch: 495: train_loss: 0.6920512744015257, train_acc: 0.3913690447807312, val_loss: 0.5514533817768097, val_acc: 1.0
epoch: 496: train_loss: 0.692048800423351, train_acc: 0.5074404776096344, val_loss: 0.5520916283130646, val_acc: 1.0
epoch: 497: train_loss: 0.6920219069743246, train_acc: 0.511904756228129, val_loss: 0.5514744818210602, val_acc: 1.0
epoch: 498: train_loss: 0.6919887389990189, train_acc: 0.5342261989911398, val_loss: 0.5517795383930206, val_acc: 1.0
epoch: 499: train_loss: 0.6918671454588566, train_acc: 0.5788690447807312, val_loss: 0.5587091743946075, val_acc: 0.984375
0.9765110611915588 0.5833333432674408
Accuracy of the network on the test images: 58%
F1 Score: 0.5872046186895812
precision_score: 0.6830357142857143
recall_total: 0.5892857142857143
Accuracy for class: Alligator Cracks is 40.0 %
F1 score: 1.0
precision_score: 1.0
recall_score: 1.0
Accuracy for class: Longitudinal Cracks is 81.0 %
F1 score: 0.3029350104821803
precision_score: 0.2229938271604938
recall_score: 0.4722222222222222
Accuracy for class: Transverse Cracks is 46.7 %
F1 score: 0.6805555555555557
precision_score: 0.6049382716049383
recall_score: 0.7777777777777778
