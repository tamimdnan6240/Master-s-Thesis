Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Transverse Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Alligator Cracks Alligator Cracks Longitudinal Cracks Alligator Cracks Alligator Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks Transverse Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.2282310724258423, train_acc: 0.3065476218859355, val_loss: 1.0960697531700134, val_acc: 0.546875
epoch: 1: train_loss: 1.1600233912467957, train_acc: 0.3675595223903656, val_loss: 1.0598781108856201, val_acc: 0.421875
epoch: 2: train_loss: 1.1414854791429307, train_acc: 0.3645833333333333, val_loss: 1.066985011100769, val_acc: 0.390625
epoch: 3: train_loss: 1.1119409501552582, train_acc: 0.4002976218859355, val_loss: 1.0177603960037231, val_acc: 0.765625
epoch: 4: train_loss: 1.081631843249003, train_acc: 0.4315476218859355, val_loss: 1.002167671918869, val_acc: 0.46875
epoch: 5: train_loss: 1.0686464409033458, train_acc: 0.3794642885526021, val_loss: 1.061574637889862, val_acc: 0.46875
epoch: 6: train_loss: 1.0463075666200548, train_acc: 0.4776785671710968, val_loss: 1.0750829577445984, val_acc: 0.34375
epoch: 7: train_loss: 1.020018900434176, train_acc: 0.5342261989911398, val_loss: 0.8665189445018768, val_acc: 0.6875
epoch: 8: train_loss: 0.9997595614857142, train_acc: 0.5163690447807312, val_loss: 0.9669615924358368, val_acc: 0.609375
epoch: 9: train_loss: 0.9820474425951637, train_acc: 0.517857144276301, val_loss: 0.8550242483615875, val_acc: 0.734375
epoch: 10: train_loss: 0.973615456711162, train_acc: 0.4568452338377635, val_loss: 0.8726420104503632, val_acc: 0.734375
epoch: 11: train_loss: 0.9621584912141162, train_acc: 0.4464285671710968, val_loss: 0.8129574954509735, val_acc: 0.671875
epoch: 12: train_loss: 0.951122007308862, train_acc: 0.4702380895614624, val_loss: 0.7973271906375885, val_acc: 0.875
epoch: 13: train_loss: 0.9365811802092051, train_acc: 0.5535714228947958, val_loss: 0.799755185842514, val_acc: 0.75
epoch: 14: train_loss: 0.9246323638492159, train_acc: 0.5208333333333334, val_loss: 0.8033255040645599, val_acc: 0.78125
epoch: 15: train_loss: 0.9173985893527665, train_acc: 0.4642857114473979, val_loss: 0.7511326968669891, val_acc: 0.828125
epoch: 16: train_loss: 0.9129843863786434, train_acc: 0.4345238109429677, val_loss: 0.9171748161315918, val_acc: 0.53125
epoch: 17: train_loss: 0.9069107550161853, train_acc: 0.4598214228947957, val_loss: 0.7411049008369446, val_acc: 0.765625
epoch: 18: train_loss: 0.8944650742045618, train_acc: 0.5729166666666666, val_loss: 0.731141984462738, val_acc: 0.828125
epoch: 19: train_loss: 0.8892027785380678, train_acc: 0.4583333333333333, val_loss: 0.7140123546123505, val_acc: 0.8125
epoch: 20: train_loss: 0.8818828681158638, train_acc: 0.4761904776096344, val_loss: 0.7910028696060181, val_acc: 0.765625
epoch: 21: train_loss: 0.8750010620463975, train_acc: 0.4642857114473979, val_loss: 0.7559550702571869, val_acc: 0.859375
epoch: 22: train_loss: 0.8700907230377194, train_acc: 0.46875, val_loss: 0.6876853704452515, val_acc: 0.90625
epoch: 23: train_loss: 0.8623150438070294, train_acc: 0.5788690447807312, val_loss: 0.6659477949142456, val_acc: 0.953125
epoch: 24: train_loss: 0.855476428667704, train_acc: 0.5416666666666666, val_loss: 0.6414943337440491, val_acc: 0.9375
epoch: 25: train_loss: 0.85029077453491, train_acc: 0.5193452338377634, val_loss: 0.6739287078380585, val_acc: 0.875
epoch: 26: train_loss: 0.8461197283532882, train_acc: 0.4895833333333333, val_loss: 0.6737104058265686, val_acc: 0.875
epoch: 27: train_loss: 0.843749221591722, train_acc: 0.4479166666666667, val_loss: 0.6365310549736023, val_acc: 0.921875
epoch: 28: train_loss: 0.8373879495708418, train_acc: 0.5505952338377634, val_loss: 0.6167398393154144, val_acc: 0.96875
epoch: 29: train_loss: 0.8336913420094381, train_acc: 0.5, val_loss: 0.6175761520862579, val_acc: 0.921875
epoch: 30: train_loss: 0.8290892461294767, train_acc: 0.5327380895614624, val_loss: 0.6117717027664185, val_acc: 0.9375
epoch: 31: train_loss: 0.824242480099201, train_acc: 0.5580357114473978, val_loss: 0.6318159997463226, val_acc: 0.9375
epoch: 32: train_loss: 0.8173106342855124, train_acc: 0.6264880895614624, val_loss: 0.6467321515083313, val_acc: 0.921875
epoch: 33: train_loss: 0.811814404585782, train_acc: 0.6086309552192688, val_loss: 0.5823858380317688, val_acc: 1.0
epoch: 34: train_loss: 0.8101327981267654, train_acc: 0.4375, val_loss: 0.5719762742519379, val_acc: 1.0
epoch: 35: train_loss: 0.8067721266437458, train_acc: 0.543154756228129, val_loss: 0.6240894496440887, val_acc: 0.921875
epoch: 36: train_loss: 0.8039878386634962, train_acc: 0.5044642885526022, val_loss: 0.6046980023384094, val_acc: 0.953125
epoch: 37: train_loss: 0.8008579449695451, train_acc: 0.550595243771871, val_loss: 0.5769326984882355, val_acc: 0.984375
epoch: 38: train_loss: 0.796468594135382, train_acc: 0.5892857114473978, val_loss: 0.5656157433986664, val_acc: 1.0
epoch: 39: train_loss: 0.7935629064838089, train_acc: 0.5520833333333334, val_loss: 0.5880327820777893, val_acc: 0.96875
epoch: 40: train_loss: 0.7918113639684226, train_acc: 0.4627976218859355, val_loss: 0.5838871002197266, val_acc: 0.984375
epoch: 41: train_loss: 0.7910947482737282, train_acc: 0.4449404776096344, val_loss: 0.6296014487743378, val_acc: 0.921875
epoch: 42: train_loss: 0.7873852862868197, train_acc: 0.5758928656578064, val_loss: 0.6155526638031006, val_acc: 0.9375
epoch: 43: train_loss: 0.7867796023686725, train_acc: 0.4479166666666667, val_loss: 0.5814136564731598, val_acc: 0.984375
epoch: 44: train_loss: 0.7840155031945969, train_acc: 0.5297619005044302, val_loss: 0.5846213102340698, val_acc: 0.984375
epoch: 45: train_loss: 0.7809245249499444, train_acc: 0.5461309552192688, val_loss: 0.605065792798996, val_acc: 0.984375
epoch: 46: train_loss: 0.7811138460822137, train_acc: 0.4032738109429677, val_loss: 0.5877830684185028, val_acc: 0.96875
epoch: 47: train_loss: 0.7793293971982265, train_acc: 0.4910714228947957, val_loss: 0.575312465429306, val_acc: 0.984375
epoch: 48: train_loss: 0.7760385814167202, train_acc: 0.5654761989911398, val_loss: 0.6213873624801636, val_acc: 0.9375
epoch: 49: train_loss: 0.7750749540328976, train_acc: 0.4583333333333333, val_loss: 0.5810623466968536, val_acc: 0.984375
epoch: 50: train_loss: 0.7740174937092398, train_acc: 0.4642857114473979, val_loss: 0.5843054056167603, val_acc: 0.96875
epoch: 51: train_loss: 0.7707194842589205, train_acc: 0.5982142885526022, val_loss: 0.6061399281024933, val_acc: 0.921875
epoch: 52: train_loss: 0.7687631766751125, train_acc: 0.543154756228129, val_loss: 0.5818077027797699, val_acc: 0.984375
epoch: 53: train_loss: 0.7676554852061799, train_acc: 0.4791666666666667, val_loss: 0.5961090624332428, val_acc: 0.984375
epoch: 54: train_loss: 0.7673901135271243, train_acc: 0.4360119005044301, val_loss: 0.5809021890163422, val_acc: 1.0
epoch: 55: train_loss: 0.7659345081164721, train_acc: 0.517857144276301, val_loss: 0.6268224716186523, val_acc: 0.90625
epoch: 56: train_loss: 0.7635884483655291, train_acc: 0.574404756228129, val_loss: 0.623892605304718, val_acc: 0.9375
epoch: 57: train_loss: 0.7626025286214102, train_acc: 0.5044642885526022, val_loss: 0.5733258724212646, val_acc: 1.0
epoch: 58: train_loss: 0.7612066161161086, train_acc: 0.543154756228129, val_loss: 0.5810196399688721, val_acc: 1.0
epoch: 59: train_loss: 0.7595531890789665, train_acc: 0.5252976218859354, val_loss: 0.5895527005195618, val_acc: 0.953125
epoch: 60: train_loss: 0.7587741307222123, train_acc: 0.5, val_loss: 0.5701098144054413, val_acc: 1.0
epoch: 61: train_loss: 0.7566046939101267, train_acc: 0.555059532324473, val_loss: 0.5966529548168182, val_acc: 0.9375
epoch: 62: train_loss: 0.7550027477678165, train_acc: 0.555059532324473, val_loss: 0.6299050450325012, val_acc: 0.921875
epoch: 63: train_loss: 0.7527046290536719, train_acc: 0.6339285771052042, val_loss: 0.612795889377594, val_acc: 0.984375
epoch: 64: train_loss: 0.7509885051311588, train_acc: 0.5788690447807312, val_loss: 0.5786484777927399, val_acc: 0.96875
epoch: 65: train_loss: 0.7498097323408027, train_acc: 0.5133928656578064, val_loss: 0.6327564716339111, val_acc: 0.90625
epoch: 66: train_loss: 0.7482695973927699, train_acc: 0.543154756228129, val_loss: 0.574338972568512, val_acc: 0.984375
epoch: 67: train_loss: 0.7473946765941729, train_acc: 0.4895833333333333, val_loss: 0.5751191675662994, val_acc: 0.984375
epoch: 68: train_loss: 0.7452550081239228, train_acc: 0.5892857114473978, val_loss: 0.5738039910793304, val_acc: 0.984375
epoch: 69: train_loss: 0.7450002287115366, train_acc: 0.4657738109429677, val_loss: 0.6065837740898132, val_acc: 0.984375
epoch: 70: train_loss: 0.7436587916853279, train_acc: 0.543154756228129, val_loss: 0.5815366804599762, val_acc: 0.96875
epoch: 71: train_loss: 0.7419328904814187, train_acc: 0.5967261989911398, val_loss: 0.5733073651790619, val_acc: 1.0
epoch: 72: train_loss: 0.7420808193890469, train_acc: 0.4330357114473979, val_loss: 0.5993346869945526, val_acc: 0.96875
epoch: 73: train_loss: 0.7424910358480502, train_acc: 0.4241071442763011, val_loss: 0.5672274827957153, val_acc: 0.984375
epoch: 74: train_loss: 0.7425840346018471, train_acc: 0.4434523781140645, val_loss: 0.5630117654800415, val_acc: 1.0
epoch: 75: train_loss: 0.7413082956744911, train_acc: 0.5654761989911398, val_loss: 0.5843875110149384, val_acc: 0.953125
epoch: 76: train_loss: 0.7411295807722841, train_acc: 0.4895833333333333, val_loss: 0.5709361732006073, val_acc: 0.984375
epoch: 77: train_loss: 0.7409610157338979, train_acc: 0.46875, val_loss: 0.582877516746521, val_acc: 0.96875
epoch: 78: train_loss: 0.7404354996822049, train_acc: 0.5208333333333334, val_loss: 0.5991494059562683, val_acc: 0.96875
epoch: 79: train_loss: 0.7394407366712886, train_acc: 0.543154756228129, val_loss: 0.5741757750511169, val_acc: 0.984375
epoch: 80: train_loss: 0.739100600466316, train_acc: 0.5104166666666666, val_loss: 0.611745297908783, val_acc: 0.9375
epoch: 81: train_loss: 0.7380948829941631, train_acc: 0.5505952338377634, val_loss: 0.6116019785404205, val_acc: 0.90625
epoch: 82: train_loss: 0.7372516728788013, train_acc: 0.5476190447807312, val_loss: 0.6456236839294434, val_acc: 0.90625
epoch: 83: train_loss: 0.7371180620458389, train_acc: 0.4866071442763011, val_loss: 0.5928640961647034, val_acc: 0.96875
epoch: 84: train_loss: 0.73692691115772, train_acc: 0.4702380895614624, val_loss: 0.6084739565849304, val_acc: 0.96875
epoch: 85: train_loss: 0.7368444585522937, train_acc: 0.5, val_loss: 0.5658305585384369, val_acc: 0.984375
epoch: 86: train_loss: 0.7364890139221686, train_acc: 0.5133928656578064, val_loss: 0.5824249684810638, val_acc: 1.0
epoch: 87: train_loss: 0.7357526410258175, train_acc: 0.53125, val_loss: 0.6468743681907654, val_acc: 0.90625
epoch: 88: train_loss: 0.7356149623456517, train_acc: 0.4598214228947957, val_loss: 0.5894795656204224, val_acc: 0.96875
epoch: 89: train_loss: 0.7350211512159416, train_acc: 0.523809532324473, val_loss: 0.5593438148498535, val_acc: 1.0
epoch: 90: train_loss: 0.7338818154492218, train_acc: 0.555059532324473, val_loss: 0.5725315809249878, val_acc: 1.0
epoch: 91: train_loss: 0.7332428968039109, train_acc: 0.5104166666666666, val_loss: 0.5915156006813049, val_acc: 1.0
epoch: 92: train_loss: 0.73293422798102, train_acc: 0.5014880895614624, val_loss: 0.563123345375061, val_acc: 0.984375
epoch: 93: train_loss: 0.7328536368853653, train_acc: 0.4598214228947957, val_loss: 0.5803268253803253, val_acc: 0.96875
epoch: 94: train_loss: 0.7335049794431315, train_acc: 0.4122023781140645, val_loss: 0.6426701247692108, val_acc: 0.90625
epoch: 95: train_loss: 0.7330389376729723, train_acc: 0.5014880994955698, val_loss: 0.589933454990387, val_acc: 0.953125
epoch: 96: train_loss: 0.7318007757573599, train_acc: 0.6190476218859354, val_loss: 0.5894903242588043, val_acc: 0.984375
epoch: 97: train_loss: 0.7313652772481746, train_acc: 0.5252976218859354, val_loss: 0.5838639438152313, val_acc: 1.0
epoch: 98: train_loss: 0.7300209944898428, train_acc: 0.6026785771052042, val_loss: 0.5912044942378998, val_acc: 0.984375
epoch: 99: train_loss: 0.7297257141272223, train_acc: 0.4955357114473979, val_loss: 0.5694105923175812, val_acc: 0.984375
epoch: 100: train_loss: 0.7300211586574512, train_acc: 0.4568452338377635, val_loss: 0.5598669946193695, val_acc: 1.0
epoch: 101: train_loss: 0.7294224171467073, train_acc: 0.543154756228129, val_loss: 0.575237512588501, val_acc: 0.984375
epoch: 102: train_loss: 0.7291899161431392, train_acc: 0.5014880895614624, val_loss: 0.560088187456131, val_acc: 1.0
epoch: 103: train_loss: 0.7292352624428573, train_acc: 0.4508928656578064, val_loss: 0.5750706791877747, val_acc: 0.984375
epoch: 104: train_loss: 0.7284355814494778, train_acc: 0.5535714228947958, val_loss: 0.5812956094741821, val_acc: 0.9375
epoch: 105: train_loss: 0.7285544216257966, train_acc: 0.4360119005044301, val_loss: 0.5695547461509705, val_acc: 0.984375
epoch: 106: train_loss: 0.7273464548253563, train_acc: 0.6041666666666666, val_loss: 0.5718520879745483, val_acc: 0.984375
epoch: 107: train_loss: 0.7271863884396019, train_acc: 0.4880952338377635, val_loss: 0.6123411357402802, val_acc: 0.9375
epoch: 108: train_loss: 0.726557942340862, train_acc: 0.581845243771871, val_loss: 0.5928107798099518, val_acc: 0.9375
epoch: 109: train_loss: 0.7265261886697822, train_acc: 0.4657738109429677, val_loss: 0.5863914787769318, val_acc: 0.96875
epoch: 110: train_loss: 0.7262039075384622, train_acc: 0.5089285671710968, val_loss: 0.5772814154624939, val_acc: 0.984375
epoch: 111: train_loss: 0.7259842183973103, train_acc: 0.5148809552192688, val_loss: 0.5872913002967834, val_acc: 0.96875
epoch: 112: train_loss: 0.7255274929479858, train_acc: 0.5401785671710968, val_loss: 0.6034775078296661, val_acc: 0.9375
epoch: 113: train_loss: 0.7254490285937544, train_acc: 0.4747023781140645, val_loss: 0.5808712244033813, val_acc: 0.953125
epoch: 114: train_loss: 0.7249876284944833, train_acc: 0.5223214228947958, val_loss: 0.5716038346290588, val_acc: 0.984375
epoch: 115: train_loss: 0.7245051024974072, train_acc: 0.5595238010088602, val_loss: 0.5883574187755585, val_acc: 0.984375
epoch: 116: train_loss: 0.7241043434863071, train_acc: 0.5729166666666666, val_loss: 0.5798709690570831, val_acc: 0.984375
epoch: 117: train_loss: 0.7237276392804696, train_acc: 0.511904756228129, val_loss: 0.5839861035346985, val_acc: 0.96875
epoch: 118: train_loss: 0.7233780884609163, train_acc: 0.543154756228129, val_loss: 0.5716228485107422, val_acc: 0.984375
epoch: 119: train_loss: 0.722798877788914, train_acc: 0.5327380895614624, val_loss: 0.5718208849430084, val_acc: 1.0
epoch: 120: train_loss: 0.7229058108710712, train_acc: 0.513392855723699, val_loss: 0.6030301153659821, val_acc: 0.953125
epoch: 121: train_loss: 0.722501705578767, train_acc: 0.5208333333333334, val_loss: 0.5686335563659668, val_acc: 0.984375
epoch: 122: train_loss: 0.7222870068175354, train_acc: 0.4880952338377635, val_loss: 0.5871016085147858, val_acc: 0.953125
epoch: 123: train_loss: 0.7220381440654872, train_acc: 0.5386904776096344, val_loss: 0.6442886590957642, val_acc: 0.90625
epoch: 124: train_loss: 0.7213691492080683, train_acc: 0.586309532324473, val_loss: 0.5761493742465973, val_acc: 0.96875
epoch: 125: train_loss: 0.7206067250519197, train_acc: 0.5982142885526022, val_loss: 0.6254962980747223, val_acc: 0.890625
epoch: 126: train_loss: 0.7208816171318209, train_acc: 0.4583333333333333, val_loss: 0.6158865392208099, val_acc: 0.9375
epoch: 127: train_loss: 0.7205848417555286, train_acc: 0.5029761989911398, val_loss: 0.560423731803894, val_acc: 1.0
epoch: 128: train_loss: 0.7207828084011713, train_acc: 0.4330357114473979, val_loss: 0.621213972568512, val_acc: 0.90625
epoch: 129: train_loss: 0.7209640628252268, train_acc: 0.4717261989911397, val_loss: 0.5846602320671082, val_acc: 0.96875
epoch: 130: train_loss: 0.7205671988067427, train_acc: 0.5372023781140646, val_loss: 0.6118413805961609, val_acc: 0.953125
epoch: 131: train_loss: 0.7208644997591919, train_acc: 0.4776785671710968, val_loss: 0.5859156847000122, val_acc: 0.984375
epoch: 132: train_loss: 0.7202221143215815, train_acc: 0.5729166666666666, val_loss: 0.5977332592010498, val_acc: 0.953125
epoch: 133: train_loss: 0.7201854826799075, train_acc: 0.5193452338377634, val_loss: 0.6949364840984344, val_acc: 0.875
epoch: 134: train_loss: 0.7204998206209248, train_acc: 0.4761904776096344, val_loss: 0.5993054211139679, val_acc: 0.96875
epoch: 135: train_loss: 0.7203280896532763, train_acc: 0.4940476218859355, val_loss: 0.571132242679596, val_acc: 0.984375
epoch: 136: train_loss: 0.7202425388814173, train_acc: 0.4776785671710968, val_loss: 0.568520724773407, val_acc: 0.984375
epoch: 137: train_loss: 0.720015543814442, train_acc: 0.5074404776096344, val_loss: 0.6262615919113159, val_acc: 0.921875
epoch: 138: train_loss: 0.7199689831665088, train_acc: 0.4880952338377635, val_loss: 0.5769467055797577, val_acc: 0.96875
epoch: 139: train_loss: 0.7203196874686644, train_acc: 0.4092261890570323, val_loss: 0.5705512464046478, val_acc: 0.984375
epoch: 140: train_loss: 0.7210206224563268, train_acc: 0.3764880895614624, val_loss: 0.5584549307823181, val_acc: 1.0
epoch: 141: train_loss: 0.7204246676303965, train_acc: 0.5758928656578064, val_loss: 0.5748924314975739, val_acc: 0.96875
epoch: 142: train_loss: 0.7198851674030983, train_acc: 0.5476190447807312, val_loss: 0.583697110414505, val_acc: 0.984375
epoch: 143: train_loss: 0.7194551833801793, train_acc: 0.523809532324473, val_loss: 0.6299204528331757, val_acc: 0.890625
epoch: 144: train_loss: 0.7191739656459321, train_acc: 0.5282738109429678, val_loss: 0.6027615666389465, val_acc: 0.9375
epoch: 145: train_loss: 0.7191059204269212, train_acc: 0.4761904776096344, val_loss: 0.5734958648681641, val_acc: 0.984375
epoch: 146: train_loss: 0.718733343542838, train_acc: 0.5163690447807312, val_loss: 0.5719337463378906, val_acc: 0.984375
epoch: 147: train_loss: 0.718071566642941, train_acc: 0.6116071343421936, val_loss: 0.6097123920917511, val_acc: 0.921875
epoch: 148: train_loss: 0.7178635278537496, train_acc: 0.5014880895614624, val_loss: 0.5747823417186737, val_acc: 0.96875
epoch: 149: train_loss: 0.7169609163867097, train_acc: 0.6116071343421936, val_loss: 0.6296346485614777, val_acc: 0.921875
epoch: 150: train_loss: 0.7168823304292112, train_acc: 0.4717261989911397, val_loss: 0.6442939043045044, val_acc: 0.875
epoch: 151: train_loss: 0.7162652081041999, train_acc: 0.5773809552192688, val_loss: 0.5533546507358551, val_acc: 1.0
epoch: 152: train_loss: 0.7164070899709891, train_acc: 0.4494047562281291, val_loss: 0.5637436509132385, val_acc: 1.0
epoch: 153: train_loss: 0.7167731687362054, train_acc: 0.4017857114473979, val_loss: 0.5882968008518219, val_acc: 0.96875
epoch: 154: train_loss: 0.7166109995175427, train_acc: 0.511904756228129, val_loss: 0.5825410783290863, val_acc: 0.96875
epoch: 155: train_loss: 0.7160126243391601, train_acc: 0.5833333333333334, val_loss: 0.6277891397476196, val_acc: 0.921875
epoch: 156: train_loss: 0.7161095430896535, train_acc: 0.4464285671710968, val_loss: 0.5670081377029419, val_acc: 0.984375
epoch: 157: train_loss: 0.7156934804805718, train_acc: 0.5565476218859354, val_loss: 0.5707385241985321, val_acc: 1.0
epoch: 158: train_loss: 0.7157184585335364, train_acc: 0.4732142885526021, val_loss: 0.568029373884201, val_acc: 0.984375
epoch: 159: train_loss: 0.7155558120459313, train_acc: 0.4985119005044301, val_loss: 0.5570685863494873, val_acc: 1.0
epoch: 160: train_loss: 0.7149260958529402, train_acc: 0.6279761989911398, val_loss: 0.5664478242397308, val_acc: 0.984375
epoch: 161: train_loss: 0.7143653239978188, train_acc: 0.5639880895614624, val_loss: 0.5960780680179596, val_acc: 0.953125
epoch: 162: train_loss: 0.7144550774970176, train_acc: 0.4315476218859355, val_loss: 0.6369271874427795, val_acc: 0.90625
epoch: 163: train_loss: 0.7142133239081231, train_acc: 0.5461309552192688, val_loss: 0.5734725594520569, val_acc: 0.96875
epoch: 164: train_loss: 0.7136753206301213, train_acc: 0.5639880895614624, val_loss: 0.567456841468811, val_acc: 0.984375
epoch: 165: train_loss: 0.712753100567553, train_acc: 0.648809532324473, val_loss: 0.5628183484077454, val_acc: 1.0
epoch: 166: train_loss: 0.7127949911915133, train_acc: 0.4747023781140645, val_loss: 0.5565616190433502, val_acc: 1.0
epoch: 167: train_loss: 0.7124920026177447, train_acc: 0.5416666666666666, val_loss: 0.5534125566482544, val_acc: 1.0
epoch: 168: train_loss: 0.7127639795901504, train_acc: 0.4151785671710968, val_loss: 0.6392744779586792, val_acc: 0.90625
epoch: 169: train_loss: 0.712476427064222, train_acc: 0.5461309552192688, val_loss: 0.5810970366001129, val_acc: 0.9375
epoch: 170: train_loss: 0.7121886307733096, train_acc: 0.5401785671710968, val_loss: 0.5755905508995056, val_acc: 0.984375
epoch: 171: train_loss: 0.711937847525574, train_acc: 0.523809532324473, val_loss: 0.5727696418762207, val_acc: 0.984375
epoch: 172: train_loss: 0.711538024261048, train_acc: 0.5595238010088602, val_loss: 0.5596191883087158, val_acc: 1.0
epoch: 173: train_loss: 0.7120187674217293, train_acc: 0.3883928557236989, val_loss: 0.5677815973758698, val_acc: 0.984375
epoch: 174: train_loss: 0.7119843815621871, train_acc: 0.46875, val_loss: 0.5682714879512787, val_acc: 0.984375
epoch: 175: train_loss: 0.7114794430407607, train_acc: 0.625, val_loss: 0.558014303445816, val_acc: 1.0
epoch: 176: train_loss: 0.7111545477883287, train_acc: 0.5416666666666666, val_loss: 0.5877271890640259, val_acc: 0.953125
epoch: 177: train_loss: 0.7107112640968419, train_acc: 0.5758928656578064, val_loss: 0.5625651180744171, val_acc: 1.0
epoch: 178: train_loss: 0.710565282867829, train_acc: 0.53125, val_loss: 0.5666655898094177, val_acc: 0.984375
epoch: 179: train_loss: 0.7108713318904237, train_acc: 0.4166666666666667, val_loss: 0.5692192614078522, val_acc: 0.984375
epoch: 180: train_loss: 0.7108465765721228, train_acc: 0.5029761989911398, val_loss: 0.6158243417739868, val_acc: 0.9375
epoch: 181: train_loss: 0.7106227531974566, train_acc: 0.5208333333333334, val_loss: 0.5816252827644348, val_acc: 0.9375
epoch: 182: train_loss: 0.7103059341781558, train_acc: 0.5416666666666666, val_loss: 0.5873682796955109, val_acc: 0.96875
epoch: 183: train_loss: 0.7098229881645974, train_acc: 0.5967261989911398, val_loss: 0.615994393825531, val_acc: 0.921875
epoch: 184: train_loss: 0.7098308457984578, train_acc: 0.4866071442763011, val_loss: 0.5637059509754181, val_acc: 0.984375
epoch: 185: train_loss: 0.7098327986228421, train_acc: 0.4880952338377635, val_loss: 0.6940086185932159, val_acc: 0.875
epoch: 186: train_loss: 0.709496970163947, train_acc: 0.5610119005044302, val_loss: 0.5786210894584656, val_acc: 0.984375
epoch: 187: train_loss: 0.7095899492079483, train_acc: 0.4479166666666667, val_loss: 0.5615381598472595, val_acc: 0.984375
epoch: 188: train_loss: 0.7093284164786968, train_acc: 0.523809532324473, val_loss: 0.5524903535842896, val_acc: 1.0
epoch: 189: train_loss: 0.7093242626441151, train_acc: 0.4866071442763011, val_loss: 0.6096760034561157, val_acc: 0.96875
epoch: 190: train_loss: 0.708998809310154, train_acc: 0.5520833333333334, val_loss: 0.6055070459842682, val_acc: 0.9375
epoch: 191: train_loss: 0.7089061044777432, train_acc: 0.4866071442763011, val_loss: 0.5657754242420197, val_acc: 0.984375
epoch: 192: train_loss: 0.7086012951244756, train_acc: 0.5520833333333334, val_loss: 0.6198387145996094, val_acc: 0.921875
epoch: 193: train_loss: 0.7082633461124708, train_acc: 0.5416666666666666, val_loss: 0.6686831414699554, val_acc: 0.90625
epoch: 194: train_loss: 0.7078022630805643, train_acc: 0.5848214228947958, val_loss: 0.5822734534740448, val_acc: 0.96875
epoch: 195: train_loss: 0.7074986693202231, train_acc: 0.5327380895614624, val_loss: 0.6280442178249359, val_acc: 0.921875
epoch: 196: train_loss: 0.7073824604352313, train_acc: 0.5223214228947958, val_loss: 0.576582670211792, val_acc: 1.0
epoch: 197: train_loss: 0.7068496593321211, train_acc: 0.5788690447807312, val_loss: 0.5725132822990417, val_acc: 0.96875
epoch: 198: train_loss: 0.7066435586267978, train_acc: 0.5372023781140646, val_loss: 0.5761823952198029, val_acc: 1.0
epoch: 199: train_loss: 0.706611663897832, train_acc: 0.4985119005044301, val_loss: 0.565022349357605, val_acc: 0.984375
epoch: 200: train_loss: 0.7066447528242865, train_acc: 0.4955357114473979, val_loss: 0.5730392932891846, val_acc: 0.984375
epoch: 201: train_loss: 0.706584726131395, train_acc: 0.5044642885526022, val_loss: 0.5701085925102234, val_acc: 1.0
epoch: 202: train_loss: 0.70648889490732, train_acc: 0.5342261989911398, val_loss: 0.5665254294872284, val_acc: 0.984375
epoch: 203: train_loss: 0.7065541977975881, train_acc: 0.4642857114473979, val_loss: 0.576511800289154, val_acc: 0.984375
epoch: 204: train_loss: 0.706491204199752, train_acc: 0.4985119005044301, val_loss: 0.6111888289451599, val_acc: 0.9375
epoch: 205: train_loss: 0.7065405632492792, train_acc: 0.4657738109429677, val_loss: 0.5817332565784454, val_acc: 0.96875
epoch: 206: train_loss: 0.7066269724265386, train_acc: 0.4464285671710968, val_loss: 0.5554639101028442, val_acc: 1.0
epoch: 207: train_loss: 0.7063856443915609, train_acc: 0.5372023781140646, val_loss: 0.6059602797031403, val_acc: 0.921875
epoch: 208: train_loss: 0.7064297294312495, train_acc: 0.4910714228947957, val_loss: 0.5806551277637482, val_acc: 0.96875
epoch: 209: train_loss: 0.7064791519490498, train_acc: 0.5, val_loss: 0.626738578081131, val_acc: 0.921875
epoch: 210: train_loss: 0.7063633106519447, train_acc: 0.5059523781140646, val_loss: 0.6143679320812225, val_acc: 0.9375
epoch: 211: train_loss: 0.7058514551077997, train_acc: 0.613095243771871, val_loss: 0.5720021724700928, val_acc: 1.0
epoch: 212: train_loss: 0.705644542855649, train_acc: 0.5089285671710968, val_loss: 0.5907330214977264, val_acc: 0.984375
epoch: 213: train_loss: 0.7051840892926182, train_acc: 0.5833333333333334, val_loss: 0.5630769729614258, val_acc: 0.984375
epoch: 214: train_loss: 0.7052841104740319, train_acc: 0.4568452338377635, val_loss: 0.5595709383487701, val_acc: 1.0
epoch: 215: train_loss: 0.7051615821266615, train_acc: 0.5059523781140646, val_loss: 0.6117595434188843, val_acc: 0.9375
epoch: 216: train_loss: 0.704567361766109, train_acc: 0.6309523781140646, val_loss: 0.5752195715904236, val_acc: 0.984375
epoch: 217: train_loss: 0.7046595161569956, train_acc: 0.4672619005044301, val_loss: 0.5663364827632904, val_acc: 0.984375
epoch: 218: train_loss: 0.7038014571053434, train_acc: 0.711309532324473, val_loss: 0.5710626542568207, val_acc: 0.96875
epoch: 219: train_loss: 0.7037919749816258, train_acc: 0.5208333333333334, val_loss: 0.5753810107707977, val_acc: 0.96875
epoch: 220: train_loss: 0.7037123503728149, train_acc: 0.5327380895614624, val_loss: 0.5696409940719604, val_acc: 0.984375
epoch: 221: train_loss: 0.7035909611899572, train_acc: 0.5580357114473978, val_loss: 0.5628983974456787, val_acc: 0.984375
epoch: 222: train_loss: 0.7035632911284407, train_acc: 0.5104166666666666, val_loss: 0.63132044672966, val_acc: 0.921875
epoch: 223: train_loss: 0.7032102506075585, train_acc: 0.569940467675527, val_loss: 0.5980543494224548, val_acc: 0.921875
epoch: 224: train_loss: 0.7034123753618309, train_acc: 0.4776785671710968, val_loss: 0.5590576827526093, val_acc: 0.984375
epoch: 225: train_loss: 0.7032501585012338, train_acc: 0.5357142885526022, val_loss: 0.5594632029533386, val_acc: 1.0
epoch: 226: train_loss: 0.7028022168666429, train_acc: 0.6190476218859354, val_loss: 0.5524949133396149, val_acc: 1.0
epoch: 227: train_loss: 0.7025604282031979, train_acc: 0.5610119005044302, val_loss: 0.5786350071430206, val_acc: 0.96875
epoch: 228: train_loss: 0.7021914612951929, train_acc: 0.5833333333333334, val_loss: 0.5688701868057251, val_acc: 0.984375
epoch: 229: train_loss: 0.7023311775663623, train_acc: 0.4479166666666667, val_loss: 0.6255977153778076, val_acc: 0.90625
epoch: 230: train_loss: 0.7024282480215096, train_acc: 0.4627976218859355, val_loss: 0.6334337294101715, val_acc: 0.921875
epoch: 231: train_loss: 0.7025085760430357, train_acc: 0.4821428656578064, val_loss: 0.5867059230804443, val_acc: 0.96875
epoch: 232: train_loss: 0.7025269927385026, train_acc: 0.4776785671710968, val_loss: 0.6515074074268341, val_acc: 0.875
epoch: 233: train_loss: 0.7024705263114722, train_acc: 0.5401785671710968, val_loss: 0.6218850910663605, val_acc: 0.921875
epoch: 234: train_loss: 0.7022887957857009, train_acc: 0.543154756228129, val_loss: 0.5593724846839905, val_acc: 1.0
epoch: 235: train_loss: 0.7020367016058183, train_acc: 0.5654761989911398, val_loss: 0.5645802319049835, val_acc: 0.984375
epoch: 236: train_loss: 0.7018133986180676, train_acc: 0.5669642885526022, val_loss: 0.5919909179210663, val_acc: 0.96875
epoch: 237: train_loss: 0.7019785565822397, train_acc: 0.4404761890570323, val_loss: 0.6463376581668854, val_acc: 0.90625
epoch: 238: train_loss: 0.7020896262537318, train_acc: 0.5059523781140646, val_loss: 0.5910756587982178, val_acc: 0.953125
epoch: 239: train_loss: 0.7018796705537371, train_acc: 0.5446428656578064, val_loss: 0.5661334693431854, val_acc: 0.984375
epoch: 240: train_loss: 0.7015657224595793, train_acc: 0.5729166666666666, val_loss: 0.5670247673988342, val_acc: 0.984375
epoch: 241: train_loss: 0.7015870605125899, train_acc: 0.4776785671710968, val_loss: 0.565956711769104, val_acc: 0.984375
epoch: 242: train_loss: 0.7016816689480806, train_acc: 0.4613095223903656, val_loss: 0.5730184316635132, val_acc: 1.0
epoch: 243: train_loss: 0.7015656874479491, train_acc: 0.5059523781140646, val_loss: 0.5567132234573364, val_acc: 1.0
epoch: 244: train_loss: 0.7013260072591353, train_acc: 0.574404756228129, val_loss: 0.5652275085449219, val_acc: 0.984375
epoch: 245: train_loss: 0.7014139956090508, train_acc: 0.4836309552192688, val_loss: 0.5772914290428162, val_acc: 1.0
epoch: 246: train_loss: 0.7014472615702755, train_acc: 0.4761904776096344, val_loss: 0.5582866668701172, val_acc: 1.0
epoch: 247: train_loss: 0.7011023658898569, train_acc: 0.6145833333333334, val_loss: 0.5574109554290771, val_acc: 1.0
epoch: 248: train_loss: 0.7007280680708776, train_acc: 0.6041666666666666, val_loss: 0.5762858688831329, val_acc: 1.0
epoch: 249: train_loss: 0.7007901261647541, train_acc: 0.4791666666666667, val_loss: 0.6182530522346497, val_acc: 0.921875
epoch: 250: train_loss: 0.7005806028605456, train_acc: 0.5684523781140646, val_loss: 0.568411260843277, val_acc: 0.984375
epoch: 251: train_loss: 0.7004815150032597, train_acc: 0.5148809552192688, val_loss: 0.5619226396083832, val_acc: 1.0
epoch: 252: train_loss: 0.7002941338598176, train_acc: 0.5758928656578064, val_loss: 0.6266737878322601, val_acc: 0.9375
epoch: 253: train_loss: 0.7004482438714484, train_acc: 0.4285714328289032, val_loss: 0.5609376132488251, val_acc: 0.984375
epoch: 254: train_loss: 0.7000315583609287, train_acc: 0.5982142885526022, val_loss: 0.5581961572170258, val_acc: 1.0
epoch: 255: train_loss: 0.6999295514542608, train_acc: 0.5148809552192688, val_loss: 0.5634391605854034, val_acc: 1.0
epoch: 256: train_loss: 0.69996398061166, train_acc: 0.4761904776096344, val_loss: 0.5608820021152496, val_acc: 0.984375
epoch: 257: train_loss: 0.7000808220655109, train_acc: 0.4880952338377635, val_loss: 0.5926630795001984, val_acc: 0.96875
epoch: 258: train_loss: 0.699995539209864, train_acc: 0.5208333333333334, val_loss: 0.577338308095932, val_acc: 0.96875
epoch: 259: train_loss: 0.6998581323868187, train_acc: 0.5074404776096344, val_loss: 0.5856059193611145, val_acc: 0.96875
epoch: 260: train_loss: 0.7001058288765439, train_acc: 0.4375, val_loss: 0.6184706091880798, val_acc: 0.9375
epoch: 261: train_loss: 0.700105244876774, train_acc: 0.4985119005044301, val_loss: 0.6174855530261993, val_acc: 0.921875
epoch: 262: train_loss: 0.7000364927888971, train_acc: 0.4970238109429677, val_loss: 0.5588540732860565, val_acc: 1.0
epoch: 263: train_loss: 0.6998035231925018, train_acc: 0.5476190447807312, val_loss: 0.5673213303089142, val_acc: 0.984375
epoch: 264: train_loss: 0.6999004128594066, train_acc: 0.4583333333333333, val_loss: 0.5521585047245026, val_acc: 1.0
epoch: 265: train_loss: 0.6997586690393602, train_acc: 0.5401785671710968, val_loss: 0.5659818351268768, val_acc: 0.984375
epoch: 266: train_loss: 0.6998069727614277, train_acc: 0.4925595323244731, val_loss: 0.5609049201011658, val_acc: 1.0
epoch: 267: train_loss: 0.6995497731427052, train_acc: 0.5997023781140646, val_loss: 0.5525692999362946, val_acc: 1.0
epoch: 268: train_loss: 0.6994285245219419, train_acc: 0.5148809552192688, val_loss: 0.5575065910816193, val_acc: 1.0
epoch: 269: train_loss: 0.6995072185993191, train_acc: 0.4717261989911397, val_loss: 0.5595320463180542, val_acc: 1.0
epoch: 270: train_loss: 0.6996699032015168, train_acc: 0.4479166666666667, val_loss: 0.5562049746513367, val_acc: 1.0
epoch: 271: train_loss: 0.6995168030261989, train_acc: 0.523809532324473, val_loss: 0.5622292757034302, val_acc: 1.0
epoch: 272: train_loss: 0.6993157040505178, train_acc: 0.5327380895614624, val_loss: 0.5694467425346375, val_acc: 0.984375
epoch: 273: train_loss: 0.698889326005086, train_acc: 0.6235119104385376, val_loss: 0.5696859359741211, val_acc: 0.984375
epoch: 274: train_loss: 0.6985529215408091, train_acc: 0.5952380895614624, val_loss: 0.5705997347831726, val_acc: 1.0
epoch: 275: train_loss: 0.698524915149822, train_acc: 0.5, val_loss: 0.5835473835468292, val_acc: 0.96875
epoch: 276: train_loss: 0.6982942307468784, train_acc: 0.5505952338377634, val_loss: 0.5621528923511505, val_acc: 1.0
epoch: 277: train_loss: 0.6983582234496976, train_acc: 0.4642857114473979, val_loss: 0.5804141163825989, val_acc: 1.0
epoch: 278: train_loss: 0.6984177345016094, train_acc: 0.4657738109429677, val_loss: 0.5921250879764557, val_acc: 0.953125
epoch: 279: train_loss: 0.6985559411701698, train_acc: 0.4538690447807312, val_loss: 0.5815027058124542, val_acc: 0.96875
epoch: 280: train_loss: 0.6980974253527773, train_acc: 0.617559532324473, val_loss: 0.6058029234409332, val_acc: 0.953125
epoch: 281: train_loss: 0.6979893456536824, train_acc: 0.555059532324473, val_loss: 0.6317556500434875, val_acc: 0.921875
epoch: 282: train_loss: 0.698062864995536, train_acc: 0.4791666666666667, val_loss: 0.5718381702899933, val_acc: 0.984375
epoch: 283: train_loss: 0.6977586385929526, train_acc: 0.5773809552192688, val_loss: 0.5534222424030304, val_acc: 1.0
epoch: 284: train_loss: 0.6975904817469634, train_acc: 0.5476190447807312, val_loss: 0.5713470876216888, val_acc: 0.984375
epoch: 285: train_loss: 0.6973214500294974, train_acc: 0.5982142885526022, val_loss: 0.5653323531150818, val_acc: 1.0
epoch: 286: train_loss: 0.6972914278576458, train_acc: 0.511904756228129, val_loss: 0.5572727620601654, val_acc: 1.0
epoch: 287: train_loss: 0.6972855817250629, train_acc: 0.5014880994955698, val_loss: 0.6274649500846863, val_acc: 0.90625
epoch: 288: train_loss: 0.6971465324448052, train_acc: 0.5565476218859354, val_loss: 0.6004857420921326, val_acc: 0.9375
epoch: 289: train_loss: 0.6970991737541109, train_acc: 0.517857144276301, val_loss: 0.578696221113205, val_acc: 1.0
epoch: 290: train_loss: 0.6968723332076273, train_acc: 0.5669642885526022, val_loss: 0.5670544505119324, val_acc: 0.984375
epoch: 291: train_loss: 0.6966117500714514, train_acc: 0.5669642885526022, val_loss: 0.5877571702003479, val_acc: 0.9375
epoch: 292: train_loss: 0.6968307021806127, train_acc: 0.4330357114473979, val_loss: 0.5694411396980286, val_acc: 0.96875
epoch: 293: train_loss: 0.6965747366146163, train_acc: 0.5684523781140646, val_loss: 0.5895822048187256, val_acc: 0.953125
epoch: 294: train_loss: 0.696531710449585, train_acc: 0.5401785671710968, val_loss: 0.5545251071453094, val_acc: 1.0
epoch: 295: train_loss: 0.6962183017518605, train_acc: 0.632440467675527, val_loss: 0.5871967077255249, val_acc: 0.953125
epoch: 296: train_loss: 0.6961611154438, train_acc: 0.555059532324473, val_loss: 0.6351943910121918, val_acc: 0.90625
epoch: 297: train_loss: 0.696200419225682, train_acc: 0.4761904776096344, val_loss: 0.5759561657905579, val_acc: 0.984375
epoch: 298: train_loss: 0.6961564142900697, train_acc: 0.5029761989911398, val_loss: 0.5725034177303314, val_acc: 0.96875
epoch: 299: train_loss: 0.6961163466175396, train_acc: 0.5208333333333334, val_loss: 0.573111355304718, val_acc: 0.984375
epoch: 300: train_loss: 0.6962238462066332, train_acc: 0.4345238109429677, val_loss: 0.5525630116462708, val_acc: 1.0
epoch: 301: train_loss: 0.6962673928235014, train_acc: 0.511904756228129, val_loss: 0.5584010183811188, val_acc: 1.0
epoch: 302: train_loss: 0.6960489631551589, train_acc: 0.5565476218859354, val_loss: 0.5532149970531464, val_acc: 1.0
epoch: 303: train_loss: 0.6959940318933181, train_acc: 0.523809532324473, val_loss: 0.5529427826404572, val_acc: 1.0
epoch: 304: train_loss: 0.6959268884254934, train_acc: 0.5148809552192688, val_loss: 0.5707893967628479, val_acc: 0.984375
epoch: 305: train_loss: 0.695865363759153, train_acc: 0.5342261989911398, val_loss: 0.5539349019527435, val_acc: 1.0
epoch: 306: train_loss: 0.6959292915499041, train_acc: 0.4791666666666667, val_loss: 0.5547643899917603, val_acc: 1.0
epoch: 307: train_loss: 0.6960124640818282, train_acc: 0.4598214228947957, val_loss: 0.5728791058063507, val_acc: 1.0
epoch: 308: train_loss: 0.6959755656338561, train_acc: 0.5044642885526022, val_loss: 0.565374344587326, val_acc: 0.984375
epoch: 309: train_loss: 0.6956833127685772, train_acc: 0.6190476218859354, val_loss: 0.5619825422763824, val_acc: 1.0
epoch: 310: train_loss: 0.6955916425103414, train_acc: 0.5133928656578064, val_loss: 0.551797091960907, val_acc: 1.0
epoch: 311: train_loss: 0.6953594380527989, train_acc: 0.5654761989911398, val_loss: 0.562648743391037, val_acc: 0.984375
epoch: 312: train_loss: 0.6952147997669772, train_acc: 0.543154756228129, val_loss: 0.5550973415374756, val_acc: 1.0
epoch: 313: train_loss: 0.6950169348349743, train_acc: 0.5952380895614624, val_loss: 0.5817573368549347, val_acc: 0.96875
epoch: 314: train_loss: 0.6951380209001914, train_acc: 0.4747023781140645, val_loss: 0.6072300970554352, val_acc: 0.921875
epoch: 315: train_loss: 0.6950265483828536, train_acc: 0.5148809552192688, val_loss: 0.5578566789627075, val_acc: 1.0
epoch: 316: train_loss: 0.6950547289961143, train_acc: 0.4985119005044301, val_loss: 0.578086793422699, val_acc: 0.96875
epoch: 317: train_loss: 0.6949783274025286, train_acc: 0.5193452338377634, val_loss: 0.5686542093753815, val_acc: 0.984375
epoch: 318: train_loss: 0.6949024473549553, train_acc: 0.5520833333333334, val_loss: 0.5849221646785736, val_acc: 0.96875
epoch: 319: train_loss: 0.6950180324725805, train_acc: 0.4866071442763011, val_loss: 0.5610732734203339, val_acc: 1.0
epoch: 320: train_loss: 0.6950920319817147, train_acc: 0.4553571442763011, val_loss: 0.5633963346481323, val_acc: 0.984375
epoch: 321: train_loss: 0.6953871790358245, train_acc: 0.4211309552192688, val_loss: 0.5652905404567719, val_acc: 0.984375
epoch: 322: train_loss: 0.6953674980791975, train_acc: 0.4806547562281291, val_loss: 0.5764811336994171, val_acc: 0.96875
epoch: 323: train_loss: 0.6953306832492596, train_acc: 0.511904756228129, val_loss: 0.6043807566165924, val_acc: 0.953125
epoch: 324: train_loss: 0.6953129795575753, train_acc: 0.5104166666666666, val_loss: 0.5861860513687134, val_acc: 0.9375
epoch: 325: train_loss: 0.6954743490689614, train_acc: 0.4851190447807312, val_loss: 0.6258116364479065, val_acc: 0.921875
epoch: 326: train_loss: 0.6953461053235328, train_acc: 0.5520833333333334, val_loss: 0.5585404634475708, val_acc: 1.0
epoch: 327: train_loss: 0.6951474849952429, train_acc: 0.5788690447807312, val_loss: 0.573427677154541, val_acc: 0.984375
epoch: 328: train_loss: 0.6952461338514252, train_acc: 0.4598214228947957, val_loss: 0.769103080034256, val_acc: 0.765625
epoch: 329: train_loss: 0.6951112264033519, train_acc: 0.5654761989911398, val_loss: 0.5701775550842285, val_acc: 0.984375
epoch: 330: train_loss: 0.6949936694428999, train_acc: 0.5461309552192688, val_loss: 0.5576281547546387, val_acc: 1.0
epoch: 331: train_loss: 0.695064664395698, train_acc: 0.4702380994955699, val_loss: 0.5790166854858398, val_acc: 0.984375
epoch: 332: train_loss: 0.6952758806424814, train_acc: 0.4389880895614624, val_loss: 0.5693458616733551, val_acc: 0.984375
epoch: 333: train_loss: 0.6952502998465787, train_acc: 0.5059523781140646, val_loss: 0.5657595694065094, val_acc: 0.984375
epoch: 334: train_loss: 0.6951735194346205, train_acc: 0.5327380895614624, val_loss: 0.5808739364147186, val_acc: 0.96875
epoch: 335: train_loss: 0.6951925400644541, train_acc: 0.5059523781140646, val_loss: 0.5973688662052155, val_acc: 0.9375
epoch: 336: train_loss: 0.6951417153890951, train_acc: 0.5223214228947958, val_loss: 0.552096039056778, val_acc: 1.0
epoch: 337: train_loss: 0.6949915295463106, train_acc: 0.5401785771052042, val_loss: 0.5518538653850555, val_acc: 1.0
epoch: 338: train_loss: 0.6950536910352922, train_acc: 0.4836309552192688, val_loss: 0.5881448984146118, val_acc: 0.96875
epoch: 339: train_loss: 0.6949937849944713, train_acc: 0.5401785671710968, val_loss: 0.5661893486976624, val_acc: 1.0
epoch: 340: train_loss: 0.6949503852649866, train_acc: 0.5297619005044302, val_loss: 0.5783754885196686, val_acc: 0.984375
epoch: 341: train_loss: 0.6950447647950337, train_acc: 0.4464285721381505, val_loss: 0.6172042489051819, val_acc: 0.9375
epoch: 342: train_loss: 0.6947875333654868, train_acc: 0.6071428656578064, val_loss: 0.5718813836574554, val_acc: 0.984375
epoch: 343: train_loss: 0.694797826858685, train_acc: 0.5446428656578064, val_loss: 0.6177593171596527, val_acc: 0.9375
epoch: 344: train_loss: 0.6947723962546546, train_acc: 0.4910714228947957, val_loss: 0.5719377994537354, val_acc: 1.0
epoch: 345: train_loss: 0.6946135768828364, train_acc: 0.5342261989911398, val_loss: 0.569771945476532, val_acc: 0.984375
epoch: 346: train_loss: 0.6946495898232565, train_acc: 0.4851190447807312, val_loss: 0.560335248708725, val_acc: 1.0
epoch: 347: train_loss: 0.694728884232227, train_acc: 0.4702380895614624, val_loss: 0.5676030814647675, val_acc: 0.984375
epoch: 348: train_loss: 0.6947016191972134, train_acc: 0.5267857114473978, val_loss: 0.563261866569519, val_acc: 0.984375
epoch: 349: train_loss: 0.6946496219578244, train_acc: 0.5208333333333334, val_loss: 0.5532477498054504, val_acc: 1.0
epoch: 350: train_loss: 0.6945707039785521, train_acc: 0.5357142885526022, val_loss: 0.5566002428531647, val_acc: 1.0
epoch: 351: train_loss: 0.6947345889709665, train_acc: 0.4255952338377635, val_loss: 0.5898403227329254, val_acc: 0.96875
epoch: 352: train_loss: 0.694735214240828, train_acc: 0.5133928656578064, val_loss: 0.5854855179786682, val_acc: 0.984375
epoch: 353: train_loss: 0.6945184764720627, train_acc: 0.5639880895614624, val_loss: 0.6268328428268433, val_acc: 0.890625
epoch: 354: train_loss: 0.6944505799264415, train_acc: 0.5297619005044302, val_loss: 0.5679267942905426, val_acc: 0.984375
epoch: 355: train_loss: 0.6944622663913596, train_acc: 0.4985119005044301, val_loss: 0.5572709441184998, val_acc: 1.0
epoch: 356: train_loss: 0.6944482933962935, train_acc: 0.5357142885526022, val_loss: 0.5581129193305969, val_acc: 1.0
epoch: 357: train_loss: 0.6946353957584229, train_acc: 0.4449404776096344, val_loss: 0.6164656579494476, val_acc: 0.9375
epoch: 358: train_loss: 0.6945221724339736, train_acc: 0.5372023781140646, val_loss: 0.5686497688293457, val_acc: 0.984375
epoch: 359: train_loss: 0.6942014564242628, train_acc: 0.6264880895614624, val_loss: 0.6051815152168274, val_acc: 0.953125
epoch: 360: train_loss: 0.694631610027861, train_acc: 0.4494047661622365, val_loss: 0.67823925614357, val_acc: 0.875
epoch: 361: train_loss: 0.6945130243540688, train_acc: 0.5535714228947958, val_loss: 0.5981734395027161, val_acc: 0.90625
epoch: 362: train_loss: 0.6944680333520646, train_acc: 0.5416666666666666, val_loss: 0.5549224615097046, val_acc: 1.0
epoch: 363: train_loss: 0.6941850282279127, train_acc: 0.6041666666666666, val_loss: 0.5750518441200256, val_acc: 0.96875
epoch: 364: train_loss: 0.6940328620068015, train_acc: 0.5803571343421936, val_loss: 0.5728148818016052, val_acc: 0.984375
epoch: 365: train_loss: 0.6938056990585693, train_acc: 0.6071428656578064, val_loss: 0.6300132870674133, val_acc: 0.921875
epoch: 366: train_loss: 0.6937652287918477, train_acc: 0.5208333333333334, val_loss: 0.5674864053726196, val_acc: 0.984375
epoch: 367: train_loss: 0.6938287476212651, train_acc: 0.4657738109429677, val_loss: 0.6575579941272736, val_acc: 0.859375
epoch: 368: train_loss: 0.6938046344304366, train_acc: 0.5476190447807312, val_loss: 0.5889220535755157, val_acc: 0.96875
epoch: 369: train_loss: 0.6938806873989535, train_acc: 0.4866071442763011, val_loss: 0.5546375513076782, val_acc: 1.0
epoch: 370: train_loss: 0.6938139517512926, train_acc: 0.5654761989911398, val_loss: 0.6289777457714081, val_acc: 0.90625
epoch: 371: train_loss: 0.6938660947278836, train_acc: 0.5014880895614624, val_loss: 0.6399454772472382, val_acc: 0.90625
epoch: 372: train_loss: 0.6939225079919527, train_acc: 0.4895833333333333, val_loss: 0.5530197024345398, val_acc: 1.0
epoch: 373: train_loss: 0.6939888002347605, train_acc: 0.4985119005044301, val_loss: 0.6426972150802612, val_acc: 0.90625
epoch: 374: train_loss: 0.6938549748261769, train_acc: 0.5788690447807312, val_loss: 0.5991718769073486, val_acc: 0.953125
epoch: 375: train_loss: 0.6935952289941462, train_acc: 0.6026785771052042, val_loss: 0.5739237666130066, val_acc: 0.984375
epoch: 376: train_loss: 0.6936004010056309, train_acc: 0.4851190447807312, val_loss: 0.6363357603549957, val_acc: 0.90625
epoch: 377: train_loss: 0.6935992668230065, train_acc: 0.4761904776096344, val_loss: 0.6116722822189331, val_acc: 0.9375
epoch: 378: train_loss: 0.6935670340176516, train_acc: 0.5104166666666666, val_loss: 0.6441471576690674, val_acc: 0.90625
epoch: 379: train_loss: 0.6934918270822155, train_acc: 0.5223214228947958, val_loss: 0.6452233195304871, val_acc: 0.90625
epoch: 380: train_loss: 0.6933016545764207, train_acc: 0.59375, val_loss: 0.560643881559372, val_acc: 0.984375
epoch: 381: train_loss: 0.6930683047931645, train_acc: 0.6190476218859354, val_loss: 0.6132398247718811, val_acc: 0.9375
epoch: 382: train_loss: 0.6930252143722496, train_acc: 0.5327380895614624, val_loss: 0.5874922573566437, val_acc: 0.96875
epoch: 383: train_loss: 0.6929517094718497, train_acc: 0.555059532324473, val_loss: 0.6015412211418152, val_acc: 0.921875
epoch: 384: train_loss: 0.6929911007354784, train_acc: 0.4985119005044301, val_loss: 0.6478399932384491, val_acc: 0.890625
epoch: 385: train_loss: 0.6930707926011043, train_acc: 0.4806547562281291, val_loss: 0.5521513223648071, val_acc: 1.0
epoch: 386: train_loss: 0.6928470612445027, train_acc: 0.5729166666666666, val_loss: 0.6808837950229645, val_acc: 0.84375
epoch: 387: train_loss: 0.6927244264081989, train_acc: 0.5520833333333334, val_loss: 0.5653944909572601, val_acc: 0.984375
epoch: 388: train_loss: 0.6926423178182876, train_acc: 0.5104166666666666, val_loss: 0.6611344516277313, val_acc: 0.859375
epoch: 389: train_loss: 0.692597358425458, train_acc: 0.5535714228947958, val_loss: 0.5804606676101685, val_acc: 0.96875
epoch: 390: train_loss: 0.6925369665496689, train_acc: 0.5223214228947958, val_loss: 0.5588470101356506, val_acc: 1.0
epoch: 391: train_loss: 0.6925217940845859, train_acc: 0.5357142885526022, val_loss: 0.5824289619922638, val_acc: 0.96875
epoch: 392: train_loss: 0.6924920815901797, train_acc: 0.5535714228947958, val_loss: 0.5727806091308594, val_acc: 1.0
epoch: 393: train_loss: 0.692490154993917, train_acc: 0.5223214228947958, val_loss: 0.6356327831745148, val_acc: 0.921875
epoch: 394: train_loss: 0.6923673009822133, train_acc: 0.580357144276301, val_loss: 0.7539405524730682, val_acc: 0.796875
epoch: 395: train_loss: 0.6925457453336374, train_acc: 0.5029761989911398, val_loss: 0.6774638891220093, val_acc: 0.890625
epoch: 396: train_loss: 0.6926149872798658, train_acc: 0.523809532324473, val_loss: 0.5535794496536255, val_acc: 1.0
epoch: 397: train_loss: 0.6925659774985141, train_acc: 0.5044642885526022, val_loss: 0.6111612617969513, val_acc: 0.953125
epoch: 398: train_loss: 0.6924416820457203, train_acc: 0.5758928656578064, val_loss: 0.6585676968097687, val_acc: 0.875
epoch: 399: train_loss: 0.6924870313952363, train_acc: 0.4880952338377635, val_loss: 0.5514867007732391, val_acc: 1.0
epoch: 400: train_loss: 0.6922792876797321, train_acc: 0.5997023781140646, val_loss: 0.6271909177303314, val_acc: 0.921875
epoch: 401: train_loss: 0.6925974540894302, train_acc: 0.3794642885526021, val_loss: 0.7040508389472961, val_acc: 0.84375
epoch: 402: train_loss: 0.692554924658648, train_acc: 0.5535714228947958, val_loss: 0.5856769979000092, val_acc: 0.96875
epoch: 403: train_loss: 0.6925749970298786, train_acc: 0.4761904776096344, val_loss: 0.553873211145401, val_acc: 1.0
epoch: 404: train_loss: 0.6925477235160243, train_acc: 0.5133928656578064, val_loss: 0.5702622830867767, val_acc: 0.984375
epoch: 405: train_loss: 0.6923835245503969, train_acc: 0.5505952338377634, val_loss: 0.5816096067428589, val_acc: 0.953125
epoch: 406: train_loss: 0.6923702972087574, train_acc: 0.4895833333333333, val_loss: 0.5568075478076935, val_acc: 1.0
epoch: 407: train_loss: 0.6920385217306266, train_acc: 0.644345243771871, val_loss: 0.5769894123077393, val_acc: 1.0
epoch: 408: train_loss: 0.692057429651176, train_acc: 0.4851190447807312, val_loss: 0.5918186604976654, val_acc: 0.953125
epoch: 409: train_loss: 0.69218793751263, train_acc: 0.4449404776096344, val_loss: 0.5788136124610901, val_acc: 0.984375
epoch: 410: train_loss: 0.69216599688236, train_acc: 0.5580357114473978, val_loss: 0.563060075044632, val_acc: 0.984375
epoch: 411: train_loss: 0.6922092216637908, train_acc: 0.46875, val_loss: 0.6231334507465363, val_acc: 0.921875
epoch: 412: train_loss: 0.6922333181577882, train_acc: 0.4642857114473979, val_loss: 0.5702145099639893, val_acc: 0.984375
epoch: 413: train_loss: 0.6922566270434717, train_acc: 0.5029761989911398, val_loss: 0.5707076787948608, val_acc: 0.984375
epoch: 414: train_loss: 0.692317733013007, train_acc: 0.46875, val_loss: 0.613044947385788, val_acc: 0.921875
epoch: 415: train_loss: 0.6921907487826846, train_acc: 0.549107144276301, val_loss: 0.5536665320396423, val_acc: 1.0
epoch: 416: train_loss: 0.6921325345738799, train_acc: 0.5133928656578064, val_loss: 0.551522433757782, val_acc: 1.0
epoch: 417: train_loss: 0.6922090509624186, train_acc: 0.4568452338377635, val_loss: 0.5550642907619476, val_acc: 1.0
epoch: 418: train_loss: 0.6921102351723152, train_acc: 0.5461309552192688, val_loss: 0.5526947677135468, val_acc: 1.0
epoch: 419: train_loss: 0.692192449574432, train_acc: 0.4866071442763011, val_loss: 0.5604059100151062, val_acc: 1.0
epoch: 420: train_loss: 0.6922652483316795, train_acc: 0.4866071442763011, val_loss: 0.5584304630756378, val_acc: 1.0
epoch: 421: train_loss: 0.692168246301429, train_acc: 0.543154756228129, val_loss: 0.5522560775279999, val_acc: 1.0
epoch: 422: train_loss: 0.6921320950496463, train_acc: 0.5014880895614624, val_loss: 0.5669940114021301, val_acc: 0.984375
epoch: 423: train_loss: 0.692348132601136, train_acc: 0.4017857114473979, val_loss: 0.5818733870983124, val_acc: 0.96875
epoch: 424: train_loss: 0.692300603366365, train_acc: 0.569940467675527, val_loss: 0.5769489705562592, val_acc: 0.984375
epoch: 425: train_loss: 0.6922523857608828, train_acc: 0.5282738109429678, val_loss: 0.5761455297470093, val_acc: 0.984375
epoch: 426: train_loss: 0.6924170294978667, train_acc: 0.4122023781140645, val_loss: 0.6354058384895325, val_acc: 0.90625
epoch: 427: train_loss: 0.6925717862177855, train_acc: 0.4092261890570323, val_loss: 0.6135529279708862, val_acc: 0.921875
epoch: 428: train_loss: 0.6926006669440556, train_acc: 0.4880952338377635, val_loss: 0.5741728842258453, val_acc: 0.984375
epoch: 429: train_loss: 0.6925623796937995, train_acc: 0.5401785671710968, val_loss: 0.5874211490154266, val_acc: 0.96875
epoch: 430: train_loss: 0.6926674853206504, train_acc: 0.4419642885526021, val_loss: 0.5672846734523773, val_acc: 0.984375
epoch: 431: train_loss: 0.6925748364746932, train_acc: 0.574404756228129, val_loss: 0.5721231997013092, val_acc: 0.984375
epoch: 432: train_loss: 0.6925749601401939, train_acc: 0.5401785771052042, val_loss: 0.6581259071826935, val_acc: 0.859375
epoch: 433: train_loss: 0.6925539956435258, train_acc: 0.5014880895614624, val_loss: 0.5879046618938446, val_acc: 0.96875
epoch: 434: train_loss: 0.6925923553234766, train_acc: 0.4925595323244731, val_loss: 0.5678586065769196, val_acc: 0.984375
epoch: 435: train_loss: 0.6926024743677638, train_acc: 0.4866071442763011, val_loss: 0.5955525636672974, val_acc: 0.9375
epoch: 436: train_loss: 0.6925301069662275, train_acc: 0.5297619005044302, val_loss: 0.5576598346233368, val_acc: 1.0
epoch: 437: train_loss: 0.6925562253417483, train_acc: 0.4806547562281291, val_loss: 0.5704998075962067, val_acc: 0.984375
epoch: 438: train_loss: 0.692444633627444, train_acc: 0.5758928656578064, val_loss: 0.6215236186981201, val_acc: 0.921875
epoch: 439: train_loss: 0.6925970141409014, train_acc: 0.4345238109429677, val_loss: 0.6007866263389587, val_acc: 0.984375
epoch: 440: train_loss: 0.6924618214751761, train_acc: 0.5758928656578064, val_loss: 0.5975467264652252, val_acc: 0.953125
epoch: 441: train_loss: 0.6923695621312467, train_acc: 0.555059532324473, val_loss: 0.7071180045604706, val_acc: 0.84375
epoch: 442: train_loss: 0.6922340178911262, train_acc: 0.586309532324473, val_loss: 0.5583219230175018, val_acc: 0.984375
epoch: 443: train_loss: 0.6921647587740737, train_acc: 0.53125, val_loss: 0.5597701966762543, val_acc: 0.984375
epoch: 444: train_loss: 0.6921352325530528, train_acc: 0.5372023781140646, val_loss: 0.6556489765644073, val_acc: 0.890625
epoch: 445: train_loss: 0.6921866860223986, train_acc: 0.511904756228129, val_loss: 0.5973803997039795, val_acc: 0.96875
epoch: 446: train_loss: 0.6920704109778957, train_acc: 0.5461309552192688, val_loss: 0.6208553314208984, val_acc: 0.921875
epoch: 447: train_loss: 0.692189361345732, train_acc: 0.4434523781140645, val_loss: 0.6055701971054077, val_acc: 0.953125
epoch: 448: train_loss: 0.6920642293464184, train_acc: 0.5595238109429678, val_loss: 0.5912255644798279, val_acc: 0.953125
epoch: 449: train_loss: 0.6918577402830117, train_acc: 0.6398809552192688, val_loss: 0.6500464975833893, val_acc: 0.90625
epoch: 450: train_loss: 0.6918361123764147, train_acc: 0.5014880895614624, val_loss: 0.5942281782627106, val_acc: 0.96875
epoch: 451: train_loss: 0.691732867914842, train_acc: 0.538690467675527, val_loss: 0.6030950546264648, val_acc: 0.96875
epoch: 452: train_loss: 0.6917875796183663, train_acc: 0.4657738109429677, val_loss: 0.5837204158306122, val_acc: 0.96875
epoch: 453: train_loss: 0.6917696803283055, train_acc: 0.5074404776096344, val_loss: 0.5819616913795471, val_acc: 0.96875
epoch: 454: train_loss: 0.691703227915606, train_acc: 0.5327380895614624, val_loss: 0.5662042796611786, val_acc: 0.984375
epoch: 455: train_loss: 0.6916929916288066, train_acc: 0.5357142885526022, val_loss: 0.5535472929477692, val_acc: 1.0
epoch: 456: train_loss: 0.6916539051470134, train_acc: 0.5461309552192688, val_loss: 0.5710265934467316, val_acc: 0.984375
epoch: 457: train_loss: 0.6915599261959561, train_acc: 0.555059532324473, val_loss: 0.5904172658920288, val_acc: 0.984375
epoch: 458: train_loss: 0.6915518723498583, train_acc: 0.5014880895614624, val_loss: 0.5635420680046082, val_acc: 0.984375
epoch: 459: train_loss: 0.6914669029738586, train_acc: 0.543154756228129, val_loss: 0.5678902864456177, val_acc: 0.984375
epoch: 460: train_loss: 0.6913450673621192, train_acc: 0.5967261989911398, val_loss: 0.5610607862472534, val_acc: 0.984375
epoch: 461: train_loss: 0.6913657395698628, train_acc: 0.4880952338377635, val_loss: 0.5847031772136688, val_acc: 0.96875
epoch: 462: train_loss: 0.6912017234200786, train_acc: 0.5952380895614624, val_loss: 0.6235716044902802, val_acc: 0.90625
epoch: 463: train_loss: 0.6912591748967245, train_acc: 0.4776785671710968, val_loss: 0.588201642036438, val_acc: 0.96875
epoch: 464: train_loss: 0.6913791986776502, train_acc: 0.4092261890570323, val_loss: 0.5750224590301514, val_acc: 0.984375
epoch: 465: train_loss: 0.6913296013686783, train_acc: 0.5357142885526022, val_loss: 0.5932283401489258, val_acc: 0.9375
epoch: 466: train_loss: 0.6912258061913396, train_acc: 0.5505952338377634, val_loss: 0.588371604681015, val_acc: 0.96875
epoch: 467: train_loss: 0.6911785012891143, train_acc: 0.5327380895614624, val_loss: 0.5536561608314514, val_acc: 1.0
epoch: 468: train_loss: 0.6912513278694796, train_acc: 0.4345238109429677, val_loss: 0.5550470352172852, val_acc: 1.0
epoch: 469: train_loss: 0.6912896432775125, train_acc: 0.4553571442763011, val_loss: 0.5697195827960968, val_acc: 0.984375
epoch: 470: train_loss: 0.6913142649315802, train_acc: 0.4866071442763011, val_loss: 0.6377745270729065, val_acc: 0.90625
epoch: 471: train_loss: 0.6911711140961964, train_acc: 0.5684523781140646, val_loss: 0.5869236886501312, val_acc: 0.984375
epoch: 472: train_loss: 0.6911768964094701, train_acc: 0.5059523781140646, val_loss: 0.6235134899616241, val_acc: 0.921875
epoch: 473: train_loss: 0.6911842518717923, train_acc: 0.5133928656578064, val_loss: 0.5680620074272156, val_acc: 0.96875
epoch: 474: train_loss: 0.6911138123796691, train_acc: 0.5580357114473978, val_loss: 0.6898992657661438, val_acc: 0.859375
epoch: 475: train_loss: 0.6910648692388822, train_acc: 0.5223214228947958, val_loss: 0.6282457709312439, val_acc: 0.921875
epoch: 476: train_loss: 0.6909521580742083, train_acc: 0.574404756228129, val_loss: 0.5665729641914368, val_acc: 0.984375
epoch: 477: train_loss: 0.6906844340316747, train_acc: 0.644345243771871, val_loss: 0.5726201236248016, val_acc: 0.96875
epoch: 478: train_loss: 0.6907275320965953, train_acc: 0.4821428656578064, val_loss: 0.5976187586784363, val_acc: 0.953125
epoch: 479: train_loss: 0.6906613485059797, train_acc: 0.555059532324473, val_loss: 0.5757349133491516, val_acc: 0.96875
epoch: 480: train_loss: 0.6907233295346489, train_acc: 0.4508928656578064, val_loss: 0.5681085884571075, val_acc: 0.984375
epoch: 481: train_loss: 0.6907137036529643, train_acc: 0.5133928656578064, val_loss: 0.5554250180721283, val_acc: 1.0
epoch: 482: train_loss: 0.6907460250839677, train_acc: 0.46875, val_loss: 0.6174637675285339, val_acc: 0.921875
epoch: 483: train_loss: 0.6906926561937181, train_acc: 0.517857144276301, val_loss: 0.5917845070362091, val_acc: 0.953125
epoch: 484: train_loss: 0.6906251780970393, train_acc: 0.5610119005044302, val_loss: 0.5919046401977539, val_acc: 0.96875
epoch: 485: train_loss: 0.6904680664115802, train_acc: 0.5848214228947958, val_loss: 0.5521374046802521, val_acc: 1.0
epoch: 486: train_loss: 0.6904688016712291, train_acc: 0.4866071442763011, val_loss: 0.5515765845775604, val_acc: 1.0
epoch: 487: train_loss: 0.6903778970689389, train_acc: 0.5476190447807312, val_loss: 0.5656167268753052, val_acc: 0.984375
epoch: 488: train_loss: 0.6903121573183915, train_acc: 0.5342261989911398, val_loss: 0.56382817029953, val_acc: 0.984375
epoch: 489: train_loss: 0.6903084548760426, train_acc: 0.511904756228129, val_loss: 0.5609826445579529, val_acc: 0.984375
epoch: 490: train_loss: 0.6904034499845284, train_acc: 0.4211309552192688, val_loss: 0.5881408751010895, val_acc: 1.0
epoch: 491: train_loss: 0.6902120207666051, train_acc: 0.6294642885526022, val_loss: 0.5946328938007355, val_acc: 0.9375
epoch: 492: train_loss: 0.6902097753570877, train_acc: 0.5223214228947958, val_loss: 0.6126824021339417, val_acc: 0.9375
epoch: 493: train_loss: 0.6902472369382893, train_acc: 0.4985119005044301, val_loss: 0.6187669038772583, val_acc: 0.9375
epoch: 494: train_loss: 0.6900647673743334, train_acc: 0.6116071343421936, val_loss: 0.6140532195568085, val_acc: 0.9375
epoch: 495: train_loss: 0.6899974118036921, train_acc: 0.5297619005044302, val_loss: 0.5517325699329376, val_acc: 1.0
epoch: 496: train_loss: 0.6898215829846357, train_acc: 0.5877976218859354, val_loss: 0.5846469402313232, val_acc: 0.9375
epoch: 497: train_loss: 0.6896852617322832, train_acc: 0.5982142885526022, val_loss: 0.6349537372589111, val_acc: 0.921875
epoch: 498: train_loss: 0.6896824795521325, train_acc: 0.4895833333333333, val_loss: 0.5822608470916748, val_acc: 0.96875
epoch: 499: train_loss: 0.6894980963269861, train_acc: 0.6160714228947958, val_loss: 0.5556738078594208, val_acc: 1.0
0.9965222775936127 0.53125
GroundTruth:  Alligator Cracks Alligator Cracks Alligator Cracks Alligator Cracks
Accuracy of the network on the test images: 51 %
Accuracy for class: Alligator Cracks is 0.0 %
Accuracy for class: Longitudinal Cracks is 57.1 %
Accuracy for class: Transverse Cracks is 53.3 %
