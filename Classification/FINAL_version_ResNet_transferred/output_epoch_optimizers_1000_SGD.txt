Follwing classes are there : 
 ['Alligator Cracks', 'Longitudinal Cracks', 'Transverse Cracks']
data length: 132
Length of Train Data : 92
Length of Validation Data : 40
Transverse Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Longitudinal Cracks Alligator Cracks Transverse Cracks Alligator Cracks Transverse Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Alligator Cracks Longitudinal Cracks Transverse Cracks Longitudinal Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Alligator Cracks Alligator Cracks Transverse Cracks Transverse Cracks Transverse Cracks Longitudinal Cracks Transverse Cracks Transverse Cracks Alligator Cracks
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=2048, out_features=512, bias=True)
    (1): ReLU(inplace=True)
    (2): Linear(in_features=512, out_features=3, bias=True)
    (3): Softmax(dim=1)
    (4): Dropout(p=0.5, inplace=False)
  )
)
epoch: 0: train_loss: 1.1658228238423665, train_acc: 0.16220238308111826, val_loss: 1.1066814064979553, val_acc: 0.234375
epoch: 1: train_loss: 1.1667837897936502, train_acc: 0.2514880994955699, val_loss: 1.1014538407325745, val_acc: 0.234375
epoch: 2: train_loss: 1.1703661812676323, train_acc: 0.2455357164144516, val_loss: 1.1014471650123596, val_acc: 0.328125
epoch: 3: train_loss: 1.1589482128620145, train_acc: 0.2708333333333333, val_loss: 1.1037907004356384, val_acc: 0.3125
epoch: 4: train_loss: 1.1499136765797933, train_acc: 0.3571428557236989, val_loss: 1.0986543893814087, val_acc: 0.359375
epoch: 5: train_loss: 1.1473987168735926, train_acc: 0.2931547661622365, val_loss: 1.0962928533554077, val_acc: 0.484375
epoch: 6: train_loss: 1.1459647133236839, train_acc: 0.2723214328289032, val_loss: 1.0970349311828613, val_acc: 0.484375
epoch: 7: train_loss: 1.1466563741366067, train_acc: 0.23214285572369894, val_loss: 1.0974408984184265, val_acc: 0.296875
epoch: 8: train_loss: 1.147551792639273, train_acc: 0.2098214328289032, val_loss: 1.09823876619339, val_acc: 0.34375
epoch: 9: train_loss: 1.1456021428108216, train_acc: 0.34375, val_loss: 1.1012988686561584, val_acc: 0.359375
epoch: 10: train_loss: 1.1443101781787295, train_acc: 0.2604166666666667, val_loss: 1.0952152609825134, val_acc: 0.34375
epoch: 11: train_loss: 1.1396343012650807, train_acc: 0.3869047661622365, val_loss: 1.0961667895317078, val_acc: 0.296875
epoch: 12: train_loss: 1.1361410984626183, train_acc: 0.3154761890570323, val_loss: 1.0971352458000183, val_acc: 0.21875
epoch: 13: train_loss: 1.1392782500811987, train_acc: 0.2797619054714839, val_loss: 1.0991095900535583, val_acc: 0.296875
epoch: 14: train_loss: 1.1379622538884482, train_acc: 0.3377976218859355, val_loss: 1.0948511362075806, val_acc: 0.390625
epoch: 15: train_loss: 1.139566108584404, train_acc: 0.3110119054714839, val_loss: 1.0925190448760986, val_acc: 0.421875
epoch: 16: train_loss: 1.1380678368549721, train_acc: 0.4002976218859355, val_loss: 1.0864665508270264, val_acc: 0.59375
epoch: 17: train_loss: 1.1383232937918768, train_acc: 0.3080357114473979, val_loss: 1.0908690690994263, val_acc: 0.453125
epoch: 18: train_loss: 1.1394551657793814, train_acc: 0.3377976218859355, val_loss: 1.087616741657257, val_acc: 0.546875
epoch: 19: train_loss: 1.1394137104352313, train_acc: 0.3482142885526021, val_loss: 1.0836131572723389, val_acc: 0.484375
epoch: 20: train_loss: 1.1384863077648102, train_acc: 0.4285714328289032, val_loss: 1.0894094109535217, val_acc: 0.421875
epoch: 21: train_loss: 1.1393756162036548, train_acc: 0.3318452338377635, val_loss: 1.082582950592041, val_acc: 0.578125
epoch: 22: train_loss: 1.140246083770973, train_acc: 0.2797619054714839, val_loss: 1.0835648775100708, val_acc: 0.515625
epoch: 23: train_loss: 1.138192267881499, train_acc: 0.4151785671710968, val_loss: 1.077878475189209, val_acc: 0.546875
epoch: 24: train_loss: 1.1377359024683633, train_acc: 0.3616071442763011, val_loss: 1.0807870626449585, val_acc: 0.5625
epoch: 25: train_loss: 1.1386382044889987, train_acc: 0.2693452388048172, val_loss: 1.0855358242988586, val_acc: 0.515625
epoch: 26: train_loss: 1.1395156722009916, train_acc: 0.3050595223903656, val_loss: 1.0864005088806152, val_acc: 0.453125
epoch: 27: train_loss: 1.1403680670829044, train_acc: 0.3377976218859355, val_loss: 1.0884800553321838, val_acc: 0.515625
epoch: 28: train_loss: 1.1390415813731047, train_acc: 0.3616071442763011, val_loss: 1.0774406790733337, val_acc: 0.59375
epoch: 29: train_loss: 1.137544759114583, train_acc: 0.3497023781140645, val_loss: 1.0881510376930237, val_acc: 0.46875
epoch: 30: train_loss: 1.1364603645058087, train_acc: 0.3735119005044301, val_loss: 1.0859864950180054, val_acc: 0.453125
epoch: 31: train_loss: 1.1366877642770608, train_acc: 0.3497023781140645, val_loss: 1.0741580724716187, val_acc: 0.546875
epoch: 32: train_loss: 1.1359596360813489, train_acc: 0.3809523781140645, val_loss: 1.0734612345695496, val_acc: 0.59375
epoch: 33: train_loss: 1.135293047802121, train_acc: 0.3392857114473979, val_loss: 1.0892500281333923, val_acc: 0.40625
epoch: 34: train_loss: 1.1345001992725192, train_acc: 0.3601190447807312, val_loss: 1.0845987200737, val_acc: 0.453125
epoch: 35: train_loss: 1.1345784807646717, train_acc: 0.3616071442763011, val_loss: 1.0831576585769653, val_acc: 0.46875
epoch: 36: train_loss: 1.1349881941133793, train_acc: 0.2946428557236989, val_loss: 1.0756471157073975, val_acc: 0.59375
epoch: 37: train_loss: 1.1332427226660546, train_acc: 0.4836309552192688, val_loss: 1.0725234150886536, val_acc: 0.59375
epoch: 38: train_loss: 1.1332345064888654, train_acc: 0.375, val_loss: 1.074006736278534, val_acc: 0.5625
epoch: 39: train_loss: 1.1330434903502467, train_acc: 0.3184523781140645, val_loss: 1.0743020176887512, val_acc: 0.5
epoch: 40: train_loss: 1.1332565851327852, train_acc: 0.2827380994955699, val_loss: 1.0739668011665344, val_acc: 0.515625
epoch: 41: train_loss: 1.132979137083841, train_acc: 0.3690476218859355, val_loss: 1.076379418373108, val_acc: 0.5
epoch: 42: train_loss: 1.1318821153899497, train_acc: 0.3794642885526021, val_loss: 1.0652879476547241, val_acc: 0.546875
epoch: 43: train_loss: 1.132757439306288, train_acc: 0.3125, val_loss: 1.0617716908454895, val_acc: 0.59375
epoch: 44: train_loss: 1.133155506187015, train_acc: 0.3244047661622365, val_loss: 1.0790789127349854, val_acc: 0.453125
epoch: 45: train_loss: 1.1335374708624852, train_acc: 0.2857142860690753, val_loss: 1.081455409526825, val_acc: 0.453125
epoch: 46: train_loss: 1.132687750014853, train_acc: 0.4315476218859355, val_loss: 1.084500253200531, val_acc: 0.40625
epoch: 47: train_loss: 1.1327635716232989, train_acc: 0.2752976218859355, val_loss: 1.0744686126708984, val_acc: 0.5
epoch: 48: train_loss: 1.1314738773975241, train_acc: 0.3913690447807312, val_loss: 1.0620497465133667, val_acc: 0.59375
epoch: 49: train_loss: 1.1318088646729787, train_acc: 0.3125, val_loss: 1.065489411354065, val_acc: 0.546875
epoch: 50: train_loss: 1.1318643821610344, train_acc: 0.3348214328289032, val_loss: 1.0693718791007996, val_acc: 0.5
epoch: 51: train_loss: 1.1312686109390013, train_acc: 0.4151785671710968, val_loss: 1.0721810460090637, val_acc: 0.453125
epoch: 52: train_loss: 1.1303071049774218, train_acc: 0.3913690447807312, val_loss: 1.069562315940857, val_acc: 0.5
epoch: 53: train_loss: 1.129496799207028, train_acc: 0.4032738109429677, val_loss: 1.0750963687896729, val_acc: 0.453125
epoch: 54: train_loss: 1.1304117849378874, train_acc: 0.2767857114473979, val_loss: 1.0611228346824646, val_acc: 0.546875
epoch: 55: train_loss: 1.1302838655454772, train_acc: 0.3169642885526021, val_loss: 1.0571230053901672, val_acc: 0.546875
epoch: 56: train_loss: 1.1296691340312623, train_acc: 0.3913690447807312, val_loss: 1.067420482635498, val_acc: 0.5
epoch: 57: train_loss: 1.129042716204435, train_acc: 0.4196428656578064, val_loss: 1.0643492341041565, val_acc: 0.5
epoch: 58: train_loss: 1.1296816838663175, train_acc: 0.28125, val_loss: 1.0802980661392212, val_acc: 0.40625
epoch: 59: train_loss: 1.129145996769269, train_acc: 0.3645833333333333, val_loss: 1.0755817294120789, val_acc: 0.40625
epoch: 60: train_loss: 1.1291005080514918, train_acc: 0.3422619005044301, val_loss: 1.060107946395874, val_acc: 0.546875
epoch: 61: train_loss: 1.1278976224442963, train_acc: 0.3898809552192688, val_loss: 1.0786558389663696, val_acc: 0.40625
epoch: 62: train_loss: 1.128809692052306, train_acc: 0.21875, val_loss: 1.0597657561302185, val_acc: 0.546875
epoch: 63: train_loss: 1.1277512103940048, train_acc: 0.4523809552192688, val_loss: 1.0615072846412659, val_acc: 0.5
epoch: 64: train_loss: 1.12802053017494, train_acc: 0.3169642885526021, val_loss: 1.0679646730422974, val_acc: 0.453125
epoch: 65: train_loss: 1.127737574806117, train_acc: 0.3690476218859355, val_loss: 1.0411608219146729, val_acc: 0.59375
epoch: 66: train_loss: 1.1273984016470646, train_acc: 0.4181547562281291, val_loss: 1.0599592328071594, val_acc: 0.5
epoch: 67: train_loss: 1.1266173033737668, train_acc: 0.4300595223903656, val_loss: 1.0341464281082153, val_acc: 0.59375
epoch: 68: train_loss: 1.1263940573314537, train_acc: 0.3645833333333333, val_loss: 1.0562822818756104, val_acc: 0.5
epoch: 69: train_loss: 1.1263955959251948, train_acc: 0.3318452388048172, val_loss: 1.0678870677947998, val_acc: 0.453125
epoch: 70: train_loss: 1.126148226955127, train_acc: 0.3273809552192688, val_loss: 1.0520297288894653, val_acc: 0.5
epoch: 71: train_loss: 1.125691989229785, train_acc: 0.3601190447807312, val_loss: 1.0669456124305725, val_acc: 0.453125
epoch: 72: train_loss: 1.1261867962471424, train_acc: 0.34375, val_loss: 1.0367997288703918, val_acc: 0.640625
epoch: 73: train_loss: 1.1267726816035604, train_acc: 0.3035714328289032, val_loss: 1.0532106161117554, val_acc: 0.5
epoch: 74: train_loss: 1.1260637413130863, train_acc: 0.4151785671710968, val_loss: 1.066641390323639, val_acc: 0.453125
epoch: 75: train_loss: 1.1253963523266606, train_acc: 0.3883928656578064, val_loss: 1.0541145205497742, val_acc: 0.546875
epoch: 76: train_loss: 1.125246542098718, train_acc: 0.3675595223903656, val_loss: 1.0653518438339233, val_acc: 0.453125
epoch: 77: train_loss: 1.1245772443775435, train_acc: 0.4508928656578064, val_loss: 1.0578511953353882, val_acc: 0.453125
epoch: 78: train_loss: 1.1244996225280597, train_acc: 0.3333333333333333, val_loss: 1.0475994944572449, val_acc: 0.546875
epoch: 79: train_loss: 1.1245851986110207, train_acc: 0.3541666666666667, val_loss: 1.0640963315963745, val_acc: 0.453125
epoch: 80: train_loss: 1.1237954404128427, train_acc: 0.4315476218859355, val_loss: 1.051684856414795, val_acc: 0.5
epoch: 81: train_loss: 1.1236473532711584, train_acc: 0.3482142885526021, val_loss: 1.0374575853347778, val_acc: 0.59375
epoch: 82: train_loss: 1.1227384870310861, train_acc: 0.4464285671710968, val_loss: 1.0485730171203613, val_acc: 0.5
epoch: 83: train_loss: 1.1224484332497153, train_acc: 0.3616071442763011, val_loss: 1.0586009621620178, val_acc: 0.453125
epoch: 84: train_loss: 1.1223124641998137, train_acc: 0.3735119005044301, val_loss: 1.0371536612510681, val_acc: 0.59375
epoch: 85: train_loss: 1.1220106729703352, train_acc: 0.3526785721381505, val_loss: 1.052428960800171, val_acc: 0.5
epoch: 86: train_loss: 1.1219707072009526, train_acc: 0.34375, val_loss: 1.030281275510788, val_acc: 0.59375
epoch: 87: train_loss: 1.121920769639087, train_acc: 0.3467261890570323, val_loss: 1.0524201393127441, val_acc: 0.5
epoch: 88: train_loss: 1.1218895575080468, train_acc: 0.3571428557236989, val_loss: 1.0517704486846924, val_acc: 0.5
epoch: 89: train_loss: 1.1219985955291318, train_acc: 0.3571428557236989, val_loss: 1.027527093887329, val_acc: 0.59375
epoch: 90: train_loss: 1.1217352688967521, train_acc: 0.3720238109429677, val_loss: 1.048193633556366, val_acc: 0.5
epoch: 91: train_loss: 1.121068094519601, train_acc: 0.4151785671710968, val_loss: 1.0579739212989807, val_acc: 0.453125
epoch: 92: train_loss: 1.1206533614025316, train_acc: 0.3794642885526021, val_loss: 1.0505290627479553, val_acc: 0.5
epoch: 93: train_loss: 1.120110783171146, train_acc: 0.4255952338377635, val_loss: 1.08351069688797, val_acc: 0.3125
epoch: 94: train_loss: 1.119993502633613, train_acc: 0.3616071442763011, val_loss: 1.0457149147987366, val_acc: 0.5
epoch: 95: train_loss: 1.1200042983724008, train_acc: 0.3511904776096344, val_loss: 1.0449870228767395, val_acc: 0.5
epoch: 96: train_loss: 1.1197676580796123, train_acc: 0.3422619054714839, val_loss: 1.0463168621063232, val_acc: 0.5
epoch: 97: train_loss: 1.1190318858542405, train_acc: 0.4672619005044301, val_loss: 1.0344278812408447, val_acc: 0.5
epoch: 98: train_loss: 1.1192748683068885, train_acc: 0.3229166666666667, val_loss: 1.036550760269165, val_acc: 0.546875
epoch: 99: train_loss: 1.119048901001612, train_acc: 0.3690476218859355, val_loss: 1.0447672009468079, val_acc: 0.5
epoch: 100: train_loss: 1.1190618099552565, train_acc: 0.3943452338377635, val_loss: 1.0365532040596008, val_acc: 0.546875
epoch: 101: train_loss: 1.118828995749841, train_acc: 0.3541666666666667, val_loss: 1.0426813960075378, val_acc: 0.5
epoch: 102: train_loss: 1.1186431459238608, train_acc: 0.3630952338377635, val_loss: 1.0553326606750488, val_acc: 0.453125
epoch: 103: train_loss: 1.118639180102409, train_acc: 0.2916666666666667, val_loss: 1.0390841364860535, val_acc: 0.5
epoch: 104: train_loss: 1.11803446856756, train_acc: 0.4464285671710968, val_loss: 1.0274914503097534, val_acc: 0.546875
epoch: 105: train_loss: 1.1176382706225287, train_acc: 0.4330357114473979, val_loss: 1.0406659245491028, val_acc: 0.5
epoch: 106: train_loss: 1.1171953724552162, train_acc: 0.4419642885526021, val_loss: 1.0491911172866821, val_acc: 0.453125
epoch: 107: train_loss: 1.1162723792187956, train_acc: 0.4613095323244731, val_loss: 1.0565696358680725, val_acc: 0.453125
epoch: 108: train_loss: 1.1162209642042802, train_acc: 0.3497023781140645, val_loss: 1.024308294057846, val_acc: 0.59375
epoch: 109: train_loss: 1.1159256888158389, train_acc: 0.3645833333333333, val_loss: 1.0476465821266174, val_acc: 0.453125
epoch: 110: train_loss: 1.1152234521356066, train_acc: 0.4449404776096344, val_loss: 1.0275185108184814, val_acc: 0.546875
epoch: 111: train_loss: 1.1152556822413484, train_acc: 0.3377976218859355, val_loss: 1.0362088680267334, val_acc: 0.5
epoch: 112: train_loss: 1.114911317122017, train_acc: 0.3467261890570323, val_loss: 1.0254575610160828, val_acc: 0.59375
epoch: 113: train_loss: 1.1147869372228427, train_acc: 0.3348214328289032, val_loss: 1.0461681485176086, val_acc: 0.453125
epoch: 114: train_loss: 1.114733497647271, train_acc: 0.3511904776096344, val_loss: 1.0197903215885162, val_acc: 0.59375
epoch: 115: train_loss: 1.1150317596293038, train_acc: 0.2708333333333333, val_loss: 1.0227804780006409, val_acc: 0.59375
epoch: 116: train_loss: 1.114797420311517, train_acc: 0.3452380994955699, val_loss: 1.032728374004364, val_acc: 0.5
epoch: 117: train_loss: 1.1138116277880583, train_acc: 0.4851190447807312, val_loss: 1.0320607423782349, val_acc: 0.546875
epoch: 118: train_loss: 1.113710700129928, train_acc: 0.3690476218859355, val_loss: 1.022510051727295, val_acc: 0.546875
epoch: 119: train_loss: 1.1138256919052862, train_acc: 0.3244047661622365, val_loss: 1.0359305143356323, val_acc: 0.5
epoch: 120: train_loss: 1.1140760168556334, train_acc: 0.2946428557236989, val_loss: 1.0392818450927734, val_acc: 0.5
epoch: 121: train_loss: 1.1138331956876426, train_acc: 0.3229166666666667, val_loss: 1.0352272391319275, val_acc: 0.5
epoch: 122: train_loss: 1.1133666889777347, train_acc: 0.4017857114473979, val_loss: 1.0337964296340942, val_acc: 0.546875
epoch: 123: train_loss: 1.1132294393034385, train_acc: 0.3452380994955699, val_loss: 1.0263051986694336, val_acc: 0.5
epoch: 124: train_loss: 1.1129188176790867, train_acc: 0.3883928557236989, val_loss: 1.0525125861167908, val_acc: 0.40625
epoch: 125: train_loss: 1.1128406832142475, train_acc: 0.3348214328289032, val_loss: 1.0272775888442993, val_acc: 0.5
epoch: 126: train_loss: 1.1126513564054727, train_acc: 0.3869047661622365, val_loss: 1.0233217477798462, val_acc: 0.546875
epoch: 127: train_loss: 1.11280119124179, train_acc: 0.2752976218859355, val_loss: 1.0542752146720886, val_acc: 0.40625
epoch: 128: train_loss: 1.1123387738099695, train_acc: 0.4270833333333333, val_loss: 1.0346296429634094, val_acc: 0.5
epoch: 129: train_loss: 1.111965114795244, train_acc: 0.4002976218859355, val_loss: 1.0485020875930786, val_acc: 0.453125
epoch: 130: train_loss: 1.1117954128267802, train_acc: 0.4047619005044301, val_loss: 1.0397651195526123, val_acc: 0.453125
epoch: 131: train_loss: 1.1111474422493361, train_acc: 0.4270833333333333, val_loss: 1.0169779658317566, val_acc: 0.546875
epoch: 132: train_loss: 1.1107822217439345, train_acc: 0.3809523781140645, val_loss: 1.020436406135559, val_acc: 0.546875
epoch: 133: train_loss: 1.1104388373408145, train_acc: 0.3348214328289032, val_loss: 1.0160117149353027, val_acc: 0.546875
epoch: 134: train_loss: 1.1098935866061543, train_acc: 0.46875, val_loss: 1.0358027219772339, val_acc: 0.453125
epoch: 135: train_loss: 1.10956440413115, train_acc: 0.4077380895614624, val_loss: 1.044130802154541, val_acc: 0.453125
epoch: 136: train_loss: 1.10895971355647, train_acc: 0.4389880895614624, val_loss: 1.0358588695526123, val_acc: 0.5
epoch: 137: train_loss: 1.1083916827388425, train_acc: 0.4598214228947957, val_loss: 1.0004687309265137, val_acc: 0.59375
epoch: 138: train_loss: 1.1083480132569505, train_acc: 0.3363095223903656, val_loss: 1.0270724892616272, val_acc: 0.5
epoch: 139: train_loss: 1.108324446138881, train_acc: 0.3318452388048172, val_loss: 1.0130141377449036, val_acc: 0.546875
epoch: 140: train_loss: 1.108333374450674, train_acc: 0.2976190447807312, val_loss: 1.0406376123428345, val_acc: 0.453125
epoch: 141: train_loss: 1.107939628228335, train_acc: 0.4122023781140645, val_loss: 1.027156412601471, val_acc: 0.5
epoch: 142: train_loss: 1.1077188810546355, train_acc: 0.3794642885526021, val_loss: 1.0131429135799408, val_acc: 0.546875
epoch: 143: train_loss: 1.107547374097285, train_acc: 0.3854166666666667, val_loss: 1.0126867294311523, val_acc: 0.546875
epoch: 144: train_loss: 1.1076533343600126, train_acc: 0.3125, val_loss: 1.0415826439857483, val_acc: 0.40625
epoch: 145: train_loss: 1.10757745822815, train_acc: 0.3943452338377635, val_loss: 1.0079978704452515, val_acc: 0.546875
epoch: 146: train_loss: 1.1071142224497799, train_acc: 0.4479166666666667, val_loss: 1.0319412350654602, val_acc: 0.5
epoch: 147: train_loss: 1.1071333175038425, train_acc: 0.3125, val_loss: 1.0190772414207458, val_acc: 0.5
epoch: 148: train_loss: 1.1065585489241063, train_acc: 0.4925595323244731, val_loss: 1.0286730527877808, val_acc: 0.453125
epoch: 149: train_loss: 1.1064657665623554, train_acc: 0.3913690447807312, val_loss: 1.0044208765029907, val_acc: 0.59375
epoch: 150: train_loss: 1.106090694731697, train_acc: 0.4494047562281291, val_loss: 1.0328776240348816, val_acc: 0.453125
epoch: 151: train_loss: 1.106139969538178, train_acc: 0.2663690497477849, val_loss: 1.0189562439918518, val_acc: 0.546875
epoch: 152: train_loss: 1.1061067704541487, train_acc: 0.3467261890570323, val_loss: 1.0212669968605042, val_acc: 0.5
epoch: 153: train_loss: 1.1061002468907983, train_acc: 0.3690476218859355, val_loss: 1.0329465866088867, val_acc: 0.453125
epoch: 154: train_loss: 1.1060129746314014, train_acc: 0.3244047661622365, val_loss: 1.0455277562141418, val_acc: 0.421875
epoch: 155: train_loss: 1.1060384762847522, train_acc: 0.2693452388048172, val_loss: 0.9846157431602478, val_acc: 0.640625
epoch: 156: train_loss: 1.1059454164434133, train_acc: 0.3675595223903656, val_loss: 1.0258445143699646, val_acc: 0.453125
epoch: 157: train_loss: 1.1055414864031068, train_acc: 0.4285714228947957, val_loss: 1.0031329691410065, val_acc: 0.609375
epoch: 158: train_loss: 1.1055092822830628, train_acc: 0.3660714228947957, val_loss: 1.0269609093666077, val_acc: 0.453125
epoch: 159: train_loss: 1.1051866294195252, train_acc: 0.4092261890570323, val_loss: 1.0127010941505432, val_acc: 0.546875
epoch: 160: train_loss: 1.104907075193851, train_acc: 0.4047619005044301, val_loss: 1.0236517786979675, val_acc: 0.453125
epoch: 161: train_loss: 1.1046951870614115, train_acc: 0.3973214328289032, val_loss: 1.015311598777771, val_acc: 0.5
epoch: 162: train_loss: 1.1043554713877186, train_acc: 0.4122023781140645, val_loss: 1.023112952709198, val_acc: 0.5
epoch: 163: train_loss: 1.1041366931626464, train_acc: 0.4241071442763011, val_loss: 1.0108727514743805, val_acc: 0.5625
epoch: 164: train_loss: 1.1040654405198913, train_acc: 0.3497023781140645, val_loss: 1.0308181047439575, val_acc: 0.453125
epoch: 165: train_loss: 1.1040687617288534, train_acc: 0.2842261890570323, val_loss: 1.001897394657135, val_acc: 0.546875
epoch: 166: train_loss: 1.1038945229229573, train_acc: 0.3898809552192688, val_loss: 1.0036217272281647, val_acc: 0.546875
epoch: 167: train_loss: 1.1037313761928722, train_acc: 0.3110119054714839, val_loss: 1.0105459690093994, val_acc: 0.515625
epoch: 168: train_loss: 1.1034695822341436, train_acc: 0.4047619005044301, val_loss: 1.00166854262352, val_acc: 0.546875
epoch: 169: train_loss: 1.1031851623572553, train_acc: 0.3913690447807312, val_loss: 1.022588312625885, val_acc: 0.5
epoch: 170: train_loss: 1.1032454512040282, train_acc: 0.3199404776096344, val_loss: 1.0234987139701843, val_acc: 0.5
epoch: 171: train_loss: 1.1029223261415495, train_acc: 0.4449404776096344, val_loss: 1.0154897570610046, val_acc: 0.453125
epoch: 172: train_loss: 1.1033433510388941, train_acc: 0.2738095223903656, val_loss: 1.004499077796936, val_acc: 0.5625
epoch: 173: train_loss: 1.1031403155619157, train_acc: 0.3883928557236989, val_loss: 1.0245022177696228, val_acc: 0.453125
epoch: 174: train_loss: 1.102778010368347, train_acc: 0.4434523781140645, val_loss: 1.0357547402381897, val_acc: 0.40625
epoch: 175: train_loss: 1.1025253823309233, train_acc: 0.4285714228947957, val_loss: 1.0275022387504578, val_acc: 0.46875
epoch: 176: train_loss: 1.102121156038762, train_acc: 0.4002976218859355, val_loss: 1.0225510001182556, val_acc: 0.46875
epoch: 177: train_loss: 1.1020823094282257, train_acc: 0.3675595223903656, val_loss: 1.0294934511184692, val_acc: 0.46875
epoch: 178: train_loss: 1.101940452719534, train_acc: 0.4211309552192688, val_loss: 1.0337786674499512, val_acc: 0.40625
epoch: 179: train_loss: 1.1015965610742569, train_acc: 0.4136904776096344, val_loss: 1.0062442421913147, val_acc: 0.546875
epoch: 180: train_loss: 1.1015731158836113, train_acc: 0.3422619054714839, val_loss: 1.0180417895317078, val_acc: 0.453125
epoch: 181: train_loss: 1.1013055309489532, train_acc: 0.4270833333333333, val_loss: 1.0125057101249695, val_acc: 0.5
epoch: 182: train_loss: 1.1009378946761181, train_acc: 0.3943452338377635, val_loss: 1.0285916328430176, val_acc: 0.46875
epoch: 183: train_loss: 1.1007302821330402, train_acc: 0.4077380994955699, val_loss: 1.0073171257972717, val_acc: 0.515625
epoch: 184: train_loss: 1.1005886611637767, train_acc: 0.3720238109429677, val_loss: 1.0044551491737366, val_acc: 0.5
epoch: 185: train_loss: 1.1004884374398056, train_acc: 0.3065476218859355, val_loss: 1.0231825113296509, val_acc: 0.453125
epoch: 186: train_loss: 1.1002348252371246, train_acc: 0.3869047562281291, val_loss: 0.9991815686225891, val_acc: 0.546875
epoch: 187: train_loss: 1.1000902387478673, train_acc: 0.3422619054714839, val_loss: 0.9973632097244263, val_acc: 0.546875
epoch: 188: train_loss: 1.0997858508974576, train_acc: 0.3943452338377635, val_loss: 0.9918953776359558, val_acc: 0.59375
epoch: 189: train_loss: 1.0994266980572749, train_acc: 0.4151785671710968, val_loss: 1.0353523790836334, val_acc: 0.40625
epoch: 190: train_loss: 1.0990063734287572, train_acc: 0.4211309552192688, val_loss: 1.026054322719574, val_acc: 0.453125
epoch: 191: train_loss: 1.0988900183389583, train_acc: 0.3377976218859355, val_loss: 1.019139051437378, val_acc: 0.46875
epoch: 192: train_loss: 1.0985685018677784, train_acc: 0.4241071442763011, val_loss: 1.0263774394989014, val_acc: 0.40625
epoch: 193: train_loss: 1.09849777725554, train_acc: 0.3586309552192688, val_loss: 0.9948924779891968, val_acc: 0.546875
epoch: 194: train_loss: 1.0984079860214493, train_acc: 0.3586309552192688, val_loss: 1.0026629567146301, val_acc: 0.5
epoch: 195: train_loss: 1.0980855604621016, train_acc: 0.4315476218859355, val_loss: 1.0072949826717377, val_acc: 0.5
epoch: 196: train_loss: 1.0980538358744831, train_acc: 0.3794642885526021, val_loss: 1.0082038044929504, val_acc: 0.515625
epoch: 197: train_loss: 1.0977172091954484, train_acc: 0.4345238109429677, val_loss: 1.001590520143509, val_acc: 0.5
epoch: 198: train_loss: 1.097407690144863, train_acc: 0.4136904776096344, val_loss: 1.0044596195220947, val_acc: 0.5
epoch: 199: train_loss: 1.0972191992402076, train_acc: 0.3511904776096344, val_loss: 1.0110461115837097, val_acc: 0.453125
epoch: 200: train_loss: 1.0969989032887701, train_acc: 0.4255952388048172, val_loss: 0.9994634389877319, val_acc: 0.5
epoch: 201: train_loss: 1.0968858343933283, train_acc: 0.3273809552192688, val_loss: 0.9906370639801025, val_acc: 0.546875
epoch: 202: train_loss: 1.0967790626344227, train_acc: 0.3869047661622365, val_loss: 0.9808264970779419, val_acc: 0.59375
epoch: 203: train_loss: 1.0963639347187055, train_acc: 0.4241071442763011, val_loss: 1.0178845822811127, val_acc: 0.484375
epoch: 204: train_loss: 1.0960750235774654, train_acc: 0.4017857114473979, val_loss: 0.9868876934051514, val_acc: 0.546875
epoch: 205: train_loss: 1.0957418095718312, train_acc: 0.4419642885526021, val_loss: 1.0040412843227386, val_acc: 0.453125
epoch: 206: train_loss: 1.095567393610059, train_acc: 0.3645833333333333, val_loss: 1.006890058517456, val_acc: 0.53125
epoch: 207: train_loss: 1.0953602275023095, train_acc: 0.4107142885526021, val_loss: 0.9920610189437866, val_acc: 0.5625
epoch: 208: train_loss: 1.095279595498263, train_acc: 0.3556547661622365, val_loss: 0.9809432625770569, val_acc: 0.59375
epoch: 209: train_loss: 1.094971301442101, train_acc: 0.4151785671710968, val_loss: 0.9948612153530121, val_acc: 0.578125
epoch: 210: train_loss: 1.0949068625014728, train_acc: 0.3735119005044301, val_loss: 1.0054974555969238, val_acc: 0.515625
epoch: 211: train_loss: 1.0946400067153967, train_acc: 0.3883928557236989, val_loss: 0.9937060475349426, val_acc: 0.515625
epoch: 212: train_loss: 1.0945443889717916, train_acc: 0.3764880994955699, val_loss: 1.0010798573493958, val_acc: 0.515625
epoch: 213: train_loss: 1.094467248964904, train_acc: 0.3660714328289032, val_loss: 0.9660798013210297, val_acc: 0.59375
epoch: 214: train_loss: 1.0945155024528506, train_acc: 0.3199404776096344, val_loss: 1.0050944089889526, val_acc: 0.53125
epoch: 215: train_loss: 1.0943653056467022, train_acc: 0.4136904776096344, val_loss: 1.0134186446666718, val_acc: 0.453125
epoch: 216: train_loss: 1.0942310286190837, train_acc: 0.3541666666666667, val_loss: 0.9958352148532867, val_acc: 0.609375
epoch: 217: train_loss: 1.094185943906096, train_acc: 0.3020833333333333, val_loss: 0.9738046824932098, val_acc: 0.65625
epoch: 218: train_loss: 1.093842298654298, train_acc: 0.4315476218859355, val_loss: 0.9924788773059845, val_acc: 0.578125
epoch: 219: train_loss: 1.0935581184697873, train_acc: 0.4107142885526021, val_loss: 0.9623225927352905, val_acc: 0.6875
epoch: 220: train_loss: 1.093127903265831, train_acc: 0.4464285671710968, val_loss: 0.9933483600616455, val_acc: 0.546875
epoch: 221: train_loss: 1.0928708354274075, train_acc: 0.4226190447807312, val_loss: 0.9750634133815765, val_acc: 0.609375
epoch: 222: train_loss: 1.0928139419299072, train_acc: 0.3601190447807312, val_loss: 1.0009075999259949, val_acc: 0.5
epoch: 223: train_loss: 1.0924102556669997, train_acc: 0.4434523781140645, val_loss: 0.985066145658493, val_acc: 0.5625
epoch: 224: train_loss: 1.0923200857197797, train_acc: 0.3898809552192688, val_loss: 0.9792608022689819, val_acc: 0.625
epoch: 225: train_loss: 1.092398146172892, train_acc: 0.2886904776096344, val_loss: 0.9687385559082031, val_acc: 0.625
epoch: 226: train_loss: 1.0920049964768948, train_acc: 0.4895833333333333, val_loss: 0.9992140829563141, val_acc: 0.453125
epoch: 227: train_loss: 1.091962486941215, train_acc: 0.3705357114473979, val_loss: 0.9786435067653656, val_acc: 0.5625
epoch: 228: train_loss: 1.0916692674420287, train_acc: 0.4136904776096344, val_loss: 0.9915310442447662, val_acc: 0.578125
epoch: 229: train_loss: 1.0913705795571425, train_acc: 0.4479166666666667, val_loss: 0.9946188032627106, val_acc: 0.484375
epoch: 230: train_loss: 1.0911008700147853, train_acc: 0.4479166666666667, val_loss: 0.9928223192691803, val_acc: 0.53125
epoch: 231: train_loss: 1.0910998088703758, train_acc: 0.3616071442763011, val_loss: 1.0139850676059723, val_acc: 0.46875
epoch: 232: train_loss: 1.090766807226664, train_acc: 0.4255952338377635, val_loss: 0.9967268109321594, val_acc: 0.578125
epoch: 233: train_loss: 1.0903920407308814, train_acc: 0.4598214328289032, val_loss: 0.9886910319328308, val_acc: 0.625
epoch: 234: train_loss: 1.0903555356018935, train_acc: 0.3422619054714839, val_loss: 1.0075016021728516, val_acc: 0.546875
epoch: 235: train_loss: 1.090181270898399, train_acc: 0.3988095223903656, val_loss: 0.9799818098545074, val_acc: 0.640625
epoch: 236: train_loss: 1.0899622740457164, train_acc: 0.4017857114473979, val_loss: 0.9807540476322174, val_acc: 0.609375
epoch: 237: train_loss: 1.0896946786999369, train_acc: 0.4285714328289032, val_loss: 1.017029196023941, val_acc: 0.484375
epoch: 238: train_loss: 1.0893523055472967, train_acc: 0.4717261989911397, val_loss: 1.013606309890747, val_acc: 0.4375
epoch: 239: train_loss: 1.0889981635742718, train_acc: 0.4583333333333333, val_loss: 0.9765809178352356, val_acc: 0.59375
epoch: 240: train_loss: 1.0887123901979228, train_acc: 0.3928571442763011, val_loss: 0.9982520937919617, val_acc: 0.59375
epoch: 241: train_loss: 1.0885240878940614, train_acc: 0.4122023781140645, val_loss: 0.9676582217216492, val_acc: 0.671875
epoch: 242: train_loss: 1.0883620396072482, train_acc: 0.4404761989911397, val_loss: 0.954755961894989, val_acc: 0.703125
epoch: 243: train_loss: 1.088184659842585, train_acc: 0.4151785671710968, val_loss: 0.9968363642692566, val_acc: 0.625
epoch: 244: train_loss: 1.087944001891986, train_acc: 0.4419642885526021, val_loss: 0.9927221238613129, val_acc: 0.59375
epoch: 245: train_loss: 1.0876846802105424, train_acc: 0.4315476218859355, val_loss: 0.9980651438236237, val_acc: 0.640625
epoch: 246: train_loss: 1.0876962402571548, train_acc: 0.3065476218859355, val_loss: 0.99940425157547, val_acc: 0.578125
epoch: 247: train_loss: 1.087792632160007, train_acc: 0.2961309552192688, val_loss: 0.9904078543186188, val_acc: 0.59375
epoch: 248: train_loss: 1.0878129467427968, train_acc: 0.34375, val_loss: 0.9464444518089294, val_acc: 0.78125
epoch: 249: train_loss: 1.0874362850983934, train_acc: 0.4955357114473979, val_loss: 0.9760725200176239, val_acc: 0.6875
epoch: 250: train_loss: 1.0871612247559492, train_acc: 0.4464285671710968, val_loss: 0.9736176431179047, val_acc: 0.703125
epoch: 251: train_loss: 1.0870830234554076, train_acc: 0.3377976218859355, val_loss: 0.9991716742515564, val_acc: 0.5625
epoch: 252: train_loss: 1.0868824692897014, train_acc: 0.4002976218859355, val_loss: 0.9912775754928589, val_acc: 0.578125
epoch: 253: train_loss: 1.0866486777470805, train_acc: 0.4241071442763011, val_loss: 0.9634199738502502, val_acc: 0.65625
epoch: 254: train_loss: 1.0865547404569735, train_acc: 0.3898809552192688, val_loss: 0.9766257405281067, val_acc: 0.625
epoch: 255: train_loss: 1.0862052045607316, train_acc: 0.5104166666666666, val_loss: 0.9663320183753967, val_acc: 0.65625
epoch: 256: train_loss: 1.086124342162012, train_acc: 0.3824404776096344, val_loss: 0.9827031493186951, val_acc: 0.59375
epoch: 257: train_loss: 1.0860324741487968, train_acc: 0.4211309552192688, val_loss: 0.9918399155139923, val_acc: 0.59375
epoch: 258: train_loss: 1.0855758121114896, train_acc: 0.53125, val_loss: 0.9994232058525085, val_acc: 0.59375
epoch: 259: train_loss: 1.0853408418404746, train_acc: 0.4375, val_loss: 0.9656793475151062, val_acc: 0.71875
epoch: 260: train_loss: 1.085133952808258, train_acc: 0.4419642885526021, val_loss: 0.9632942080497742, val_acc: 0.671875
epoch: 261: train_loss: 1.0848710945696014, train_acc: 0.4553571442763011, val_loss: 0.9904131889343262, val_acc: 0.5625
epoch: 262: train_loss: 1.0849010083914103, train_acc: 0.3586309552192688, val_loss: 0.9720970690250397, val_acc: 0.640625
epoch: 263: train_loss: 1.0847537280483677, train_acc: 0.3943452338377635, val_loss: 1.0026645958423615, val_acc: 0.578125
epoch: 264: train_loss: 1.0843962964771676, train_acc: 0.5014880895614624, val_loss: 1.0177548229694366, val_acc: 0.546875
epoch: 265: train_loss: 1.0841608133381766, train_acc: 0.5044642885526022, val_loss: 0.9781625270843506, val_acc: 0.65625
epoch: 266: train_loss: 1.0839899926447538, train_acc: 0.4553571442763011, val_loss: 0.961918830871582, val_acc: 0.71875
epoch: 267: train_loss: 1.083636303594456, train_acc: 0.5297619005044302, val_loss: 0.9691198468208313, val_acc: 0.625
epoch: 268: train_loss: 1.0833160631275527, train_acc: 0.4330357114473979, val_loss: 0.9924262166023254, val_acc: 0.515625
epoch: 269: train_loss: 1.082977159891599, train_acc: 0.4970238109429677, val_loss: 0.9720376133918762, val_acc: 0.640625
epoch: 270: train_loss: 1.082821021660667, train_acc: 0.4568452338377635, val_loss: 0.9366679787635803, val_acc: 0.796875
epoch: 271: train_loss: 1.0826645931776828, train_acc: 0.4761904776096344, val_loss: 0.9563087522983551, val_acc: 0.6875
epoch: 272: train_loss: 1.0824801189704398, train_acc: 0.4017857114473979, val_loss: 0.9656974971294403, val_acc: 0.609375
epoch: 273: train_loss: 1.0822277217740848, train_acc: 0.4479166666666667, val_loss: 0.9641185402870178, val_acc: 0.6875
epoch: 274: train_loss: 1.0818681669235228, train_acc: 0.4880952338377635, val_loss: 0.9674559235572815, val_acc: 0.75
epoch: 275: train_loss: 1.0816331040024179, train_acc: 0.4255952338377635, val_loss: 0.9713200926780701, val_acc: 0.703125
epoch: 276: train_loss: 1.0815683985588493, train_acc: 0.4032738109429677, val_loss: 0.9490167796611786, val_acc: 0.765625
epoch: 277: train_loss: 1.081148252998896, train_acc: 0.5, val_loss: 1.0048160254955292, val_acc: 0.578125
epoch: 278: train_loss: 1.0811196067854492, train_acc: 0.3497023781140645, val_loss: 0.9742664396762848, val_acc: 0.65625
epoch: 279: train_loss: 1.0808743022027465, train_acc: 0.4732142885526021, val_loss: 0.9481926262378693, val_acc: 0.765625
epoch: 280: train_loss: 1.0806853641818848, train_acc: 0.4151785671710968, val_loss: 0.9887425899505615, val_acc: 0.671875
epoch: 281: train_loss: 1.0806610594827228, train_acc: 0.3556547661622365, val_loss: 0.9649058282375336, val_acc: 0.78125
epoch: 282: train_loss: 1.0802893960152973, train_acc: 0.5163690447807312, val_loss: 0.9740175604820251, val_acc: 0.640625
epoch: 283: train_loss: 1.0799423209238497, train_acc: 0.5252976218859354, val_loss: 0.989881843328476, val_acc: 0.5625
epoch: 284: train_loss: 1.0796450401607312, train_acc: 0.4508928656578064, val_loss: 0.9985773861408234, val_acc: 0.5625
epoch: 285: train_loss: 1.0795788671864772, train_acc: 0.4464285671710968, val_loss: 0.9520308673381805, val_acc: 0.71875
epoch: 286: train_loss: 1.0795700003184228, train_acc: 0.3690476218859355, val_loss: 0.9818068146705627, val_acc: 0.671875
epoch: 287: train_loss: 1.0793569463960546, train_acc: 0.4553571442763011, val_loss: 0.9989008903503418, val_acc: 0.609375
epoch: 288: train_loss: 1.0791272332649051, train_acc: 0.4672619005044301, val_loss: 0.9798557460308075, val_acc: 0.703125
epoch: 289: train_loss: 1.0790258845378609, train_acc: 0.3869047661622365, val_loss: 0.9861555695533752, val_acc: 0.5625
epoch: 290: train_loss: 1.0787664677815472, train_acc: 0.4895833333333333, val_loss: 0.9498661160469055, val_acc: 0.75
epoch: 291: train_loss: 1.078628006455016, train_acc: 0.46875, val_loss: 0.9538297653198242, val_acc: 0.75
epoch: 292: train_loss: 1.0783877141510934, train_acc: 0.4434523781140645, val_loss: 0.9758537411689758, val_acc: 0.71875
epoch: 293: train_loss: 1.0781009897353153, train_acc: 0.4776785671710968, val_loss: 0.9474723041057587, val_acc: 0.78125
epoch: 294: train_loss: 1.0780033526447528, train_acc: 0.3928571442763011, val_loss: 0.9715595543384552, val_acc: 0.65625
epoch: 295: train_loss: 1.077960842968644, train_acc: 0.4107142885526021, val_loss: 0.9535468518733978, val_acc: 0.734375
epoch: 296: train_loss: 1.0777438200015141, train_acc: 0.46875, val_loss: 0.9967343211174011, val_acc: 0.65625
epoch: 297: train_loss: 1.0774575123850927, train_acc: 0.4732142885526021, val_loss: 0.9601169228553772, val_acc: 0.765625
epoch: 298: train_loss: 1.077464805531794, train_acc: 0.3720238109429677, val_loss: 0.9581601023674011, val_acc: 0.6875
epoch: 299: train_loss: 1.077183753252029, train_acc: 0.517857144276301, val_loss: 0.9526351690292358, val_acc: 0.734375
epoch: 300: train_loss: 1.076938153039313, train_acc: 0.4508928656578064, val_loss: 0.9697742462158203, val_acc: 0.78125
epoch: 301: train_loss: 1.0768841709653796, train_acc: 0.4032738109429677, val_loss: 0.9836310148239136, val_acc: 0.671875
epoch: 302: train_loss: 1.0767931864075402, train_acc: 0.4375, val_loss: 0.9763025939464569, val_acc: 0.625
epoch: 303: train_loss: 1.076677074949992, train_acc: 0.3898809552192688, val_loss: 0.9758928418159485, val_acc: 0.71875
epoch: 304: train_loss: 1.0765081688354574, train_acc: 0.4241071442763011, val_loss: 0.9618491232395172, val_acc: 0.765625
epoch: 305: train_loss: 1.076459517211436, train_acc: 0.3809523781140645, val_loss: 0.96488356590271, val_acc: 0.75
epoch: 306: train_loss: 1.076285748670725, train_acc: 0.4315476218859355, val_loss: 0.9832608699798584, val_acc: 0.625
epoch: 307: train_loss: 1.0761552951036593, train_acc: 0.375, val_loss: 0.9543470442295074, val_acc: 0.71875
epoch: 308: train_loss: 1.075735586489417, train_acc: 0.511904756228129, val_loss: 0.939367026090622, val_acc: 0.765625
epoch: 309: train_loss: 1.075689254268523, train_acc: 0.4047619005044301, val_loss: 0.9675546884536743, val_acc: 0.609375
epoch: 310: train_loss: 1.0753992645175763, train_acc: 0.4598214228947957, val_loss: 0.9794095754623413, val_acc: 0.609375
epoch: 311: train_loss: 1.0751918065879078, train_acc: 0.4419642885526021, val_loss: 0.9431707859039307, val_acc: 0.796875
epoch: 312: train_loss: 1.0748219442443725, train_acc: 0.4880952338377635, val_loss: 0.9655123353004456, val_acc: 0.59375
epoch: 313: train_loss: 1.074597944436306, train_acc: 0.4434523781140645, val_loss: 0.9501307606697083, val_acc: 0.78125
epoch: 314: train_loss: 1.0743242955081678, train_acc: 0.4851190447807312, val_loss: 0.9695875644683838, val_acc: 0.671875
epoch: 315: train_loss: 1.0742744139478178, train_acc: 0.3869047562281291, val_loss: 0.9597422182559967, val_acc: 0.6875
epoch: 316: train_loss: 1.074022846329726, train_acc: 0.4196428557236989, val_loss: 0.9508002698421478, val_acc: 0.8125
epoch: 317: train_loss: 1.0738659944049467, train_acc: 0.4479166666666667, val_loss: 0.9543774127960205, val_acc: 0.796875
epoch: 318: train_loss: 1.0735468559877999, train_acc: 0.5401785671710968, val_loss: 0.9596996009349823, val_acc: 0.65625
epoch: 319: train_loss: 1.0734040768196187, train_acc: 0.4241071442763011, val_loss: 0.9438620209693909, val_acc: 0.8125
epoch: 320: train_loss: 1.0731322660748215, train_acc: 0.4880952338377635, val_loss: 0.9718523025512695, val_acc: 0.75
epoch: 321: train_loss: 1.072991939188284, train_acc: 0.4181547562281291, val_loss: 0.9727508425712585, val_acc: 0.6875
epoch: 322: train_loss: 1.0727553002236427, train_acc: 0.4419642885526021, val_loss: 0.9495823383331299, val_acc: 0.78125
epoch: 323: train_loss: 1.0725598707488535, train_acc: 0.4479166666666667, val_loss: 0.965152770280838, val_acc: 0.78125
epoch: 324: train_loss: 1.072473690693195, train_acc: 0.4211309552192688, val_loss: 0.9478665590286255, val_acc: 0.734375
epoch: 325: train_loss: 1.0723738197412471, train_acc: 0.4136904776096344, val_loss: 0.9711492955684662, val_acc: 0.671875
epoch: 326: train_loss: 1.07216491915521, train_acc: 0.4404761890570323, val_loss: 0.9432173371315002, val_acc: 0.796875
epoch: 327: train_loss: 1.071933547050003, train_acc: 0.4642857114473979, val_loss: 0.954238086938858, val_acc: 0.75
epoch: 328: train_loss: 1.0716371679378498, train_acc: 0.5014880895614624, val_loss: 0.959960013628006, val_acc: 0.828125
epoch: 329: train_loss: 1.0715329580234758, train_acc: 0.4211309552192688, val_loss: 0.9563528299331665, val_acc: 0.65625
epoch: 330: train_loss: 1.0713786053033274, train_acc: 0.4553571442763011, val_loss: 0.9356001913547516, val_acc: 0.796875
epoch: 331: train_loss: 1.0710624839407372, train_acc: 0.5446428656578064, val_loss: 0.9429090321063995, val_acc: 0.8125
epoch: 332: train_loss: 1.0707913030375231, train_acc: 0.4895833333333333, val_loss: 0.9496880769729614, val_acc: 0.71875
epoch: 333: train_loss: 1.0705159129853734, train_acc: 0.4925595323244731, val_loss: 0.9814178943634033, val_acc: 0.6875
epoch: 334: train_loss: 1.0703648181103949, train_acc: 0.4196428557236989, val_loss: 0.9733596742153168, val_acc: 0.6875
epoch: 335: train_loss: 1.0699418977730804, train_acc: 0.5535714228947958, val_loss: 0.9622140228748322, val_acc: 0.734375
epoch: 336: train_loss: 1.069797177696794, train_acc: 0.4270833333333333, val_loss: 0.9808317422866821, val_acc: 0.75
epoch: 337: train_loss: 1.0696355036259637, train_acc: 0.4791666666666667, val_loss: 0.9549758434295654, val_acc: 0.796875
epoch: 338: train_loss: 1.0694889548136655, train_acc: 0.4657738109429677, val_loss: 0.9536176025867462, val_acc: 0.734375
epoch: 339: train_loss: 1.0693443932369642, train_acc: 0.4479166666666667, val_loss: 0.9286868274211884, val_acc: 0.84375
epoch: 340: train_loss: 1.069159016814283, train_acc: 0.4360119005044301, val_loss: 0.9495415389537811, val_acc: 0.796875
epoch: 341: train_loss: 1.0689168688614232, train_acc: 0.5104166666666666, val_loss: 0.9489386677742004, val_acc: 0.78125
epoch: 342: train_loss: 1.068674669610979, train_acc: 0.5208333333333334, val_loss: 0.9439234733581543, val_acc: 0.84375
epoch: 343: train_loss: 1.0685397753881851, train_acc: 0.4375, val_loss: 0.9652403593063354, val_acc: 0.734375
epoch: 344: train_loss: 1.0683736168244033, train_acc: 0.4315476218859355, val_loss: 0.9445582628250122, val_acc: 0.765625
epoch: 345: train_loss: 1.0681732001791568, train_acc: 0.4494047562281291, val_loss: 0.9490748941898346, val_acc: 0.703125
epoch: 346: train_loss: 1.0680091282590778, train_acc: 0.4479166666666667, val_loss: 0.9422574937343597, val_acc: 0.75
epoch: 347: train_loss: 1.0678769994056085, train_acc: 0.4360119005044301, val_loss: 0.9366970360279083, val_acc: 0.75
epoch: 348: train_loss: 1.0675272619940603, train_acc: 0.5163690447807312, val_loss: 0.9351334869861603, val_acc: 0.84375
epoch: 349: train_loss: 1.0673201299281347, train_acc: 0.4880952338377635, val_loss: 0.9301967322826385, val_acc: 0.796875
epoch: 350: train_loss: 1.06713624427348, train_acc: 0.4508928557236989, val_loss: 0.9514797627925873, val_acc: 0.75
epoch: 351: train_loss: 1.0668922602340127, train_acc: 0.4985119005044301, val_loss: 0.9470220804214478, val_acc: 0.8125
epoch: 352: train_loss: 1.066718115347303, train_acc: 0.4568452338377635, val_loss: 0.9515447616577148, val_acc: 0.828125
epoch: 353: train_loss: 1.066513654258516, train_acc: 0.4702380994955699, val_loss: 0.9534872174263, val_acc: 0.71875
epoch: 354: train_loss: 1.0661863530745528, train_acc: 0.5565476218859354, val_loss: 0.949324369430542, val_acc: 0.734375
epoch: 355: train_loss: 1.0659202583385317, train_acc: 0.4985119005044301, val_loss: 0.9435063302516937, val_acc: 0.75
epoch: 356: train_loss: 1.065717056908416, train_acc: 0.46875, val_loss: 0.9427408277988434, val_acc: 0.84375
epoch: 357: train_loss: 1.0654415917241151, train_acc: 0.5074404776096344, val_loss: 0.948778361082077, val_acc: 0.75
epoch: 358: train_loss: 1.0652651292035837, train_acc: 0.4836309552192688, val_loss: 0.9271173477172852, val_acc: 0.734375
epoch: 359: train_loss: 1.0650262010870155, train_acc: 0.5133928656578064, val_loss: 0.9226738810539246, val_acc: 0.828125
epoch: 360: train_loss: 1.0647660116299587, train_acc: 0.5327380895614624, val_loss: 0.9272661805152893, val_acc: 0.75
epoch: 361: train_loss: 1.0645209694106277, train_acc: 0.4880952338377635, val_loss: 0.9184629917144775, val_acc: 0.78125
epoch: 362: train_loss: 1.0642168769275957, train_acc: 0.5208333333333334, val_loss: 0.9232566058635712, val_acc: 0.84375
epoch: 363: train_loss: 1.064055221148463, train_acc: 0.4598214228947957, val_loss: 0.9707607924938202, val_acc: 0.734375
epoch: 364: train_loss: 1.0638099924070106, train_acc: 0.5, val_loss: 0.9542066156864166, val_acc: 0.640625
epoch: 365: train_loss: 1.0636325646836469, train_acc: 0.46875, val_loss: 0.9835286140441895, val_acc: 0.703125
epoch: 366: train_loss: 1.063350696871218, train_acc: 0.5342261989911398, val_loss: 0.9363759160041809, val_acc: 0.75
epoch: 367: train_loss: 1.0630722826049812, train_acc: 0.5267857114473978, val_loss: 0.9261926710605621, val_acc: 0.8125
epoch: 368: train_loss: 1.0628506063122736, train_acc: 0.4836309552192688, val_loss: 0.9252583682537079, val_acc: 0.8125
epoch: 369: train_loss: 1.0626193649596996, train_acc: 0.4523809552192688, val_loss: 0.9035006463527679, val_acc: 0.875
epoch: 370: train_loss: 1.0624901980081338, train_acc: 0.4226190447807312, val_loss: 0.9486424028873444, val_acc: 0.671875
epoch: 371: train_loss: 1.0622859488251388, train_acc: 0.4776785671710968, val_loss: 0.9309448301792145, val_acc: 0.765625
epoch: 372: train_loss: 1.062149652633633, train_acc: 0.4494047562281291, val_loss: 0.9123682975769043, val_acc: 0.84375
epoch: 373: train_loss: 1.0621312415217334, train_acc: 0.3690476218859355, val_loss: 0.9295229315757751, val_acc: 0.734375
epoch: 374: train_loss: 1.0618812470436096, train_acc: 0.5446428656578064, val_loss: 0.9316751658916473, val_acc: 0.8125
epoch: 375: train_loss: 1.06160131154965, train_acc: 0.5461309552192688, val_loss: 0.9640454053878784, val_acc: 0.6875
epoch: 376: train_loss: 1.0613795598980393, train_acc: 0.5252976218859354, val_loss: 0.9403703510761261, val_acc: 0.734375
epoch: 377: train_loss: 1.061093822410708, train_acc: 0.555059532324473, val_loss: 0.9346608221530914, val_acc: 0.78125
epoch: 378: train_loss: 1.0609446632411257, train_acc: 0.4508928557236989, val_loss: 0.9587967693805695, val_acc: 0.671875
epoch: 379: train_loss: 1.0605324631720259, train_acc: 0.625, val_loss: 0.913013368844986, val_acc: 0.765625
epoch: 380: train_loss: 1.0603199408748958, train_acc: 0.5223214228947958, val_loss: 0.9279807507991791, val_acc: 0.8125
epoch: 381: train_loss: 1.0599515620118454, train_acc: 0.5982142885526022, val_loss: 0.9543648958206177, val_acc: 0.625
epoch: 382: train_loss: 1.059709754583626, train_acc: 0.4806547562281291, val_loss: 0.9511871933937073, val_acc: 0.75
epoch: 383: train_loss: 1.0594592767043247, train_acc: 0.511904756228129, val_loss: 0.9287443161010742, val_acc: 0.84375
epoch: 384: train_loss: 1.059201201009544, train_acc: 0.5148809552192688, val_loss: 0.9211834073066711, val_acc: 0.75
epoch: 385: train_loss: 1.0590631005986366, train_acc: 0.4583333333333333, val_loss: 0.928926020860672, val_acc: 0.828125
epoch: 386: train_loss: 1.0589846859298626, train_acc: 0.4181547661622365, val_loss: 0.9317805171012878, val_acc: 0.78125
epoch: 387: train_loss: 1.0588548752348037, train_acc: 0.4434523781140645, val_loss: 0.9220792949199677, val_acc: 0.84375
epoch: 388: train_loss: 1.058667697912623, train_acc: 0.4181547661622365, val_loss: 0.9390135109424591, val_acc: 0.796875
epoch: 389: train_loss: 1.0584666363194457, train_acc: 0.4702380895614624, val_loss: 0.9120365679264069, val_acc: 0.796875
epoch: 390: train_loss: 1.0582407002050656, train_acc: 0.4776785671710968, val_loss: 0.934659481048584, val_acc: 0.8125
epoch: 391: train_loss: 1.0581634644748403, train_acc: 0.4107142885526021, val_loss: 0.9224866926670074, val_acc: 0.8125
epoch: 392: train_loss: 1.0579568200236766, train_acc: 0.5223214228947958, val_loss: 0.9065539240837097, val_acc: 0.90625
epoch: 393: train_loss: 1.0578690009875549, train_acc: 0.4136904776096344, val_loss: 0.9158294200897217, val_acc: 0.84375
epoch: 394: train_loss: 1.0578205939586656, train_acc: 0.3779761890570323, val_loss: 0.9211122691631317, val_acc: 0.84375
epoch: 395: train_loss: 1.0575986025911388, train_acc: 0.5327380895614624, val_loss: 0.9621952474117279, val_acc: 0.671875
epoch: 396: train_loss: 1.057244296738523, train_acc: 0.5684523781140646, val_loss: 0.938022792339325, val_acc: 0.703125
epoch: 397: train_loss: 1.0570154594416594, train_acc: 0.5044642885526022, val_loss: 0.930681049823761, val_acc: 0.765625
epoch: 398: train_loss: 1.056794968613407, train_acc: 0.5163690447807312, val_loss: 0.9269145727157593, val_acc: 0.78125
epoch: 399: train_loss: 1.0566567031045755, train_acc: 0.4523809552192688, val_loss: 0.9263130128383636, val_acc: 0.921875
epoch: 400: train_loss: 1.0565774707326467, train_acc: 0.4285714328289032, val_loss: 0.8917557597160339, val_acc: 0.828125
epoch: 401: train_loss: 1.0564233240598864, train_acc: 0.4895833333333333, val_loss: 0.9268255531787872, val_acc: 0.734375
epoch: 402: train_loss: 1.056229560162234, train_acc: 0.4985119005044301, val_loss: 0.9093790948390961, val_acc: 0.8125
epoch: 403: train_loss: 1.0561023068801796, train_acc: 0.4241071442763011, val_loss: 0.9505691230297089, val_acc: 0.6875
epoch: 404: train_loss: 1.0557816596678744, train_acc: 0.5654761989911398, val_loss: 0.9158256947994232, val_acc: 0.9375
epoch: 405: train_loss: 1.0554406484359589, train_acc: 0.5342261989911398, val_loss: 0.9123260080814362, val_acc: 0.875
epoch: 406: train_loss: 1.0552859406896928, train_acc: 0.4880952338377635, val_loss: 0.9446835815906525, val_acc: 0.75
epoch: 407: train_loss: 1.0550413422432598, train_acc: 0.5535714228947958, val_loss: 0.9084195494651794, val_acc: 0.859375
epoch: 408: train_loss: 1.0548851701333062, train_acc: 0.4821428656578064, val_loss: 0.9305974245071411, val_acc: 0.71875
epoch: 409: train_loss: 1.0546530227835584, train_acc: 0.5014880895614624, val_loss: 0.8909917771816254, val_acc: 0.90625
epoch: 410: train_loss: 1.0545554556019017, train_acc: 0.4315476218859355, val_loss: 0.9114655256271362, val_acc: 0.859375
epoch: 411: train_loss: 1.054364041844232, train_acc: 0.4895833333333333, val_loss: 0.9355107843875885, val_acc: 0.65625
epoch: 412: train_loss: 1.0541113011773502, train_acc: 0.5089285671710968, val_loss: 0.9205317795276642, val_acc: 0.796875
epoch: 413: train_loss: 1.053900386640223, train_acc: 0.53125, val_loss: 0.9270430207252502, val_acc: 0.875
epoch: 414: train_loss: 1.0537207060549632, train_acc: 0.4583333333333333, val_loss: 0.9282531142234802, val_acc: 0.8125
epoch: 415: train_loss: 1.053421300286666, train_acc: 0.523809532324473, val_loss: 0.9377299249172211, val_acc: 0.75
epoch: 416: train_loss: 1.0531564243882299, train_acc: 0.538690467675527, val_loss: 0.9140235483646393, val_acc: 0.796875
epoch: 417: train_loss: 1.052973560739362, train_acc: 0.4761904776096344, val_loss: 0.9356525540351868, val_acc: 0.796875
epoch: 418: train_loss: 1.0527975275199757, train_acc: 0.5014880895614624, val_loss: 0.9284903109073639, val_acc: 0.796875
epoch: 419: train_loss: 1.052704341118298, train_acc: 0.4345238109429677, val_loss: 0.9416568279266357, val_acc: 0.734375
epoch: 420: train_loss: 1.0525698814237296, train_acc: 0.4434523781140645, val_loss: 0.9314571619033813, val_acc: 0.84375
epoch: 421: train_loss: 1.0523591261433576, train_acc: 0.4910714228947957, val_loss: 0.9364039897918701, val_acc: 0.71875
epoch: 422: train_loss: 1.0522109456565834, train_acc: 0.4583333333333333, val_loss: 0.9108557999134064, val_acc: 0.890625
epoch: 423: train_loss: 1.0519614199984748, train_acc: 0.5610119005044302, val_loss: 0.9369771480560303, val_acc: 0.8125
epoch: 424: train_loss: 1.051669380945318, train_acc: 0.5416666666666666, val_loss: 0.8976709842681885, val_acc: 0.875
epoch: 425: train_loss: 1.051519233949121, train_acc: 0.4806547562281291, val_loss: 0.9049654603004456, val_acc: 0.90625
epoch: 426: train_loss: 1.0513750549222602, train_acc: 0.4761904776096344, val_loss: 0.9136452674865723, val_acc: 0.75
epoch: 427: train_loss: 1.0510517195841977, train_acc: 0.5461309552192688, val_loss: 0.9138666689395905, val_acc: 0.828125
epoch: 428: train_loss: 1.0509217667227673, train_acc: 0.5014880895614624, val_loss: 0.9068371951580048, val_acc: 0.84375
epoch: 429: train_loss: 1.0508038058299427, train_acc: 0.4642857114473979, val_loss: 0.8958866000175476, val_acc: 0.796875
epoch: 430: train_loss: 1.0506671829363772, train_acc: 0.4657738109429677, val_loss: 0.8929904103279114, val_acc: 0.8125
epoch: 431: train_loss: 1.050440376509487, train_acc: 0.5788690447807312, val_loss: 0.8944105803966522, val_acc: 0.84375
epoch: 432: train_loss: 1.050197998866198, train_acc: 0.5133928656578064, val_loss: 0.8991504311561584, val_acc: 0.90625
epoch: 433: train_loss: 1.0499587962246528, train_acc: 0.5148809552192688, val_loss: 0.9184984862804413, val_acc: 0.796875
epoch: 434: train_loss: 1.0496512120710937, train_acc: 0.5461309552192688, val_loss: 0.8973529040813446, val_acc: 0.875
epoch: 435: train_loss: 1.049425975946476, train_acc: 0.4851190447807312, val_loss: 0.8985835909843445, val_acc: 0.84375
epoch: 436: train_loss: 1.0491882416694607, train_acc: 0.5327380895614624, val_loss: 0.8888109624385834, val_acc: 0.859375
epoch: 437: train_loss: 1.0491206530748074, train_acc: 0.3958333333333333, val_loss: 0.9041185975074768, val_acc: 0.859375
epoch: 438: train_loss: 1.048996688519451, train_acc: 0.4583333333333333, val_loss: 0.8919396996498108, val_acc: 0.796875
epoch: 439: train_loss: 1.0487181097720608, train_acc: 0.5610119005044302, val_loss: 0.9194066524505615, val_acc: 0.734375
epoch: 440: train_loss: 1.048542886184006, train_acc: 0.4880952338377635, val_loss: 0.8763042390346527, val_acc: 0.828125
epoch: 441: train_loss: 1.048290499240205, train_acc: 0.5580357114473978, val_loss: 0.9302405118942261, val_acc: 0.828125
epoch: 442: train_loss: 1.0480405010165263, train_acc: 0.5610119005044302, val_loss: 0.9259669780731201, val_acc: 0.8125
epoch: 443: train_loss: 1.047774624269646, train_acc: 0.5565476218859354, val_loss: 0.8899905979633331, val_acc: 0.828125
epoch: 444: train_loss: 1.047457314162665, train_acc: 0.574404756228129, val_loss: 0.8852857053279877, val_acc: 0.796875
epoch: 445: train_loss: 1.047362333099582, train_acc: 0.4255952338377635, val_loss: 0.8912729620933533, val_acc: 0.8125
epoch: 446: train_loss: 1.0471381139701912, train_acc: 0.5148809552192688, val_loss: 0.883561909198761, val_acc: 0.859375
epoch: 447: train_loss: 1.0470029340968245, train_acc: 0.4389880895614624, val_loss: 0.9193015396595001, val_acc: 0.703125
epoch: 448: train_loss: 1.0467247831086894, train_acc: 0.5461309552192688, val_loss: 0.9085603952407837, val_acc: 0.78125
epoch: 449: train_loss: 1.0465116702185737, train_acc: 0.4895833333333333, val_loss: 0.9132535457611084, val_acc: 0.78125
epoch: 450: train_loss: 1.0464055714832794, train_acc: 0.4821428656578064, val_loss: 0.9219277799129486, val_acc: 0.796875
epoch: 451: train_loss: 1.046247387749959, train_acc: 0.4985119005044301, val_loss: 0.8951887786388397, val_acc: 0.78125
epoch: 452: train_loss: 1.0459991467621854, train_acc: 0.5163690447807312, val_loss: 0.8982225060462952, val_acc: 0.78125
epoch: 453: train_loss: 1.0457327929099751, train_acc: 0.5967261989911398, val_loss: 0.900409460067749, val_acc: 0.78125
epoch: 454: train_loss: 1.0455553329471268, train_acc: 0.4479166666666667, val_loss: 0.8869443833827972, val_acc: 0.84375
epoch: 455: train_loss: 1.0453300370197547, train_acc: 0.5267857114473978, val_loss: 0.8948867917060852, val_acc: 0.8125
epoch: 456: train_loss: 1.0451016009158238, train_acc: 0.5074404776096344, val_loss: 0.90934619307518, val_acc: 0.78125
epoch: 457: train_loss: 1.0447753526845178, train_acc: 0.5669642885526022, val_loss: 0.8715868294239044, val_acc: 0.921875
epoch: 458: train_loss: 1.0445346139701448, train_acc: 0.5297619005044302, val_loss: 0.9056974947452545, val_acc: 0.796875
epoch: 459: train_loss: 1.0442800432875536, train_acc: 0.5, val_loss: 0.9035814702510834, val_acc: 0.859375
epoch: 460: train_loss: 1.0439922988888843, train_acc: 0.5208333333333334, val_loss: 0.8789902627468109, val_acc: 0.875
epoch: 461: train_loss: 1.0437681685708473, train_acc: 0.5074404776096344, val_loss: 0.9175681173801422, val_acc: 0.8125
epoch: 462: train_loss: 1.043610866546974, train_acc: 0.4583333333333333, val_loss: 0.8805196285247803, val_acc: 0.8125
epoch: 463: train_loss: 1.0433755332025987, train_acc: 0.4925595323244731, val_loss: 0.8887088298797607, val_acc: 0.78125
epoch: 464: train_loss: 1.0432046956913443, train_acc: 0.4613095323244731, val_loss: 0.9065546095371246, val_acc: 0.765625
epoch: 465: train_loss: 1.0429640269501184, train_acc: 0.5327380895614624, val_loss: 0.8955738842487335, val_acc: 0.859375
epoch: 466: train_loss: 1.0427260335899096, train_acc: 0.5252976218859354, val_loss: 0.894578605890274, val_acc: 0.8125
epoch: 467: train_loss: 1.0424279336661013, train_acc: 0.5773809552192688, val_loss: 0.8990435004234314, val_acc: 0.84375
epoch: 468: train_loss: 1.0421187377772434, train_acc: 0.5654761989911398, val_loss: 0.8620977103710175, val_acc: 0.828125
epoch: 469: train_loss: 1.0418985627221722, train_acc: 0.5282738109429678, val_loss: 0.9219995141029358, val_acc: 0.671875
epoch: 470: train_loss: 1.0418094731524574, train_acc: 0.4494047661622365, val_loss: 0.9061379730701447, val_acc: 0.8125
epoch: 471: train_loss: 1.0416317312730905, train_acc: 0.5163690447807312, val_loss: 0.8783205449581146, val_acc: 0.90625
epoch: 472: train_loss: 1.0413695123096516, train_acc: 0.5074404776096344, val_loss: 0.9184448719024658, val_acc: 0.75
epoch: 473: train_loss: 1.041260944053258, train_acc: 0.4553571442763011, val_loss: 0.8871840238571167, val_acc: 0.859375
epoch: 474: train_loss: 1.0410456209433705, train_acc: 0.5357142885526022, val_loss: 0.9194820523262024, val_acc: 0.71875
epoch: 475: train_loss: 1.0407500274541999, train_acc: 0.5342261989911398, val_loss: 0.8689391613006592, val_acc: 0.90625
epoch: 476: train_loss: 1.0404236033027776, train_acc: 0.581845243771871, val_loss: 0.8963273763656616, val_acc: 0.84375
epoch: 477: train_loss: 1.0403237266769967, train_acc: 0.4404761890570323, val_loss: 0.8935955166816711, val_acc: 0.875
epoch: 478: train_loss: 1.0401051704802278, train_acc: 0.4970238109429677, val_loss: 0.8835793137550354, val_acc: 0.8125
epoch: 479: train_loss: 1.039943455118272, train_acc: 0.4598214328289032, val_loss: 0.8753015697002411, val_acc: 0.875
epoch: 480: train_loss: 1.0398009090231923, train_acc: 0.4538690447807312, val_loss: 0.8969696760177612, val_acc: 0.828125
epoch: 481: train_loss: 1.039688933192447, train_acc: 0.4196428557236989, val_loss: 0.8694911003112793, val_acc: 0.890625
epoch: 482: train_loss: 1.0396201515543289, train_acc: 0.4077380994955699, val_loss: 0.8862330317497253, val_acc: 0.859375
epoch: 483: train_loss: 1.0394155304704487, train_acc: 0.4895833333333333, val_loss: 0.8924797177314758, val_acc: 0.796875
epoch: 484: train_loss: 1.0392905312715117, train_acc: 0.4568452338377635, val_loss: 0.9034019708633423, val_acc: 0.75
epoch: 485: train_loss: 1.0391347885377122, train_acc: 0.4449404776096344, val_loss: 0.8766075074672699, val_acc: 0.875
epoch: 486: train_loss: 1.038932636288402, train_acc: 0.5461309552192688, val_loss: 0.8811320960521698, val_acc: 0.84375
epoch: 487: train_loss: 1.0387006498099676, train_acc: 0.5074404776096344, val_loss: 0.919234424829483, val_acc: 0.71875
epoch: 488: train_loss: 1.0385929596773524, train_acc: 0.4032738109429677, val_loss: 0.9005398452281952, val_acc: 0.78125
epoch: 489: train_loss: 1.0383849963038958, train_acc: 0.523809532324473, val_loss: 0.9146912693977356, val_acc: 0.78125
epoch: 490: train_loss: 1.0381246788195826, train_acc: 0.5342261989911398, val_loss: 0.9077963829040527, val_acc: 0.734375
epoch: 491: train_loss: 1.0378868987324439, train_acc: 0.511904756228129, val_loss: 0.868395209312439, val_acc: 0.890625
epoch: 492: train_loss: 1.0376195907592776, train_acc: 0.5773809552192688, val_loss: 0.8892665803432465, val_acc: 0.90625
epoch: 493: train_loss: 1.0375885266443658, train_acc: 0.4017857114473979, val_loss: 0.8752636313438416, val_acc: 0.921875
epoch: 494: train_loss: 1.0374919233900128, train_acc: 0.4538690447807312, val_loss: 0.8856297135353088, val_acc: 0.921875
epoch: 495: train_loss: 1.03746988872687, train_acc: 0.3705357114473979, val_loss: 0.868623286485672, val_acc: 0.84375
epoch: 496: train_loss: 1.0373452459302706, train_acc: 0.4360119005044301, val_loss: 0.8786168098449707, val_acc: 0.890625
epoch: 497: train_loss: 1.037104121772161, train_acc: 0.5401785671710968, val_loss: 0.8835873007774353, val_acc: 0.875
epoch: 498: train_loss: 1.036892051448325, train_acc: 0.5342261989911398, val_loss: 0.877103179693222, val_acc: 0.84375
epoch: 499: train_loss: 1.0366208789745968, train_acc: 0.569940467675527, val_loss: 0.8973914086818695, val_acc: 0.828125
epoch: 500: train_loss: 1.0363119219194947, train_acc: 0.6041666666666666, val_loss: 0.8893073797225952, val_acc: 0.8125
epoch: 501: train_loss: 1.036127876554669, train_acc: 0.5104166666666666, val_loss: 0.8688181638717651, val_acc: 0.921875
epoch: 502: train_loss: 1.035814826065061, train_acc: 0.5565476218859354, val_loss: 0.8867835700511932, val_acc: 0.84375
epoch: 503: train_loss: 1.0355753992048524, train_acc: 0.5163690447807312, val_loss: 0.8769262731075287, val_acc: 0.90625
epoch: 504: train_loss: 1.0353305160409156, train_acc: 0.5416666666666666, val_loss: 0.8841198086738586, val_acc: 0.765625
epoch: 505: train_loss: 1.0351025886215242, train_acc: 0.5163690447807312, val_loss: 0.886443704366684, val_acc: 0.78125
epoch: 506: train_loss: 1.034872757063972, train_acc: 0.586309532324473, val_loss: 0.8975189626216888, val_acc: 0.765625
epoch: 507: train_loss: 1.0347252312529434, train_acc: 0.4717261989911397, val_loss: 0.8876575827598572, val_acc: 0.78125
epoch: 508: train_loss: 1.0345784190361536, train_acc: 0.4717261989911397, val_loss: 0.8634176552295685, val_acc: 0.875
epoch: 509: train_loss: 1.0343717979449853, train_acc: 0.5327380895614624, val_loss: 0.8518659472465515, val_acc: 0.921875
epoch: 510: train_loss: 1.0341421498322285, train_acc: 0.5, val_loss: 0.903129905462265, val_acc: 0.8125
epoch: 511: train_loss: 1.0340381749362375, train_acc: 0.4330357114473979, val_loss: 0.8848783671855927, val_acc: 0.84375
epoch: 512: train_loss: 1.0340006265491537, train_acc: 0.3794642885526021, val_loss: 0.8729303479194641, val_acc: 0.921875
epoch: 513: train_loss: 1.033843422948773, train_acc: 0.4925595323244731, val_loss: 0.8478914499282837, val_acc: 0.9375
epoch: 514: train_loss: 1.0337112577216139, train_acc: 0.4657738109429677, val_loss: 0.8797109425067902, val_acc: 0.875
epoch: 515: train_loss: 1.0335359133645239, train_acc: 0.5193452338377634, val_loss: 0.9190523326396942, val_acc: 0.734375
epoch: 516: train_loss: 1.033397752304065, train_acc: 0.4776785671710968, val_loss: 0.8988650143146515, val_acc: 0.75
epoch: 517: train_loss: 1.0330955581008392, train_acc: 0.581845243771871, val_loss: 0.881763219833374, val_acc: 0.875
epoch: 518: train_loss: 1.032925673647423, train_acc: 0.4672619005044301, val_loss: 0.9092349708080292, val_acc: 0.765625
epoch: 519: train_loss: 1.0328090741848333, train_acc: 0.4598214228947957, val_loss: 0.8875027298927307, val_acc: 0.859375
epoch: 520: train_loss: 1.0326579863721563, train_acc: 0.4434523781140645, val_loss: 0.8384047150611877, val_acc: 0.90625
epoch: 521: train_loss: 1.0325071195456867, train_acc: 0.4523809552192688, val_loss: 0.8788353204727173, val_acc: 0.78125
epoch: 522: train_loss: 1.032248219614047, train_acc: 0.580357144276301, val_loss: 0.9033131003379822, val_acc: 0.75
epoch: 523: train_loss: 1.0321084503984934, train_acc: 0.5014880895614624, val_loss: 0.8605221807956696, val_acc: 0.875
epoch: 524: train_loss: 1.032002522112831, train_acc: 0.4419642885526021, val_loss: 0.8766351342201233, val_acc: 0.84375
epoch: 525: train_loss: 1.0318028044111374, train_acc: 0.4925595323244731, val_loss: 0.8696711957454681, val_acc: 0.859375
epoch: 526: train_loss: 1.0315614618657887, train_acc: 0.5252976218859354, val_loss: 0.8471968472003937, val_acc: 0.90625
epoch: 527: train_loss: 1.0313797522750163, train_acc: 0.5208333333333334, val_loss: 0.883714109659195, val_acc: 0.84375
epoch: 528: train_loss: 1.0312389147003713, train_acc: 0.4627976218859355, val_loss: 0.8726620376110077, val_acc: 0.875
epoch: 529: train_loss: 1.0310743081119822, train_acc: 0.5029761989911398, val_loss: 0.885615885257721, val_acc: 0.828125
epoch: 530: train_loss: 1.0308187878962511, train_acc: 0.5669642885526022, val_loss: 0.87580206990242, val_acc: 0.828125
epoch: 531: train_loss: 1.0306040448577776, train_acc: 0.5133928656578064, val_loss: 0.898034393787384, val_acc: 0.75
epoch: 532: train_loss: 1.0303863527776298, train_acc: 0.5327380895614624, val_loss: 0.8793026208877563, val_acc: 0.9375
epoch: 533: train_loss: 1.0301324450567864, train_acc: 0.5535714228947958, val_loss: 0.8526907861232758, val_acc: 0.90625
epoch: 534: train_loss: 1.0299686884582968, train_acc: 0.4880952338377635, val_loss: 0.9006853103637695, val_acc: 0.828125
epoch: 535: train_loss: 1.0298584241846305, train_acc: 0.4449404776096344, val_loss: 0.8818649649620056, val_acc: 0.828125
epoch: 536: train_loss: 1.0296173211878532, train_acc: 0.5416666666666666, val_loss: 0.8899485468864441, val_acc: 0.78125
epoch: 537: train_loss: 1.029378566600133, train_acc: 0.543154756228129, val_loss: 0.8772883415222168, val_acc: 0.84375
epoch: 538: train_loss: 1.0291857121163261, train_acc: 0.4821428656578064, val_loss: 0.852118581533432, val_acc: 0.8125
epoch: 539: train_loss: 1.02898691451108, train_acc: 0.5446428656578064, val_loss: 0.8857904374599457, val_acc: 0.828125
epoch: 540: train_loss: 1.0288608818970852, train_acc: 0.4389880994955699, val_loss: 0.8573372066020966, val_acc: 0.921875
epoch: 541: train_loss: 1.0286807716992505, train_acc: 0.4836309552192688, val_loss: 0.8604929447174072, val_acc: 0.859375
epoch: 542: train_loss: 1.0283700908306113, train_acc: 0.6026785771052042, val_loss: 0.8437029421329498, val_acc: 0.921875
epoch: 543: train_loss: 1.0282193573155236, train_acc: 0.4598214328289032, val_loss: 0.8635145425796509, val_acc: 0.78125
epoch: 544: train_loss: 1.0280580421471082, train_acc: 0.5342261989911398, val_loss: 0.8578090965747833, val_acc: 0.84375
epoch: 545: train_loss: 1.0277899028471826, train_acc: 0.5133928656578064, val_loss: 0.8706056177616119, val_acc: 0.90625
epoch: 546: train_loss: 1.0276180558782897, train_acc: 0.4851190447807312, val_loss: 0.8474940061569214, val_acc: 0.859375
epoch: 547: train_loss: 1.0274110614093257, train_acc: 0.523809532324473, val_loss: 0.8864472210407257, val_acc: 0.78125
epoch: 548: train_loss: 1.0272433594927757, train_acc: 0.4910714228947957, val_loss: 0.8824310898780823, val_acc: 0.9375
epoch: 549: train_loss: 1.0270096317927038, train_acc: 0.523809532324473, val_loss: 0.8695179522037506, val_acc: 0.84375
epoch: 550: train_loss: 1.026889784943314, train_acc: 0.4747023781140645, val_loss: 0.8599885702133179, val_acc: 0.875
epoch: 551: train_loss: 1.0266833996787157, train_acc: 0.5014880895614624, val_loss: 0.8871540129184723, val_acc: 0.734375
epoch: 552: train_loss: 1.026562778363391, train_acc: 0.4479166666666667, val_loss: 0.8157915771007538, val_acc: 0.90625
epoch: 553: train_loss: 1.0263699366656158, train_acc: 0.4910714228947957, val_loss: 0.8969964683055878, val_acc: 0.75
epoch: 554: train_loss: 1.026161030606106, train_acc: 0.5104166666666666, val_loss: 0.8567689061164856, val_acc: 0.859375
epoch: 555: train_loss: 1.0259908794356185, train_acc: 0.4955357114473979, val_loss: 0.8395674526691437, val_acc: 0.859375
epoch: 556: train_loss: 1.0257473324031467, train_acc: 0.5684523781140646, val_loss: 0.8842470049858093, val_acc: 0.859375
epoch: 557: train_loss: 1.0255717304729923, train_acc: 0.4851190447807312, val_loss: 0.8772185742855072, val_acc: 0.765625
epoch: 558: train_loss: 1.0253956230111818, train_acc: 0.5059523781140646, val_loss: 0.8666933178901672, val_acc: 0.875
epoch: 559: train_loss: 1.0251939498952451, train_acc: 0.5223214228947958, val_loss: 0.8585706353187561, val_acc: 0.90625
epoch: 560: train_loss: 1.0250091571150541, train_acc: 0.5029761989911398, val_loss: 0.8596127927303314, val_acc: 0.78125
epoch: 561: train_loss: 1.0248808867849344, train_acc: 0.4419642885526021, val_loss: 0.8557708263397217, val_acc: 0.828125
epoch: 562: train_loss: 1.0247474177974856, train_acc: 0.46875, val_loss: 0.8438238203525543, val_acc: 0.90625
epoch: 563: train_loss: 1.0244922438200479, train_acc: 0.5386904776096344, val_loss: 0.8475223183631897, val_acc: 0.921875
epoch: 564: train_loss: 1.0241832041810743, train_acc: 0.5833333333333334, val_loss: 0.8199905157089233, val_acc: 0.9375
epoch: 565: train_loss: 1.0239682764481874, train_acc: 0.5178571343421936, val_loss: 0.831127792596817, val_acc: 0.921875
epoch: 566: train_loss: 1.023703694588853, train_acc: 0.5565476218859354, val_loss: 0.8314586877822876, val_acc: 0.875
epoch: 567: train_loss: 1.0235149842192865, train_acc: 0.5297619005044302, val_loss: 0.8844941258430481, val_acc: 0.828125
epoch: 568: train_loss: 1.0232872764269505, train_acc: 0.5758928656578064, val_loss: 0.8509480357170105, val_acc: 0.890625
epoch: 569: train_loss: 1.0230421001799617, train_acc: 0.53125, val_loss: 0.854264885187149, val_acc: 0.9375
epoch: 570: train_loss: 1.0229156855178445, train_acc: 0.4464285671710968, val_loss: 0.8262932896614075, val_acc: 0.921875
epoch: 571: train_loss: 1.0226902235776945, train_acc: 0.5535714228947958, val_loss: 0.8614147305488586, val_acc: 0.90625
epoch: 572: train_loss: 1.0224925516094139, train_acc: 0.5252976218859354, val_loss: 0.8190585672855377, val_acc: 0.953125
epoch: 573: train_loss: 1.0222517686715382, train_acc: 0.5357142885526022, val_loss: 0.8637477457523346, val_acc: 0.8125
epoch: 574: train_loss: 1.0220631243871603, train_acc: 0.4866071442763011, val_loss: 0.8573246598243713, val_acc: 0.859375
epoch: 575: train_loss: 1.021944156537453, train_acc: 0.4627976218859355, val_loss: 0.8398315906524658, val_acc: 0.890625
epoch: 576: train_loss: 1.0218002040010181, train_acc: 0.4702380895614624, val_loss: 0.8273943960666656, val_acc: 0.875
epoch: 577: train_loss: 1.0215894445484754, train_acc: 0.5267857114473978, val_loss: 0.8275309503078461, val_acc: 0.90625
epoch: 578: train_loss: 1.0213825310793285, train_acc: 0.5342261989911398, val_loss: 0.8366765379905701, val_acc: 0.921875
epoch: 579: train_loss: 1.0210661292076104, train_acc: 0.6071428656578064, val_loss: 0.8478277325630188, val_acc: 0.9375
epoch: 580: train_loss: 1.020946487604853, train_acc: 0.46875, val_loss: 0.8596254587173462, val_acc: 0.875
epoch: 581: train_loss: 1.0207123880328994, train_acc: 0.5416666666666666, val_loss: 0.8454064130783081, val_acc: 0.9375
epoch: 582: train_loss: 1.0205383533542665, train_acc: 0.4955357114473979, val_loss: 0.8258585035800934, val_acc: 0.921875
epoch: 583: train_loss: 1.020396840218539, train_acc: 0.4583333333333333, val_loss: 0.8821592926979065, val_acc: 0.765625
epoch: 584: train_loss: 1.0201917944470695, train_acc: 0.4836309552192688, val_loss: 0.8194132149219513, val_acc: 0.921875
epoch: 585: train_loss: 1.0198123479798107, train_acc: 0.6413690447807312, val_loss: 0.8482279777526855, val_acc: 0.859375
epoch: 586: train_loss: 1.0194610427347919, train_acc: 0.6086309552192688, val_loss: 0.8159660696983337, val_acc: 0.84375
epoch: 587: train_loss: 1.0193446972945912, train_acc: 0.4732142885526021, val_loss: 0.8254059553146362, val_acc: 0.84375
epoch: 588: train_loss: 1.0192242958617188, train_acc: 0.4627976218859355, val_loss: 0.8747850060462952, val_acc: 0.796875
epoch: 589: train_loss: 1.0190944780400908, train_acc: 0.5029761989911398, val_loss: 0.8662139475345612, val_acc: 0.796875
epoch: 590: train_loss: 1.0188518247897511, train_acc: 0.549107144276301, val_loss: 0.833398312330246, val_acc: 0.9375
epoch: 591: train_loss: 1.0185699559211185, train_acc: 0.5595238010088602, val_loss: 0.8470969498157501, val_acc: 0.890625
epoch: 592: train_loss: 1.018380040664093, train_acc: 0.523809532324473, val_loss: 0.8848564326763153, val_acc: 0.75
epoch: 593: train_loss: 1.0181951066928538, train_acc: 0.513392855723699, val_loss: 0.8559706509113312, val_acc: 0.859375
epoch: 594: train_loss: 1.0180840247819396, train_acc: 0.4300595223903656, val_loss: 0.8499613106250763, val_acc: 0.75
epoch: 595: train_loss: 1.0179917064955828, train_acc: 0.4434523781140645, val_loss: 0.8652208745479584, val_acc: 0.75
epoch: 596: train_loss: 1.0179166982521357, train_acc: 0.4434523781140645, val_loss: 0.8589090406894684, val_acc: 0.875
epoch: 597: train_loss: 1.0177188472803613, train_acc: 0.4985119005044301, val_loss: 0.8747533559799194, val_acc: 0.796875
epoch: 598: train_loss: 1.0176151504965039, train_acc: 0.4300595223903656, val_loss: 0.818613588809967, val_acc: 0.9375
epoch: 599: train_loss: 1.017461966805987, train_acc: 0.5, val_loss: 0.8320648074150085, val_acc: 0.859375
epoch: 600: train_loss: 1.0173092172401048, train_acc: 0.4791666666666667, val_loss: 0.8295046389102936, val_acc: 0.84375
epoch: 601: train_loss: 1.0171085952194285, train_acc: 0.5282738109429678, val_loss: 0.8263276219367981, val_acc: 0.9375
epoch: 602: train_loss: 1.016816197178388, train_acc: 0.5580357114473978, val_loss: 0.8538753092288971, val_acc: 0.859375
epoch: 603: train_loss: 1.0166048537816423, train_acc: 0.538690467675527, val_loss: 0.8708848059177399, val_acc: 0.828125
epoch: 604: train_loss: 1.0164223176717093, train_acc: 0.5104166666666666, val_loss: 0.8105231523513794, val_acc: 0.9375
epoch: 605: train_loss: 1.0161887570904156, train_acc: 0.5625, val_loss: 0.8521280288696289, val_acc: 0.828125
epoch: 606: train_loss: 1.015980886961325, train_acc: 0.5163690447807312, val_loss: 0.8514144122600555, val_acc: 0.875
epoch: 607: train_loss: 1.015710869444566, train_acc: 0.5833333333333334, val_loss: 0.8406287133693695, val_acc: 0.875
epoch: 608: train_loss: 1.015501037182935, train_acc: 0.5282738010088602, val_loss: 0.8398321270942688, val_acc: 0.84375
epoch: 609: train_loss: 1.015217426174976, train_acc: 0.5818452338377634, val_loss: 0.8686999976634979, val_acc: 0.765625
epoch: 610: train_loss: 1.0149896480122738, train_acc: 0.5788690447807312, val_loss: 0.8528479337692261, val_acc: 0.875
epoch: 611: train_loss: 1.0147982719222437, train_acc: 0.5, val_loss: 0.837927907705307, val_acc: 0.953125
epoch: 612: train_loss: 1.0146152601415783, train_acc: 0.4955357114473979, val_loss: 0.8371224105358124, val_acc: 0.890625
epoch: 613: train_loss: 1.014403371061486, train_acc: 0.5416666666666666, val_loss: 0.8584294319152832, val_acc: 0.8125
epoch: 614: train_loss: 1.0142896706818882, train_acc: 0.4657738109429677, val_loss: 0.8500136733055115, val_acc: 0.859375
epoch: 615: train_loss: 1.0140624940072813, train_acc: 0.550595243771871, val_loss: 0.812484472990036, val_acc: 0.90625
epoch: 616: train_loss: 1.0138284637759267, train_acc: 0.5580357114473978, val_loss: 0.837693601846695, val_acc: 0.90625
epoch: 617: train_loss: 1.0137733938946043, train_acc: 0.3928571442763011, val_loss: 0.8719911575317383, val_acc: 0.875
epoch: 618: train_loss: 1.013670842398477, train_acc: 0.4107142885526021, val_loss: 0.8425935208797455, val_acc: 0.890625
epoch: 619: train_loss: 1.0135800246910376, train_acc: 0.4434523781140645, val_loss: 0.8684746325016022, val_acc: 0.828125
epoch: 620: train_loss: 1.0134002668078197, train_acc: 0.4761904776096344, val_loss: 0.8618198931217194, val_acc: 0.890625
epoch: 621: train_loss: 1.0132621929193228, train_acc: 0.4583333333333333, val_loss: 0.8613495826721191, val_acc: 0.84375
epoch: 622: train_loss: 1.013016726448932, train_acc: 0.5461309552192688, val_loss: 0.8434786200523376, val_acc: 0.78125
epoch: 623: train_loss: 1.0128370920817051, train_acc: 0.5223214228947958, val_loss: 0.8488267958164215, val_acc: 0.796875
epoch: 624: train_loss: 1.0126945528666171, train_acc: 0.4806547562281291, val_loss: 0.8382432460784912, val_acc: 0.921875
epoch: 625: train_loss: 1.0124615433053588, train_acc: 0.5565476218859354, val_loss: 0.8320306539535522, val_acc: 0.921875
epoch: 626: train_loss: 1.0121732769273553, train_acc: 0.6026785771052042, val_loss: 0.8372068405151367, val_acc: 0.9375
epoch: 627: train_loss: 1.0118756494198122, train_acc: 0.6026785771052042, val_loss: 0.8752506375312805, val_acc: 0.8125
epoch: 628: train_loss: 1.0116774272400668, train_acc: 0.5014880895614624, val_loss: 0.8363589644432068, val_acc: 0.9375
epoch: 629: train_loss: 1.0116172540440125, train_acc: 0.4211309552192688, val_loss: 0.819047600030899, val_acc: 0.921875
epoch: 630: train_loss: 1.0113740159730962, train_acc: 0.5327380895614624, val_loss: 0.8808637857437134, val_acc: 0.796875
epoch: 631: train_loss: 1.011149097469788, train_acc: 0.5297619005044302, val_loss: 0.8356689810752869, val_acc: 0.796875
epoch: 632: train_loss: 1.0109998601056698, train_acc: 0.4657738109429677, val_loss: 0.8345451354980469, val_acc: 0.921875
epoch: 633: train_loss: 1.0107523863185213, train_acc: 0.5818452338377634, val_loss: 0.8479833602905273, val_acc: 0.875
epoch: 634: train_loss: 1.0104772171323377, train_acc: 0.5788690447807312, val_loss: 0.8623856902122498, val_acc: 0.828125
epoch: 635: train_loss: 1.010270442509051, train_acc: 0.555059532324473, val_loss: 0.8108894526958466, val_acc: 0.953125
epoch: 636: train_loss: 1.010021626637407, train_acc: 0.5580357114473978, val_loss: 0.8150311708450317, val_acc: 0.90625
epoch: 637: train_loss: 1.009829572375665, train_acc: 0.5044642885526022, val_loss: 0.8362188637256622, val_acc: 0.875
epoch: 638: train_loss: 1.0096901889014003, train_acc: 0.4791666666666667, val_loss: 0.8834858536720276, val_acc: 0.84375
epoch: 639: train_loss: 1.0095751613688961, train_acc: 0.4657738109429677, val_loss: 0.8517736196517944, val_acc: 0.890625
epoch: 640: train_loss: 1.0093842360481142, train_acc: 0.5014880895614624, val_loss: 0.8393694758415222, val_acc: 0.828125
epoch: 641: train_loss: 1.009131093912778, train_acc: 0.580357144276301, val_loss: 0.8198863565921783, val_acc: 0.8125
epoch: 642: train_loss: 1.0089988481077643, train_acc: 0.4479166666666667, val_loss: 0.8554290235042572, val_acc: 0.859375
epoch: 643: train_loss: 1.0087660113600219, train_acc: 0.5252976218859354, val_loss: 0.8632491230964661, val_acc: 0.890625
epoch: 644: train_loss: 1.0085846558097715, train_acc: 0.5342261989911398, val_loss: 0.8074852228164673, val_acc: 0.90625
epoch: 645: train_loss: 1.0083247095008132, train_acc: 0.5669642885526022, val_loss: 0.8561365604400635, val_acc: 0.8125
epoch: 646: train_loss: 1.0081506320829547, train_acc: 0.4836309552192688, val_loss: 0.8093875050544739, val_acc: 0.859375
epoch: 647: train_loss: 1.0080599005452884, train_acc: 0.4389880895614624, val_loss: 0.8242000937461853, val_acc: 0.828125
epoch: 648: train_loss: 1.0078791433808487, train_acc: 0.5, val_loss: 0.8337014615535736, val_acc: 0.875
epoch: 649: train_loss: 1.0076832004082503, train_acc: 0.5133928656578064, val_loss: 0.8126481771469116, val_acc: 0.890625
epoch: 650: train_loss: 1.0074932491907502, train_acc: 0.4985119005044301, val_loss: 0.8290105760097504, val_acc: 0.84375
epoch: 651: train_loss: 1.0073801535648308, train_acc: 0.4226190447807312, val_loss: 0.8467241525650024, val_acc: 0.796875
epoch: 652: train_loss: 1.0071757085290467, train_acc: 0.5104166666666666, val_loss: 0.8064340353012085, val_acc: 0.859375
epoch: 653: train_loss: 1.006989026227614, train_acc: 0.4895833333333333, val_loss: 0.818535566329956, val_acc: 0.875
epoch: 654: train_loss: 1.0068536299785578, train_acc: 0.5014880895614624, val_loss: 0.8461487293243408, val_acc: 0.890625
epoch: 655: train_loss: 1.0068527914099086, train_acc: 0.3303571442763011, val_loss: 0.8584539294242859, val_acc: 0.828125
epoch: 656: train_loss: 1.0067014431118777, train_acc: 0.4657738109429677, val_loss: 0.8356451988220215, val_acc: 0.875
epoch: 657: train_loss: 1.0065224727477942, train_acc: 0.5, val_loss: 0.8077839910984039, val_acc: 0.859375
epoch: 658: train_loss: 1.00642703805961, train_acc: 0.4419642885526021, val_loss: 0.8452789485454559, val_acc: 0.875
epoch: 659: train_loss: 1.0061774191832296, train_acc: 0.5044642885526022, val_loss: 0.8341136872768402, val_acc: 0.90625
epoch: 660: train_loss: 1.0058712283890467, train_acc: 0.5877976218859354, val_loss: 0.8080798387527466, val_acc: 0.859375
epoch: 661: train_loss: 1.0056954020819988, train_acc: 0.4925595323244731, val_loss: 0.8424966037273407, val_acc: 0.859375
epoch: 662: train_loss: 1.0054555079216563, train_acc: 0.5773809552192688, val_loss: 0.8149607181549072, val_acc: 0.9375
epoch: 663: train_loss: 1.00518317649283, train_acc: 0.5788690447807312, val_loss: 0.8346061408519745, val_acc: 0.921875
epoch: 664: train_loss: 1.0049230513118554, train_acc: 0.5654761989911398, val_loss: 0.8617590665817261, val_acc: 0.78125
epoch: 665: train_loss: 1.0047267111572051, train_acc: 0.5461309552192688, val_loss: 0.835136204957962, val_acc: 0.875
epoch: 666: train_loss: 1.0044995481106465, train_acc: 0.5193452338377634, val_loss: 0.8048931658267975, val_acc: 0.9375
epoch: 667: train_loss: 1.0042156812078693, train_acc: 0.555059532324473, val_loss: 0.8518438339233398, val_acc: 0.859375
epoch: 668: train_loss: 1.0040920411821728, train_acc: 0.4776785671710968, val_loss: 0.8011752367019653, val_acc: 0.90625
epoch: 669: train_loss: 1.0038735549841347, train_acc: 0.5357142885526022, val_loss: 0.835360586643219, val_acc: 0.84375
epoch: 670: train_loss: 1.0036644806508919, train_acc: 0.4776785671710968, val_loss: 0.7924621403217316, val_acc: 0.921875
epoch: 671: train_loss: 1.0034068536959462, train_acc: 0.53125, val_loss: 0.8270320296287537, val_acc: 0.953125
epoch: 672: train_loss: 1.0032679735285734, train_acc: 0.4672619005044301, val_loss: 0.8136919736862183, val_acc: 0.859375
epoch: 673: train_loss: 1.0030398191082717, train_acc: 0.538690467675527, val_loss: 0.8250004351139069, val_acc: 0.859375
epoch: 674: train_loss: 1.0029272268730909, train_acc: 0.4330357114473979, val_loss: 0.8270772397518158, val_acc: 0.875
epoch: 675: train_loss: 1.002622696627996, train_acc: 0.6101190447807312, val_loss: 0.8095449805259705, val_acc: 0.90625
epoch: 676: train_loss: 1.002438872539016, train_acc: 0.5044642885526022, val_loss: 0.8172532021999359, val_acc: 0.90625
epoch: 677: train_loss: 1.0023036363846893, train_acc: 0.46875, val_loss: 0.8253659307956696, val_acc: 0.890625
epoch: 678: train_loss: 1.0021088025589806, train_acc: 0.5401785671710968, val_loss: 0.7990200221538544, val_acc: 0.953125
epoch: 679: train_loss: 1.0019814951162704, train_acc: 0.4732142885526021, val_loss: 0.8103840053081512, val_acc: 0.875
epoch: 680: train_loss: 1.0018248631273823, train_acc: 0.4880952338377635, val_loss: 0.8265595734119415, val_acc: 0.890625
epoch: 681: train_loss: 1.0016883967092884, train_acc: 0.4613095323244731, val_loss: 0.8026142418384552, val_acc: 0.9375
epoch: 682: train_loss: 1.0014855881735654, train_acc: 0.5014880895614624, val_loss: 0.8059380948543549, val_acc: 0.84375
epoch: 683: train_loss: 1.0012514837826896, train_acc: 0.6116071343421936, val_loss: 0.8230347633361816, val_acc: 0.9375
epoch: 684: train_loss: 1.00109042442628, train_acc: 0.5193452338377634, val_loss: 0.8015236258506775, val_acc: 0.9375
epoch: 685: train_loss: 1.0009376372783587, train_acc: 0.4627976218859355, val_loss: 0.8276822865009308, val_acc: 0.890625
epoch: 686: train_loss: 1.0006857104292193, train_acc: 0.5565476218859354, val_loss: 0.81354159116745, val_acc: 0.890625
epoch: 687: train_loss: 1.0005529985813664, train_acc: 0.4717261989911397, val_loss: 0.8223419785499573, val_acc: 0.90625
epoch: 688: train_loss: 1.0003062353424819, train_acc: 0.5788690447807312, val_loss: 0.8611056208610535, val_acc: 0.828125
epoch: 689: train_loss: 1.0001241420202203, train_acc: 0.4880952338377635, val_loss: 0.8235694169998169, val_acc: 0.890625
epoch: 690: train_loss: 1.0000143731727262, train_acc: 0.4255952338377635, val_loss: 0.7883265614509583, val_acc: 0.90625
epoch: 691: train_loss: 0.9999039863632816, train_acc: 0.4717261989911397, val_loss: 0.8372725248336792, val_acc: 0.796875
epoch: 692: train_loss: 0.9997488254248488, train_acc: 0.5044642885526022, val_loss: 0.8016724288463593, val_acc: 0.890625
epoch: 693: train_loss: 0.9995990373501968, train_acc: 0.5014880895614624, val_loss: 0.8385540544986725, val_acc: 0.890625
epoch: 694: train_loss: 0.999343290003083, train_acc: 0.6101190447807312, val_loss: 0.8002210855484009, val_acc: 0.890625
epoch: 695: train_loss: 0.999212971677953, train_acc: 0.4910714328289032, val_loss: 0.8531516492366791, val_acc: 0.796875
epoch: 696: train_loss: 0.9989779152578276, train_acc: 0.543154756228129, val_loss: 0.8455043435096741, val_acc: 0.84375
epoch: 697: train_loss: 0.9988349615218184, train_acc: 0.4553571442763011, val_loss: 0.785394012928009, val_acc: 0.9375
epoch: 698: train_loss: 0.9987241058163142, train_acc: 0.4642857114473979, val_loss: 0.8072906732559204, val_acc: 0.875
epoch: 699: train_loss: 0.9985628694295875, train_acc: 0.5044642885526022, val_loss: 0.8520425856113434, val_acc: 0.78125
epoch: 700: train_loss: 0.9983873727828384, train_acc: 0.4866071442763011, val_loss: 0.8235368728637695, val_acc: 0.84375
epoch: 701: train_loss: 0.9982671499365521, train_acc: 0.4821428656578064, val_loss: 0.818621963262558, val_acc: 0.78125
epoch: 702: train_loss: 0.9980723550106002, train_acc: 0.5342261989911398, val_loss: 0.7918891906738281, val_acc: 0.9375
epoch: 703: train_loss: 0.9979613087506901, train_acc: 0.4464285671710968, val_loss: 0.8083164989948273, val_acc: 0.828125
epoch: 704: train_loss: 0.9978275339372326, train_acc: 0.4672619005044301, val_loss: 0.8203122615814209, val_acc: 0.921875
epoch: 705: train_loss: 0.9975351664792595, train_acc: 0.6205357114473978, val_loss: 0.818954348564148, val_acc: 0.90625
epoch: 706: train_loss: 0.9973783807223942, train_acc: 0.4985119005044301, val_loss: 0.8067662119865417, val_acc: 0.859375
epoch: 707: train_loss: 0.9971850789143308, train_acc: 0.5267857114473978, val_loss: 0.8070211112499237, val_acc: 0.90625
epoch: 708: train_loss: 0.9970124858222652, train_acc: 0.4880952338377635, val_loss: 0.8079102635383606, val_acc: 0.8125
epoch: 709: train_loss: 0.9968340740797097, train_acc: 0.5342261989911398, val_loss: 0.8188270926475525, val_acc: 0.9375
epoch: 710: train_loss: 0.9966027026326224, train_acc: 0.5476190447807312, val_loss: 0.8173337578773499, val_acc: 0.828125
epoch: 711: train_loss: 0.996513546210772, train_acc: 0.4404761989911397, val_loss: 0.8319027125835419, val_acc: 0.8125
epoch: 712: train_loss: 0.9964007263765419, train_acc: 0.4553571442763011, val_loss: 0.7531420290470123, val_acc: 0.921875
epoch: 713: train_loss: 0.9962383614979975, train_acc: 0.4761904776096344, val_loss: 0.8287107944488525, val_acc: 0.75
epoch: 714: train_loss: 0.9960625453984534, train_acc: 0.5133928656578064, val_loss: 0.7845208942890167, val_acc: 0.921875
epoch: 715: train_loss: 0.9958928224372233, train_acc: 0.5148809552192688, val_loss: 0.8254179656505585, val_acc: 0.828125
epoch: 716: train_loss: 0.9957244959835219, train_acc: 0.4717261989911397, val_loss: 0.8291011154651642, val_acc: 0.890625
epoch: 717: train_loss: 0.9954701896165398, train_acc: 0.6026785671710968, val_loss: 0.8682995438575745, val_acc: 0.890625
epoch: 718: train_loss: 0.995372970429182, train_acc: 0.4747023781140645, val_loss: 0.7851009666919708, val_acc: 0.953125
epoch: 719: train_loss: 0.995027362831212, train_acc: 0.6279761989911398, val_loss: 0.824745386838913, val_acc: 0.859375
epoch: 720: train_loss: 0.9947852882807631, train_acc: 0.5223214228947958, val_loss: 0.8283399641513824, val_acc: 0.828125
epoch: 721: train_loss: 0.9946436686363898, train_acc: 0.4657738109429677, val_loss: 0.811643660068512, val_acc: 0.859375
epoch: 722: train_loss: 0.994435475794825, train_acc: 0.5446428656578064, val_loss: 0.7894600331783295, val_acc: 0.890625
epoch: 723: train_loss: 0.9941915743478305, train_acc: 0.6086309552192688, val_loss: 0.813417911529541, val_acc: 0.9375
epoch: 724: train_loss: 0.9940108028499549, train_acc: 0.4806547562281291, val_loss: 0.8094654381275177, val_acc: 0.921875
epoch: 725: train_loss: 0.9939293852084476, train_acc: 0.4568452338377635, val_loss: 0.7973856925964355, val_acc: 0.890625
epoch: 726: train_loss: 0.9937228323231582, train_acc: 0.5223214228947958, val_loss: 0.7859496474266052, val_acc: 0.921875
epoch: 727: train_loss: 0.9936128405806336, train_acc: 0.4657738109429677, val_loss: 0.781694233417511, val_acc: 0.90625
epoch: 728: train_loss: 0.9933879558877625, train_acc: 0.5416666666666666, val_loss: 0.8024340569972992, val_acc: 0.90625
epoch: 729: train_loss: 0.9932123669478437, train_acc: 0.5133928656578064, val_loss: 0.8179677724838257, val_acc: 0.890625
epoch: 730: train_loss: 0.9930128469295378, train_acc: 0.5223214228947958, val_loss: 0.7778179049491882, val_acc: 0.9375
epoch: 731: train_loss: 0.9927599569324577, train_acc: 0.5610119005044302, val_loss: 0.8368568122386932, val_acc: 0.875
epoch: 732: train_loss: 0.9925668506038996, train_acc: 0.4985119005044301, val_loss: 0.8225158452987671, val_acc: 0.890625
epoch: 733: train_loss: 0.992353013490785, train_acc: 0.5074404776096344, val_loss: 0.8505060076713562, val_acc: 0.75
epoch: 734: train_loss: 0.9922047436372482, train_acc: 0.5044642885526022, val_loss: 0.797742486000061, val_acc: 0.9375
epoch: 735: train_loss: 0.991858229122083, train_acc: 0.6383928656578064, val_loss: 0.8247579336166382, val_acc: 0.828125
epoch: 736: train_loss: 0.9916004275423743, train_acc: 0.5967261989911398, val_loss: 0.8552146255970001, val_acc: 0.78125
epoch: 737: train_loss: 0.991494771157068, train_acc: 0.4375, val_loss: 0.8337983787059784, val_acc: 0.890625
epoch: 738: train_loss: 0.9913208478734887, train_acc: 0.5044642885526022, val_loss: 0.8159623146057129, val_acc: 0.875
epoch: 739: train_loss: 0.9911780373201703, train_acc: 0.4880952338377635, val_loss: 0.8474522531032562, val_acc: 0.875
epoch: 740: train_loss: 0.9910390659382458, train_acc: 0.5297619005044302, val_loss: 0.7763833105564117, val_acc: 0.9375
epoch: 741: train_loss: 0.990825360033389, train_acc: 0.555059532324473, val_loss: 0.8101578950881958, val_acc: 0.859375
epoch: 742: train_loss: 0.9907174354018828, train_acc: 0.4761904776096344, val_loss: 0.7917093634605408, val_acc: 0.953125
epoch: 743: train_loss: 0.9905680598003451, train_acc: 0.4702380994955699, val_loss: 0.7757296562194824, val_acc: 0.953125
epoch: 744: train_loss: 0.9904355125405899, train_acc: 0.4880952338377635, val_loss: 0.7782365381717682, val_acc: 0.890625
epoch: 745: train_loss: 0.9902917350691476, train_acc: 0.46875, val_loss: 0.8156410753726959, val_acc: 0.8125
epoch: 746: train_loss: 0.9900938436847762, train_acc: 0.511904756228129, val_loss: 0.805513322353363, val_acc: 0.796875
epoch: 747: train_loss: 0.9899776571043448, train_acc: 0.5059523781140646, val_loss: 0.7852394580841064, val_acc: 0.859375
epoch: 748: train_loss: 0.9896997015477276, train_acc: 0.5967261989911398, val_loss: 0.8046658337116241, val_acc: 0.875
epoch: 749: train_loss: 0.9895347257984998, train_acc: 0.5, val_loss: 0.7946152091026306, val_acc: 0.9375
epoch: 750: train_loss: 0.989344843773644, train_acc: 0.5342261989911398, val_loss: 0.7997740805149078, val_acc: 0.859375
epoch: 751: train_loss: 0.9891878545601307, train_acc: 0.5104166666666666, val_loss: 0.7817643284797668, val_acc: 0.875
epoch: 752: train_loss: 0.9890154563467699, train_acc: 0.4985119005044301, val_loss: 0.8083368241786957, val_acc: 0.796875
epoch: 753: train_loss: 0.9889158766739764, train_acc: 0.4657738109429677, val_loss: 0.8013848066329956, val_acc: 0.796875
epoch: 754: train_loss: 0.9887585888372078, train_acc: 0.4940476218859355, val_loss: 0.7749805748462677, val_acc: 0.921875
epoch: 755: train_loss: 0.9885381619450897, train_acc: 0.5610119005044302, val_loss: 0.769490659236908, val_acc: 0.921875
epoch: 756: train_loss: 0.988421531749264, train_acc: 0.4627976218859355, val_loss: 0.8088458478450775, val_acc: 0.890625
epoch: 757: train_loss: 0.9882554082149135, train_acc: 0.5, val_loss: 0.7803044617176056, val_acc: 0.890625
epoch: 758: train_loss: 0.9881329113420384, train_acc: 0.4508928656578064, val_loss: 0.7899218201637268, val_acc: 0.921875
epoch: 759: train_loss: 0.9879626045885826, train_acc: 0.4940476218859355, val_loss: 0.7959523797035217, val_acc: 0.9375
epoch: 760: train_loss: 0.9878907518015051, train_acc: 0.4523809552192688, val_loss: 0.7496591210365295, val_acc: 0.953125
epoch: 761: train_loss: 0.9877991355883886, train_acc: 0.4122023781140645, val_loss: 0.8070373237133026, val_acc: 0.828125
epoch: 762: train_loss: 0.9875508801236106, train_acc: 0.5892857114473978, val_loss: 0.8065907061100006, val_acc: 0.875
epoch: 763: train_loss: 0.9873104640534608, train_acc: 0.586309532324473, val_loss: 0.8050170242786407, val_acc: 0.8125
epoch: 764: train_loss: 0.9870995061070301, train_acc: 0.5446428656578064, val_loss: 0.7883710265159607, val_acc: 0.890625
epoch: 765: train_loss: 0.9868703624598341, train_acc: 0.5803571343421936, val_loss: 0.766802191734314, val_acc: 0.96875
epoch: 766: train_loss: 0.9867540256503549, train_acc: 0.4791666666666667, val_loss: 0.7930027842521667, val_acc: 0.875
epoch: 767: train_loss: 0.9865244592332995, train_acc: 0.5967261989911398, val_loss: 0.8081627488136292, val_acc: 0.890625
epoch: 768: train_loss: 0.9863482244509755, train_acc: 0.5104166666666666, val_loss: 0.8293893039226532, val_acc: 0.84375
epoch: 769: train_loss: 0.9861420587027733, train_acc: 0.5342261989911398, val_loss: 0.8081456124782562, val_acc: 0.84375
epoch: 770: train_loss: 0.9859099845787266, train_acc: 0.5729166666666666, val_loss: 0.7610119879245758, val_acc: 0.9375
epoch: 771: train_loss: 0.9857020352634114, train_acc: 0.5357142885526022, val_loss: 0.7939054667949677, val_acc: 0.9375
epoch: 772: train_loss: 0.9855440424762434, train_acc: 0.5074404776096344, val_loss: 0.8143502771854401, val_acc: 0.828125
epoch: 773: train_loss: 0.9853698262667256, train_acc: 0.4955357114473979, val_loss: 0.8372780978679657, val_acc: 0.84375
epoch: 774: train_loss: 0.985221959647311, train_acc: 0.4791666666666667, val_loss: 0.7863167524337769, val_acc: 0.953125
epoch: 775: train_loss: 0.9850976573130509, train_acc: 0.4494047562281291, val_loss: 0.8024612665176392, val_acc: 0.96875
epoch: 776: train_loss: 0.9848840519728337, train_acc: 0.5565476218859354, val_loss: 0.7686487138271332, val_acc: 0.921875
epoch: 777: train_loss: 0.9846586924694284, train_acc: 0.574404756228129, val_loss: 0.7785483002662659, val_acc: 0.953125
epoch: 778: train_loss: 0.9844957559752465, train_acc: 0.543154756228129, val_loss: 0.8271403312683105, val_acc: 0.84375
epoch: 779: train_loss: 0.9843069230644103, train_acc: 0.543154756228129, val_loss: 0.8089889287948608, val_acc: 0.890625
epoch: 780: train_loss: 0.9841561672833035, train_acc: 0.5252976218859354, val_loss: 0.770165354013443, val_acc: 0.9375
epoch: 781: train_loss: 0.9840834908320767, train_acc: 0.3988095223903656, val_loss: 0.7919552624225616, val_acc: 0.921875
epoch: 782: train_loss: 0.9839407465923783, train_acc: 0.5148809552192688, val_loss: 0.7946701943874359, val_acc: 0.875
epoch: 783: train_loss: 0.9837407248259392, train_acc: 0.5416666666666666, val_loss: 0.7774971723556519, val_acc: 0.921875
epoch: 784: train_loss: 0.9835782725087134, train_acc: 0.5059523781140646, val_loss: 0.8063566088676453, val_acc: 0.828125
epoch: 785: train_loss: 0.983430696125248, train_acc: 0.4806547562281291, val_loss: 0.8079913854598999, val_acc: 0.828125
epoch: 786: train_loss: 0.9832482636606014, train_acc: 0.5074404776096344, val_loss: 0.8063561916351318, val_acc: 0.9375
epoch: 787: train_loss: 0.983147886652631, train_acc: 0.4538690447807312, val_loss: 0.8157279193401337, val_acc: 0.859375
epoch: 788: train_loss: 0.9829415006127892, train_acc: 0.5892857114473978, val_loss: 0.792866051197052, val_acc: 0.953125
epoch: 789: train_loss: 0.9827934163280672, train_acc: 0.4836309552192688, val_loss: 0.7624607384204865, val_acc: 0.9375
epoch: 790: train_loss: 0.982652298770169, train_acc: 0.4627976218859355, val_loss: 0.7615041136741638, val_acc: 0.953125
epoch: 791: train_loss: 0.9825384000987307, train_acc: 0.4241071442763011, val_loss: 0.7779976427555084, val_acc: 0.921875
epoch: 792: train_loss: 0.9823781628476407, train_acc: 0.4955357114473979, val_loss: 0.792788028717041, val_acc: 0.9375
epoch: 793: train_loss: 0.9822372763873539, train_acc: 0.5, val_loss: 0.796114981174469, val_acc: 0.796875
epoch: 794: train_loss: 0.9820564224035226, train_acc: 0.4970238109429677, val_loss: 0.7702278792858124, val_acc: 0.875
epoch: 795: train_loss: 0.9818112270865998, train_acc: 0.574404756228129, val_loss: 0.7933342456817627, val_acc: 0.890625
epoch: 796: train_loss: 0.9817348951727006, train_acc: 0.4732142885526021, val_loss: 0.7873681783676147, val_acc: 0.890625
epoch: 797: train_loss: 0.9815601352960136, train_acc: 0.5372023781140646, val_loss: 0.8201718032360077, val_acc: 0.859375
epoch: 798: train_loss: 0.9813471361155496, train_acc: 0.5342261989911398, val_loss: 0.7556670904159546, val_acc: 0.9375
epoch: 799: train_loss: 0.9811736789594085, train_acc: 0.5133928656578064, val_loss: 0.7729493677616119, val_acc: 0.9375
epoch: 800: train_loss: 0.9810172007126549, train_acc: 0.543154756228129, val_loss: 0.8207830190658569, val_acc: 0.84375
epoch: 801: train_loss: 0.9808352192986133, train_acc: 0.5386904776096344, val_loss: 0.7880109250545502, val_acc: 0.90625
epoch: 802: train_loss: 0.9806610428975193, train_acc: 0.4836309552192688, val_loss: 0.8406337201595306, val_acc: 0.78125
epoch: 803: train_loss: 0.9804195051218928, train_acc: 0.5803571343421936, val_loss: 0.804364949464798, val_acc: 0.828125
epoch: 804: train_loss: 0.9802471681658023, train_acc: 0.543154756228129, val_loss: 0.7495481371879578, val_acc: 0.9375
epoch: 805: train_loss: 0.9800838680677507, train_acc: 0.4955357114473979, val_loss: 0.7745670080184937, val_acc: 0.9375
epoch: 806: train_loss: 0.9798649377391527, train_acc: 0.5520833333333334, val_loss: 0.7824605107307434, val_acc: 0.9375
epoch: 807: train_loss: 0.9797548873983193, train_acc: 0.4389880895614624, val_loss: 0.8244853913784027, val_acc: 0.875
epoch: 808: train_loss: 0.9795835370333725, train_acc: 0.5044642885526022, val_loss: 0.803799957036972, val_acc: 0.890625
epoch: 809: train_loss: 0.9794690833415506, train_acc: 0.4553571442763011, val_loss: 0.7753324210643768, val_acc: 0.890625
epoch: 810: train_loss: 0.9793775865338824, train_acc: 0.4449404776096344, val_loss: 0.7782714664936066, val_acc: 0.875
epoch: 811: train_loss: 0.9791524452577859, train_acc: 0.555059532324473, val_loss: 0.7880776226520538, val_acc: 0.953125
epoch: 812: train_loss: 0.9789751090686692, train_acc: 0.5342261989911398, val_loss: 0.7821262180805206, val_acc: 0.96875
epoch: 813: train_loss: 0.9788356373341324, train_acc: 0.4538690447807312, val_loss: 0.7974392771720886, val_acc: 0.859375
epoch: 814: train_loss: 0.9786250866751478, train_acc: 0.53125, val_loss: 0.8138009905815125, val_acc: 0.875
epoch: 815: train_loss: 0.9784303183113431, train_acc: 0.5565476218859354, val_loss: 0.7942960560321808, val_acc: 0.859375
epoch: 816: train_loss: 0.9782940371091968, train_acc: 0.4895833333333333, val_loss: 0.7964175045490265, val_acc: 0.828125
epoch: 817: train_loss: 0.9781386373532714, train_acc: 0.5461309552192688, val_loss: 0.7843714356422424, val_acc: 0.953125
epoch: 818: train_loss: 0.9779858342942341, train_acc: 0.4791666666666667, val_loss: 0.7937283515930176, val_acc: 0.828125
epoch: 819: train_loss: 0.9777748392607131, train_acc: 0.549107144276301, val_loss: 0.7656006515026093, val_acc: 0.9375
epoch: 820: train_loss: 0.9776071933677797, train_acc: 0.53125, val_loss: 0.7962034642696381, val_acc: 0.859375
epoch: 821: train_loss: 0.9774933096507841, train_acc: 0.4672619005044301, val_loss: 0.7788543403148651, val_acc: 0.875
epoch: 822: train_loss: 0.9772950293275202, train_acc: 0.53125, val_loss: 0.7920505106449127, val_acc: 0.84375
epoch: 823: train_loss: 0.9771638395668613, train_acc: 0.4761904776096344, val_loss: 0.7845613062381744, val_acc: 0.890625
epoch: 824: train_loss: 0.9770881852959135, train_acc: 0.4226190447807312, val_loss: 0.7983052730560303, val_acc: 0.859375
epoch: 825: train_loss: 0.9769146986263613, train_acc: 0.5089285671710968, val_loss: 0.7768766582012177, val_acc: 0.9375
epoch: 826: train_loss: 0.9767035569985124, train_acc: 0.5788690447807312, val_loss: 0.8114845156669617, val_acc: 0.8125
epoch: 827: train_loss: 0.9764754535831299, train_acc: 0.5669642885526022, val_loss: 0.7590762972831726, val_acc: 0.953125
epoch: 828: train_loss: 0.9762917806644916, train_acc: 0.5446428656578064, val_loss: 0.7510247528553009, val_acc: 0.984375
epoch: 829: train_loss: 0.9762013572526258, train_acc: 0.4523809552192688, val_loss: 0.756534606218338, val_acc: 0.90625
epoch: 830: train_loss: 0.9760927771211764, train_acc: 0.4270833333333333, val_loss: 0.7396341264247894, val_acc: 0.96875
epoch: 831: train_loss: 0.9759426913343555, train_acc: 0.511904756228129, val_loss: 0.7803597450256348, val_acc: 0.828125
epoch: 832: train_loss: 0.975782230812437, train_acc: 0.4880952338377635, val_loss: 0.7875303626060486, val_acc: 0.890625
epoch: 833: train_loss: 0.9755916960662497, train_acc: 0.555059532324473, val_loss: 0.7588163018226624, val_acc: 0.875
epoch: 834: train_loss: 0.9753992039286442, train_acc: 0.5416666666666666, val_loss: 0.7661830484867096, val_acc: 0.953125
epoch: 835: train_loss: 0.975284926605186, train_acc: 0.4821428656578064, val_loss: 0.8337626159191132, val_acc: 0.8125
epoch: 836: train_loss: 0.9751214220769747, train_acc: 0.5148809552192688, val_loss: 0.7881994247436523, val_acc: 0.859375
epoch: 837: train_loss: 0.9748777885615298, train_acc: 0.6235119104385376, val_loss: 0.776901364326477, val_acc: 0.953125
epoch: 838: train_loss: 0.9747723738352452, train_acc: 0.4136904776096344, val_loss: 0.7905653119087219, val_acc: 0.875
epoch: 839: train_loss: 0.9746425920062589, train_acc: 0.4851190447807312, val_loss: 0.755186140537262, val_acc: 0.890625
epoch: 840: train_loss: 0.9744859266696914, train_acc: 0.5223214228947958, val_loss: 0.7707324326038361, val_acc: 0.921875
epoch: 841: train_loss: 0.9743745207833747, train_acc: 0.4389880895614624, val_loss: 0.7592986524105072, val_acc: 0.921875
epoch: 842: train_loss: 0.9742015556131435, train_acc: 0.5610119005044302, val_loss: 0.788522869348526, val_acc: 0.921875
epoch: 843: train_loss: 0.9740458557391048, train_acc: 0.5163690447807312, val_loss: 0.8028163611888885, val_acc: 0.828125
epoch: 844: train_loss: 0.9737765428112334, train_acc: 0.613095243771871, val_loss: 0.8010505735874176, val_acc: 0.890625
epoch: 845: train_loss: 0.9736167418383721, train_acc: 0.5029761989911398, val_loss: 0.8009994328022003, val_acc: 0.859375
epoch: 846: train_loss: 0.9734287353914955, train_acc: 0.5610119005044302, val_loss: 0.7731600403785706, val_acc: 0.84375
epoch: 847: train_loss: 0.9733316156568013, train_acc: 0.4583333333333333, val_loss: 0.7842382788658142, val_acc: 0.9375
epoch: 848: train_loss: 0.9732183846109369, train_acc: 0.5163690447807312, val_loss: 0.7951395511627197, val_acc: 0.890625
epoch: 849: train_loss: 0.973039852941737, train_acc: 0.5788690447807312, val_loss: 0.7805722653865814, val_acc: 0.9375
epoch: 850: train_loss: 0.9728597480784754, train_acc: 0.5357142885526022, val_loss: 0.7912885248661041, val_acc: 0.890625
epoch: 851: train_loss: 0.9727016589684474, train_acc: 0.5029761989911398, val_loss: 0.765602707862854, val_acc: 0.953125
epoch: 852: train_loss: 0.9725271224649491, train_acc: 0.5148809552192688, val_loss: 0.7743180990219116, val_acc: 0.8125
epoch: 853: train_loss: 0.9724008814605481, train_acc: 0.4672619005044301, val_loss: 0.7512661218643188, val_acc: 0.953125
epoch: 854: train_loss: 0.972200845556649, train_acc: 0.5625, val_loss: 0.7867448925971985, val_acc: 0.90625
epoch: 855: train_loss: 0.9720952158729973, train_acc: 0.4538690447807312, val_loss: 0.8008241355419159, val_acc: 0.90625
epoch: 856: train_loss: 0.9719698821835271, train_acc: 0.4717261989911397, val_loss: 0.7643825113773346, val_acc: 0.90625
epoch: 857: train_loss: 0.9718254900710376, train_acc: 0.5014880895614624, val_loss: 0.805330216884613, val_acc: 0.78125
epoch: 858: train_loss: 0.971683886840355, train_acc: 0.4672619005044301, val_loss: 0.7639371752738953, val_acc: 0.859375
epoch: 859: train_loss: 0.9714442825825634, train_acc: 0.5892857114473978, val_loss: 0.7840820848941803, val_acc: 0.90625
epoch: 860: train_loss: 0.9713138553969762, train_acc: 0.4880952338377635, val_loss: 0.8360269665718079, val_acc: 0.828125
epoch: 861: train_loss: 0.9711977948491549, train_acc: 0.4449404776096344, val_loss: 0.7450861930847168, val_acc: 0.953125
epoch: 862: train_loss: 0.9709887254325609, train_acc: 0.5758928656578064, val_loss: 0.7554369568824768, val_acc: 0.9375
epoch: 863: train_loss: 0.9708329771909812, train_acc: 0.5372023781140646, val_loss: 0.7527008056640625, val_acc: 0.953125
epoch: 864: train_loss: 0.9706667513975973, train_acc: 0.517857144276301, val_loss: 0.7877458930015564, val_acc: 0.859375
epoch: 865: train_loss: 0.9704985689483665, train_acc: 0.5446428656578064, val_loss: 0.8269804120063782, val_acc: 0.796875
epoch: 866: train_loss: 0.9703407321055084, train_acc: 0.4508928656578064, val_loss: 0.8003595173358917, val_acc: 0.8125
epoch: 867: train_loss: 0.9701714270308999, train_acc: 0.5372023781140646, val_loss: 0.7867685258388519, val_acc: 0.890625
epoch: 868: train_loss: 0.9699805260939205, train_acc: 0.5639880895614624, val_loss: 0.782158374786377, val_acc: 0.890625
epoch: 869: train_loss: 0.9698310453316257, train_acc: 0.4940476218859355, val_loss: 0.8320484459400177, val_acc: 0.828125
epoch: 870: train_loss: 0.9696251815231282, train_acc: 0.555059532324473, val_loss: 0.8124741315841675, val_acc: 0.828125
epoch: 871: train_loss: 0.9694460320545625, train_acc: 0.5982142885526022, val_loss: 0.7940252125263214, val_acc: 0.8125
epoch: 872: train_loss: 0.9692925680862575, train_acc: 0.4568452338377635, val_loss: 0.7674221396446228, val_acc: 0.953125
epoch: 873: train_loss: 0.9692048863724473, train_acc: 0.4434523781140645, val_loss: 0.783509373664856, val_acc: 0.8125
epoch: 874: train_loss: 0.9690121776262915, train_acc: 0.5550595223903656, val_loss: 0.7608388364315033, val_acc: 0.890625
epoch: 875: train_loss: 0.9688350215803722, train_acc: 0.5595238109429678, val_loss: 0.7717518210411072, val_acc: 0.953125
epoch: 876: train_loss: 0.9687342012576481, train_acc: 0.4434523781140645, val_loss: 0.7488687634468079, val_acc: 0.953125
epoch: 877: train_loss: 0.9685587121531556, train_acc: 0.53125, val_loss: 0.8058456480503082, val_acc: 0.828125
epoch: 878: train_loss: 0.9684021998455361, train_acc: 0.5208333333333334, val_loss: 0.77623251080513, val_acc: 0.859375
epoch: 879: train_loss: 0.9682269102018888, train_acc: 0.5357142885526022, val_loss: 0.7213383913040161, val_acc: 0.96875
epoch: 880: train_loss: 0.9681078178432461, train_acc: 0.4925595323244731, val_loss: 0.8128301203250885, val_acc: 0.890625
epoch: 881: train_loss: 0.9679324225954095, train_acc: 0.5357142885526022, val_loss: 0.776946634054184, val_acc: 0.875
epoch: 882: train_loss: 0.9677991689084022, train_acc: 0.4776785671710968, val_loss: 0.7993215620517731, val_acc: 0.90625
epoch: 883: train_loss: 0.9676805715080835, train_acc: 0.4568452338377635, val_loss: 0.7834941744804382, val_acc: 0.890625
epoch: 884: train_loss: 0.9674509106607307, train_acc: 0.5535714228947958, val_loss: 0.7784815728664398, val_acc: 0.84375
epoch: 885: train_loss: 0.9673675654345295, train_acc: 0.4479166666666667, val_loss: 0.7537455856800079, val_acc: 0.890625
epoch: 886: train_loss: 0.9671954630192665, train_acc: 0.5639880895614624, val_loss: 0.7818911671638489, val_acc: 0.890625
epoch: 887: train_loss: 0.967066161267392, train_acc: 0.5104166666666666, val_loss: 0.7890189290046692, val_acc: 0.796875
epoch: 888: train_loss: 0.9669316438954554, train_acc: 0.5208333333333334, val_loss: 0.7577617466449738, val_acc: 0.90625
epoch: 889: train_loss: 0.9667890482181017, train_acc: 0.5297619005044302, val_loss: 0.790930986404419, val_acc: 0.84375
epoch: 890: train_loss: 0.9664718441155086, train_acc: 0.6741071343421936, val_loss: 0.784475564956665, val_acc: 0.875
epoch: 891: train_loss: 0.966256501050095, train_acc: 0.6086309552192688, val_loss: 0.7644846439361572, val_acc: 0.953125
epoch: 892: train_loss: 0.9661106871953423, train_acc: 0.5372023781140646, val_loss: 0.7602407336235046, val_acc: 0.953125
epoch: 893: train_loss: 0.9659263493630708, train_acc: 0.5639880895614624, val_loss: 0.7472735941410065, val_acc: 0.96875
epoch: 894: train_loss: 0.9657665859831553, train_acc: 0.5401785671710968, val_loss: 0.7807277739048004, val_acc: 0.890625
epoch: 895: train_loss: 0.9655805271163225, train_acc: 0.5684523781140646, val_loss: 0.7901570796966553, val_acc: 0.828125
epoch: 896: train_loss: 0.9653810575875769, train_acc: 0.5669642885526022, val_loss: 0.7749428153038025, val_acc: 0.921875
epoch: 897: train_loss: 0.9652746655406468, train_acc: 0.4761904776096344, val_loss: 0.7667119204998016, val_acc: 0.875
epoch: 898: train_loss: 0.9652087823921189, train_acc: 0.4017857114473979, val_loss: 0.7822805047035217, val_acc: 0.875
epoch: 899: train_loss: 0.9650433466611082, train_acc: 0.4880952338377635, val_loss: 0.7674863338470459, val_acc: 0.90625
epoch: 900: train_loss: 0.9649065423796803, train_acc: 0.4895833333333333, val_loss: 0.7938600182533264, val_acc: 0.875
epoch: 901: train_loss: 0.9646664731210013, train_acc: 0.625, val_loss: 0.7750056087970734, val_acc: 0.890625
epoch: 902: train_loss: 0.9645392776151649, train_acc: 0.5089285671710968, val_loss: 0.7524910867214203, val_acc: 0.90625
epoch: 903: train_loss: 0.9643589973801353, train_acc: 0.5610119005044302, val_loss: 0.7389275133609772, val_acc: 0.953125
epoch: 904: train_loss: 0.9641903185273618, train_acc: 0.5461309552192688, val_loss: 0.7336259186267853, val_acc: 0.921875
epoch: 905: train_loss: 0.9640499624458629, train_acc: 0.5029761989911398, val_loss: 0.7181825041770935, val_acc: 0.96875
epoch: 906: train_loss: 0.963852370389898, train_acc: 0.5327380895614624, val_loss: 0.7874308526515961, val_acc: 0.9375
epoch: 907: train_loss: 0.9636663941559809, train_acc: 0.5193452338377634, val_loss: 0.7252372801303864, val_acc: 0.9375
epoch: 908: train_loss: 0.9635481715071185, train_acc: 0.5, val_loss: 0.7926819026470184, val_acc: 0.953125
epoch: 909: train_loss: 0.963457804830956, train_acc: 0.4821428656578064, val_loss: 0.7823520004749298, val_acc: 0.859375
epoch: 910: train_loss: 0.9633805995845895, train_acc: 0.4092261890570323, val_loss: 0.7949382960796356, val_acc: 0.859375
epoch: 911: train_loss: 0.9632315845038106, train_acc: 0.5208333333333334, val_loss: 0.7636711895465851, val_acc: 0.859375
epoch: 912: train_loss: 0.9630839066289555, train_acc: 0.53125, val_loss: 0.772438645362854, val_acc: 0.953125
epoch: 913: train_loss: 0.9629799853833723, train_acc: 0.4657738109429677, val_loss: 0.7723233997821808, val_acc: 0.953125
epoch: 914: train_loss: 0.9627443135327542, train_acc: 0.6517857114473978, val_loss: 0.7397375702857971, val_acc: 0.9375
epoch: 915: train_loss: 0.9626137319480484, train_acc: 0.4985119005044301, val_loss: 0.7604062855243683, val_acc: 0.859375
epoch: 916: train_loss: 0.9624390342546693, train_acc: 0.5104166666666666, val_loss: 0.7724505662918091, val_acc: 0.9375
epoch: 917: train_loss: 0.9623076802981267, train_acc: 0.5252976218859354, val_loss: 0.78032585978508, val_acc: 0.859375
epoch: 918: train_loss: 0.9621952838295816, train_acc: 0.4880952338377635, val_loss: 0.8044915199279785, val_acc: 0.875
epoch: 919: train_loss: 0.9620429964601125, train_acc: 0.5297619005044302, val_loss: 0.7336494028568268, val_acc: 0.890625
epoch: 920: train_loss: 0.9618911696351867, train_acc: 0.5014880895614624, val_loss: 0.7562387883663177, val_acc: 0.953125
epoch: 921: train_loss: 0.9616596167361654, train_acc: 0.601190467675527, val_loss: 0.8343599438667297, val_acc: 0.71875
epoch: 922: train_loss: 0.961452769152564, train_acc: 0.5535714228947958, val_loss: 0.7835304141044617, val_acc: 0.828125
epoch: 923: train_loss: 0.9613980968922242, train_acc: 0.4107142885526021, val_loss: 0.7737377882003784, val_acc: 0.828125
epoch: 924: train_loss: 0.9612706235292791, train_acc: 0.5416666666666666, val_loss: 0.7675407826900482, val_acc: 0.8125
epoch: 925: train_loss: 0.9610800598489881, train_acc: 0.555059532324473, val_loss: 0.7760681509971619, val_acc: 0.875
epoch: 926: train_loss: 0.9609153126907276, train_acc: 0.5773809552192688, val_loss: 0.7821161150932312, val_acc: 0.890625
epoch: 927: train_loss: 0.9608251158490602, train_acc: 0.4880952338377635, val_loss: 0.7826434969902039, val_acc: 0.921875
epoch: 928: train_loss: 0.960633216684421, train_acc: 0.574404756228129, val_loss: 0.7452675402164459, val_acc: 0.921875
epoch: 929: train_loss: 0.9604030969108727, train_acc: 0.6517857114473978, val_loss: 0.7733604311943054, val_acc: 0.796875
epoch: 930: train_loss: 0.9603073314403142, train_acc: 0.4434523781140645, val_loss: 0.7872594594955444, val_acc: 0.828125
epoch: 931: train_loss: 0.9600519069999408, train_acc: 0.6160714228947958, val_loss: 0.7643358111381531, val_acc: 0.875
epoch: 932: train_loss: 0.9599139514928542, train_acc: 0.549107144276301, val_loss: 0.7566007077693939, val_acc: 0.90625
epoch: 933: train_loss: 0.9598329312285717, train_acc: 0.4806547562281291, val_loss: 0.7290458381175995, val_acc: 0.9375
epoch: 934: train_loss: 0.9597039100958058, train_acc: 0.4642857114473979, val_loss: 0.7650657892227173, val_acc: 0.84375
epoch: 935: train_loss: 0.9595788606678652, train_acc: 0.4940476218859355, val_loss: 0.7622888088226318, val_acc: 0.84375
epoch: 936: train_loss: 0.9594765020748419, train_acc: 0.4330357114473979, val_loss: 0.757099449634552, val_acc: 0.875
epoch: 937: train_loss: 0.9593315574406046, train_acc: 0.5342261989911398, val_loss: 0.7760899662971497, val_acc: 0.90625
epoch: 938: train_loss: 0.9592572432864328, train_acc: 0.4241071442763011, val_loss: 0.7495860159397125, val_acc: 0.921875
epoch: 939: train_loss: 0.9590296369283752, train_acc: 0.586309532324473, val_loss: 0.7410753965377808, val_acc: 0.9375
epoch: 940: train_loss: 0.9588414760840258, train_acc: 0.549107144276301, val_loss: 0.756912112236023, val_acc: 0.921875
epoch: 941: train_loss: 0.9587015522893061, train_acc: 0.5610119104385376, val_loss: 0.7255302369594574, val_acc: 0.921875
epoch: 942: train_loss: 0.9586033566169152, train_acc: 0.4449404776096344, val_loss: 0.7302696704864502, val_acc: 0.90625
epoch: 943: train_loss: 0.9584180523488811, train_acc: 0.555059532324473, val_loss: 0.7298675775527954, val_acc: 0.9375
epoch: 944: train_loss: 0.9582991686657827, train_acc: 0.5059523781140646, val_loss: 0.7218764126300812, val_acc: 0.9375
epoch: 945: train_loss: 0.9581003881324078, train_acc: 0.5684523781140646, val_loss: 0.7480250597000122, val_acc: 0.90625
epoch: 946: train_loss: 0.9579672822856595, train_acc: 0.4851190447807312, val_loss: 0.7504798769950867, val_acc: 0.9375
epoch: 947: train_loss: 0.9577580975413149, train_acc: 0.5758928656578064, val_loss: 0.7593474388122559, val_acc: 0.875
epoch: 948: train_loss: 0.9575571013954084, train_acc: 0.613095243771871, val_loss: 0.814471423625946, val_acc: 0.890625
epoch: 949: train_loss: 0.9573924851208396, train_acc: 0.5595238109429678, val_loss: 0.7718567252159119, val_acc: 0.96875
epoch: 950: train_loss: 0.9572086791970453, train_acc: 0.5625, val_loss: 0.7315816879272461, val_acc: 0.953125
epoch: 951: train_loss: 0.9569713882997584, train_acc: 0.6145833333333334, val_loss: 0.7625867426395416, val_acc: 0.890625
epoch: 952: train_loss: 0.9568768312523265, train_acc: 0.5208333333333334, val_loss: 0.7682371735572815, val_acc: 0.890625
epoch: 953: train_loss: 0.9566638184793333, train_acc: 0.586309532324473, val_loss: 0.761996865272522, val_acc: 0.921875
epoch: 954: train_loss: 0.9565533942369051, train_acc: 0.4776785671710968, val_loss: 0.7681036591529846, val_acc: 0.875
epoch: 955: train_loss: 0.9564364951109113, train_acc: 0.4910714228947957, val_loss: 0.7875040471553802, val_acc: 0.890625
epoch: 956: train_loss: 0.9562556056642474, train_acc: 0.5327380895614624, val_loss: 0.7599910497665405, val_acc: 0.890625
epoch: 957: train_loss: 0.9560186827066622, train_acc: 0.6592261989911398, val_loss: 0.7960127890110016, val_acc: 0.84375
epoch: 958: train_loss: 0.9559034225963071, train_acc: 0.4657738109429677, val_loss: 0.7947176992893219, val_acc: 0.828125
epoch: 959: train_loss: 0.9557151187418228, train_acc: 0.5431547661622366, val_loss: 0.7400790452957153, val_acc: 0.921875
epoch: 960: train_loss: 0.9555465465529768, train_acc: 0.5461309552192688, val_loss: 0.7525230944156647, val_acc: 0.875
epoch: 961: train_loss: 0.9554519333198339, train_acc: 0.4360119005044301, val_loss: 0.7305412590503693, val_acc: 0.921875
epoch: 962: train_loss: 0.9552421711975821, train_acc: 0.5982142885526022, val_loss: 0.7673559188842773, val_acc: 0.84375
epoch: 963: train_loss: 0.9551945414656913, train_acc: 0.3988095223903656, val_loss: 0.7615808844566345, val_acc: 0.875
epoch: 964: train_loss: 0.9550497258471283, train_acc: 0.5059523781140646, val_loss: 0.746890664100647, val_acc: 0.9375
epoch: 965: train_loss: 0.9549033370118368, train_acc: 0.5044642885526022, val_loss: 0.7455190718173981, val_acc: 0.9375
epoch: 966: train_loss: 0.9547627562902905, train_acc: 0.5148809552192688, val_loss: 0.7635538876056671, val_acc: 0.84375
epoch: 967: train_loss: 0.9546371209063143, train_acc: 0.4702380895614624, val_loss: 0.7316330671310425, val_acc: 0.96875
epoch: 968: train_loss: 0.9544789861230282, train_acc: 0.5223214228947958, val_loss: 0.7784260511398315, val_acc: 0.890625
epoch: 969: train_loss: 0.9542893720451492, train_acc: 0.5595238109429678, val_loss: 0.723188191652298, val_acc: 0.9375
epoch: 970: train_loss: 0.9541743880401876, train_acc: 0.4613095323244731, val_loss: 0.7527856528759003, val_acc: 0.90625
epoch: 971: train_loss: 0.9540232790963632, train_acc: 0.5535714228947958, val_loss: 0.789405107498169, val_acc: 0.828125
epoch: 972: train_loss: 0.9538699375639709, train_acc: 0.5491071343421936, val_loss: 0.7828566133975983, val_acc: 0.84375
epoch: 973: train_loss: 0.953732161449292, train_acc: 0.5372023781140646, val_loss: 0.730063408613205, val_acc: 0.953125
epoch: 974: train_loss: 0.9536471532348886, train_acc: 0.4717261989911397, val_loss: 0.7861565053462982, val_acc: 0.78125
epoch: 975: train_loss: 0.9534202573077919, train_acc: 0.5892857114473978, val_loss: 0.7458384931087494, val_acc: 0.859375
epoch: 976: train_loss: 0.9532126320461811, train_acc: 0.605654756228129, val_loss: 0.7163679599761963, val_acc: 0.921875
epoch: 977: train_loss: 0.9531509490669701, train_acc: 0.4464285671710968, val_loss: 0.7679898142814636, val_acc: 0.890625
epoch: 978: train_loss: 0.9529445994699891, train_acc: 0.5729166666666666, val_loss: 0.7656087875366211, val_acc: 0.90625
epoch: 979: train_loss: 0.9527712997327848, train_acc: 0.5357142885526022, val_loss: 0.754424661397934, val_acc: 0.890625
epoch: 980: train_loss: 0.952634396903011, train_acc: 0.4836309552192688, val_loss: 0.7715782225131989, val_acc: 0.84375
epoch: 981: train_loss: 0.9525341575475906, train_acc: 0.4895833333333333, val_loss: 0.783076822757721, val_acc: 0.828125
epoch: 982: train_loss: 0.9524022093179784, train_acc: 0.4717261989911397, val_loss: 0.7635577321052551, val_acc: 0.875
epoch: 983: train_loss: 0.9522546020747837, train_acc: 0.5029761989911398, val_loss: 0.7398189306259155, val_acc: 0.875
epoch: 984: train_loss: 0.9521580370146199, train_acc: 0.4211309552192688, val_loss: 0.7059412002563477, val_acc: 0.96875
epoch: 985: train_loss: 0.9519729582420299, train_acc: 0.5505952338377634, val_loss: 0.7442167699337006, val_acc: 0.875
epoch: 986: train_loss: 0.95187963562936, train_acc: 0.4866071442763011, val_loss: 0.7331553399562836, val_acc: 0.890625
epoch: 987: train_loss: 0.951681085906208, train_acc: 0.5788690447807312, val_loss: 0.7455880641937256, val_acc: 0.90625
epoch: 988: train_loss: 0.9515394300654306, train_acc: 0.5565476218859354, val_loss: 0.7898582220077515, val_acc: 0.78125
epoch: 989: train_loss: 0.9513838829416209, train_acc: 0.5476190447807312, val_loss: 0.7142044305801392, val_acc: 0.921875
epoch: 990: train_loss: 0.9513228835780485, train_acc: 0.4122023781140645, val_loss: 0.7265364229679108, val_acc: 0.921875
epoch: 991: train_loss: 0.9512136184600405, train_acc: 0.5, val_loss: 0.7350543439388275, val_acc: 0.953125
epoch: 992: train_loss: 0.9509710843989649, train_acc: 0.6532738010088602, val_loss: 0.7503024935722351, val_acc: 0.890625
epoch: 993: train_loss: 0.9508078514050502, train_acc: 0.549107144276301, val_loss: 0.7997598648071289, val_acc: 0.796875
epoch: 994: train_loss: 0.9506671976004806, train_acc: 0.5208333333333334, val_loss: 0.7553243935108185, val_acc: 0.890625
epoch: 995: train_loss: 0.9504729839493146, train_acc: 0.5982142885526022, val_loss: 0.73885378241539, val_acc: 0.875
epoch: 996: train_loss: 0.9503690680306471, train_acc: 0.4702380895614624, val_loss: 0.7511442303657532, val_acc: 0.875
epoch: 997: train_loss: 0.9502183467488163, train_acc: 0.5282738109429678, val_loss: 0.7702779769897461, val_acc: 0.8125
epoch: 998: train_loss: 0.9500773688097728, train_acc: 0.4985119005044301, val_loss: 0.7616472542285919, val_acc: 0.875
epoch: 999: train_loss: 0.9499458439747486, train_acc: 0.5193452338377634, val_loss: 0.748824417591095, val_acc: 0.875
0.9350122809410095 0.59375
GroundTruth:  Alligator Cracks Alligator Cracks Alligator Cracks Alligator Cracks
Accuracy of the network on the test images: 53 %
Accuracy for class: Alligator Cracks is 0.0 %
Accuracy for class: Longitudinal Cracks is 4.8 %
Accuracy for class: Transverse Cracks is 100.0 %
